### **Trust, attitudes ** **and use of artificial ** **intelligence**

**A global study 2025**

University of Melbourne | KPMG International

[unimelb.edu.au | kpmg.com](http://unimelb.edu.au)

**Citation**

Gillespie, N., Lockey, S., Ward, T., Macdade, A., & Hassed, G. (2025). _Trust, attitudes and use of artificial_
_intelligence:_ A global study 2025. The University of Melbourne and KPMG. DOI 10.26188/28822919.

_Trust, attitudes and use of artificial intelligence: A global study_ is provided under a Creative Commons
Attribution, Non-Commercial, Share Alike 4.0 International licence. You are free to use, share,
reproduce and distribute the work under this licence for non-commercial purposes only, as long as
you give appropriate credit to the original author(s) and the source via the citation. If any changes
are made to the material, information, graphics, etc, contained in this report, the changes must be
clearly indicated. Under this licence, you may not use the material for any commercial purposes.
Any re-sharing of this material can only be done under the CC NC SA licence conditions.

**University of Melbourne Research Team**

Professor Nicole Gillespie, Dr Steve Lockey, Alexandria Macdade, Tabi Ward, and Gerard Hassed.

Professor Nicole Gillespie and Dr Steve Lockey from the University of Melbourne led the design,
conduct, data collection, analysis, and reporting of this research.

At various stages of the project, the research team sought feedback and input from a
multidisciplinary advisory board, including academics and industry experts, while maintaining
independence over the conduct and reporting of the research.

**Acknowledgments**

Advisory group: James Mabbott, Jessica Wyndham, Nicola Stone, Sam Gloede, Dan Konigsburg,
Sam Burns, Kathryn Wright, Melany Eli, Rita Fentener van Vlissingen, David Rowlands, Laurent Gobbi,
Rene Vader, Adrian Clamp, Jane Lawrie, Jessica Seddon, Ed O’Brien, Kristin Silva, and Richard Boele.

We are grateful for the insightful expert input and feedback provided at various stages of the research by
Ali Akbari, Nick Davis, Shazia Sadiq, Ed Santow, Jeannie Paterson, Llewellyn Spink, Tapani-Rinta-Kahila,
Alice Rickert, Lucy Kenyon-Jones, Morteza Namvar, Olya Ohrimenko, Saeed Akhlaghpour, Chris Ziguras,
Sam Forsyth, Greg Dober, Giles Hirst, and Madhava Jay.

We appreciate the data analysis support provided by Jake Morrill.

Report production: Kathryn Wright, Melany Eli, Bethany Fracassi, Nancy Stewart, Yong Dithavong,
Marty Scerri and Lachlan Hardisty.

**Funding**

This research was supported by the Chair in Trust research partnership between the University of
Melbourne and KPMG Australia, and funding from KPMG International, KPMG Australia, and the
University of Melbourne.

The research was conducted independently by the university research team.


© 2025 The University of Melbourne.

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.


Trust, attitudes and use of AI: A global study 2025 | **2**

##### **Contents**

List of figures **2**

Executive summary **4**

Introduction **11**

How the research was conducted **13**

**Section 1: Public attitudes towards AI** **18**

- To what extent do people use and understand AI systems? **19**

- To what extent do people trust and accept AI systems? **27**

- How do people view and experience the benefits

and risks of AI? **37**

- What do people expect from the regulation and

governance of AI? **47**

- What are the key drivers of trust and acceptance

of AI systems? **59**

- How do demographic factors influence trust, attitudes

and use of AI?  **62**

**Section 2: Employee attitudes towards AI at work** **66**

- How is AI being used by employees at work? **67**

- What are the impacts of AI use at work? **77**

- How do demographic factors influence use

and perceptions of AI at work? **85**

**Section 3: Student attitudes towards AI in education** **89**

- How is AI being used by students? **90**

- What are the impacts of AI use in education? **93**

Conclusion and implications **96**

Appendix 1: Methodological and statistical notes **104**

Appendix 2: Sample demographics **107**

Appendix 3: Key indicators for each country **109**

Appendix 4: Changes in key indicators over time for 17 countries **110**

© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **1**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

##### **List of figures**

Figure 1: Frequency of intentional use of AI tools for personal, work, or study purposes 20

Figure 2: Use of AI systems on a regular or semi-regular basis across countries 21

Figure 3: AI-related training or education 22

Figure 4: Self-reported AI knowledge 22

Figure 5: Self-reported AI efficacy 22

Figure 6: AI training and education, knowledge and AI efficacy across economic groups 23

Figure 7: AI knowledge, efficacy, and training across countries 24

Figure 8: Use of common technologies and awareness that they involve AI 25

Figure 9: Perceptions of the trustworthiness of AI systems 28

Figure 10: Trust and acceptance of AI systems 29

Figure 11: Trust in AI applications across countries 30

Figure 12: Trust and acceptance of AI systems across economic groups 31

Figure 13: Trust and acceptance of AI systems across countries 32

Figure 14: Emotions associated with AI 33

Figure 15: Emotions toward AI across countries 34

Figure 16: Trust of AI systems and worry about AI in 2022 and 2024 35

Figure 17: Expected and experienced benefits of AI use 38

Figure 18: Expected benefits of AI across countries 39

Figure 19: Experienced benefits of AI across countries 40

Figure 20: Perceived risks and experienced negative outcomes from AI use 41

Figure 21: Concerns about the risks of AI across countries 43

Figure 22: Experienced negative outcomes from AI use across countries 44

Figure 23: Perceptions across countries that AI benefits outweigh risks 45

Figure 24: Need for AI regulation across countries 49

Figure 25: Perceived adequacy of current regulation and laws to make AI use safe 50

Figure 26: Expectations of who should regulate AI 51

Figure 27: Expectations of who should regulate AI across countries 52

Figure 28: Impacts and management of AI generated misinformation 53

Figure 29: AI assurance mechanisms 54

Figure 30: Confidence in entities to develop and use AI 56

Figure 31: Confidence in entities to develop and use AI across countries 57

© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **2**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

##### **List of figures cont’d**

Figure 32: A model of the key drivers of trust and acceptance of AI use in society 60

Figure 33: Trust and acceptance of AI systems by age, income, education, and AI training 64

Figure 34: Use of AI and AI training by age, income, and education 64

Figure 35: AI knowledge and AI efficacy by age, income, and education 65

Figure 36: Organizational use of AI (employee reported) 67

Figure 37: Frequency of intentional use of AI at work 68

Figure 38: Organizational and employee AI adoption have increased over time 69

Figure 39: Types of AI tools intentionally used at work 70

Figure 40: Access to AI tools used at work 71

Figure 41: Organizational policy or guidance on generative AI at work (employee reported) 71

Figure 42: Frequency of intentional use of AI at work 72

Figure 43: Intentional use of AI at work and trust of AI at work 73

Figure 44: Inappropriate and complacent use of AI at work 76

Figure 45: Critical engagement with AI at work 76

Figure 46: Impacts of AI use in the workplace as reported by employees 78

Figure 47: Employee reliance on AI at work 79

Figure 48: Preference for human–AI involvement in managerial decision-making 79

Figure 49: Perceived organizational support for AI and responsible AI use 81

Figure 50: Organizational support for AI and responsible use across countries 82

Figure 51: Perceived impact of AI on jobs 83

Figure 52: Demographic differences in trust and use of AI at work 87

Figure 53: Demographic differences in complacent use and positive impacts of AI 87

Figure 54: Industry differences in use of AI and organizational support for AI 88

Figure 55: Frequency of student use of AI compared to employee use of AI for work 90

Figure 56: Types of AI tools intentionally used for study, compared to employees 91

Figure 57: Inappropriate and complacent use of AI in education 92

Figure 58: Impacts of AI use in education as reported by students 94

Figure 59: Education provider support for responsible AI use as reported by students 95

Figure 60: Education providers’ guidance on generative AI use for students 95

© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **3**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

##### **Executive summary**

The release of ChatGPT in late 2022 brought the transformative power
of AI firmly into the public consciousness and everyday experience.
While exponential investment in AI predated its release, individual and
organizational use of AI has increased dramatically and rapidly since 2022. [1]
For example, OpenAI’s suite of generative AI tools obtained over 100 million
users in only two months. [2] AI is now firmly part of everyday life and work
for many people and is widely embraced across all sectors of the global
economy, including finance, education, transport, manufacturing, agriculture,
healthcare, retail, and media. [3]


The benefits and promise of AI for society and
business are undeniable. AI systems are being
used to make cancer detection faster and more
accurate, enhance the efficiency of renewable
energies, and drive productivity and innovation
in the workplace, among other impactful use
cases. [4] However, as AI’s capabilities and reach
become more apparent, so too has awareness
of the risks and challenges, raising questions
about the trustworthiness, regulation, and
governance of AI systems. The public’s trust in AI
technologies and its responsible and ethical use
is central to sustained acceptance and adoption
and in realizing the full societal and economic
benefits of these technologies.

Given the rapid advancement and widespread
adoption of AI technologies—and their
transformative effects on society, work, education,
and the economy—bringing the public voice into
the conversation has never been more critical.

This research aims to provide an evidence-based
understanding of people’s trust, use and attitudes
toward AI, their views on the impacts of AI, and
expectations of its governance and regulation.

The insights are important to inform public policy
and industry practice and a human-centered
approach to stewarding AI into work and society.
They can help policymakers, organizational leaders,
and those involved in developing, deploying, and
governing AI systems to understand and align


Now in its fourth iteration, the research captures
the views of more than 48,000 people from
47 countries, representing all global geographic
regions. It offers the most comprehensive
examination to date of public trust and attitudes
toward AI. In addition, it takes a deep dive into
how employees and students use AI in work
and education and their experience of the
impacts of AI in these specific settings.

with evolving public expectations, and deepen
understanding of the opportunities and challenges
of AI integration.

The report provides timely, global research
insights on a range of questions, including the
extent to which people trust, use, and understand
AI systems; how they perceive and experience
the benefits, risks and impacts of AI use in
society, at work and in education; expectations
for the management, governance and regulation
of AI by organizations and governments; how
employees and students are using AI for work
and study; and perceived support for the
responsible use of AI. It draws out commonalities
and differences in these key dimensions across
countries and sub-groups of the population, and
sheds light on how trust and attitudes toward AI
have changed over the past two years since the
widespread uptake of generative AI.

Next, we summarize the key research insights.


© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **4**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **5**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**The age of working with AI is here and is**
**delivering performance benefits, but also**
**mixed impacts**

Across countries, almost three in five employees
intentionally [5] use AI at work on a regular basis,
with almost a third using it weekly or more.
General-purpose generative AI tools are by far
the most widely used, with most employees
using free, public tools like ChatGPT rather than
tools provided by their employer. Three in four
report that their organization uses AI, with almost
half stating AI is used in a broad range of tasks
and functions.

Emerging economies [6] are leading workplace
adoption of AI, with employees in these economies
more likely to use AI regularly (72% vs 49%) than
those in advanced economies.

The use of AI at work is clearly delivering a range
of positive performance benefits. Most employees
report increased work efficiency, access to accurate
information, innovation, higher quality of work and
decisions, and better use and development of
skills and abilities. Almost half report that AI use
has increased revenue-generating activity.

However, employees also report mixed impacts
on workload, stress, human collaboration,
compliance, and surveillance at work. For example,
half say they use AI rather than collaborating with
peers or supervisors to get work done, and one
in five say AI use has reduced communication,
interaction and collaboration, raising the question
of how human connectivity will be retained in AIaugmented workplaces. These insights underscore
the importance of understanding and managing
the impacts of AI at work, ensuring appropriate
work design, and building employee capabilities
in effective human-AI collaboration.


**The responsible use and governance of**
**AI is not keeping pace with adoption:**
**many employees are using AI in**
**complacent and inappropriate ways**
**which increase risk**

While the rapid adoption of AI is delivering
benefits, many employees are using AI in
complacent and inappropriate ways, increasing
risks for organizations and individuals and raising
quality issues. For example, almost half admit to
using AI in ways that contravene organizational
policies and uploading sensitive company
information, such as financial, sales, or customer
information, to public AI tools. Three in five report
they have seen or heard of other employees
using AI tools in inappropriate ways. Two in three
report relying on AI output without evaluating the
information it provides, and over half say they
have made mistakes in their work due to AI.

What makes these risks even more challenging
to manage is that over half of employees avoid
revealing when they use AI to complete their work
and present AI-generated content as their own.
These findings highlight a lack of transparency and
accountability in the way AI, particularly generative
AI tools, are being used by employees at work.

This complacent use may be fueled by inadequate
training, guidance, and governance of responsible
AI use at work: within organizations that use
AI, only one in two employees in advanced
economies report that their organization offers
training in responsible AI, has policies and
practices on responsible AI use, or a strategy and
culture that supports AI. Despite the high use of
generative AI tools, only two in five say there is a
policy guiding its use. Complacent use may also
be exacerbated by a sense of pressure to use AI,
with half of employees feeling they will be left
behind if they don’t.

From a governance perspective, these findings
highlight a critical gap and urgent need for
organizations to proactively invest in responsible
AI training and the AI literacy of employees
to promote critical engagement with AI tools.
They also underscore the need to put in place
mechanisms to effectively guide and govern h _ow_
_employees use AI tools_ in their everyday work,
to promote greater accountability, transparency,
and employee engagement.


© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **6**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**Most students use AI and report benefits,**
**but inappropriate use and over-reliance**
**is widespread and challenging critical**
**skill development**

The findings for students (predominately tertiary
students) provide insight into how AI is being
used by the next generation of the workforce and
affecting education and training. Results mirror
those for employees but are more pronounced.
Four in five (83%) students regularly use AI in
their studies, with half using it weekly or daily.
The large majority use free, publicly available
generative AI tools.

Most students are deriving significant benefits
from AI use in education, such as increased
efficiency, access to information, quality of work,
idea generation and personalization of learning,
and reduced workload and stress. However, AI’s
influence on social dynamics, critical thinking,
and assessment is mixed. For example, a quarter
to a third of students report reduced critical
thinking and less communication, interaction,
and collaboration with instructors and peers.
A similar number perceive less trust of students
by instructors and peers, and reduced fairness
and equity of assessment due to AI.

The complacent use of AI by students is
widespread. Most students have used AI
inappropriately, contravening rules and guidelines
and over-relying on AI. Two-thirds have not
been transparent in their AI use, presenting
AI-generated content as their own and hiding
their use of AI tools. Only half regularly engage
critically with AI tools and their output.

The level of student dependence on AI is
concerning: over three-quarters have felt they
could not complete their work without the
help of AI and rely on it to do tasks rather than
learning how themselves. Four in five say they
put less effort into their studies and assessment
knowing they can rely on AI.

A lack of institutional support for responsible AI
use may be contributing to this problem: only half
of students report their education provider has
policies to guide responsible use of AI in learning
and assessment, or training and resources to
support AI understanding and responsible use.

These findings may have longer-term implications
for the effective development of essential skills—
such as critical thinking, communication and
collaboration, with implications for organizations
as these students enter the workforce.


**Trust in AI cannot be taken for granted:**
**many people are wary about trusting**
**AI systems, particularly in advanced**
**economies**

Despite high rates of individual adoption, trust
remains a critical challenge. Over half (54%)
of people are wary about trusting AI systems.
Underlying this average are differences between
economic groups: three in five people in
advanced economies are unwilling or unsure
about trusting AI systems. In contrast, in
emerging economies, three in five people trust
AI systems. We find similar levels for employee
trust in the use of AI at work, and student trust
of AI for educational purposes.

People are more skeptical about the safety,
security, and ethical use of AI systems and more
trusting of the technical ability of AI to provide
helpful output and services. This helps explain
individual use of AI to gain performance benefits,
despite trust concerns around its broader impact
on society and people. While the majority accept
the use of AI systems, most people report low
or moderate acceptance and approval levels.
People’s ambivalence toward AI is also reflected
in their emotions: the majority report optimism
and excitement, coupled with worry.

People have high confidence in universities,
research, and healthcare institutions to use and
develop AI in the best interests of the public,
and generally less confidence in government
to do so. People in advanced economies have
lower confidence in industry and big technology
companies to develop and use AI in the public
interest, whereas confidence in these entities
is high in emerging economies.

Organizations can build stakeholders’ trust in
their use of AI by investing in responsible AI
governance mechanisms that signal trustworthy
use: four in five people report they would
be more willing to trust an AI system when
assurance mechanisms are in place, such as
monitoring system reliability, human oversight and
accountability, responsible AI policies and training,
adhering to international AI standards, and
independent third-party AI assurance systems.


© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **7**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**People are experiencing a range of**
**benefits and negative outcomes from**
**the use of AI in society**

People’s ambivalence toward AI stems from the
mixed benefits, risks and negative impacts that
are being felt from AI use in society: 42 percent
believe the benefits outweigh the risks, 32 percent
believe the risks outweigh the benefits, and
26 percent believe the benefits and risks
are balanced.

Three in four report experiencing a broad
range of benefits, including improved efficiency
and effectiveness, enhanced accessibility to
information and services, greater precision and
personalization, improved decision-making and
outcomes, greater innovation and creativity,
reduced costs and better use of resources.
These outcomes benefit individuals, while
also bringing performance-oriented benefits
to organizations and society more broadly.

However, people’s experience of these benefits
is coupled with clear concerns about the risks
and negative impacts of AI on society. Four in
five people are concerned about—and two in
five have personally experienced or observed—
negative outcomes from AI. These include
the loss of human interaction and connection,
cybersecurity risks, loss of privacy or intellectual
property, misinformation and manipulation,
harmful or inaccurate outcomes, deskilling and
dependency, job loss, and disadvantage from
unequal access to AI. Comparatively fewer
people are concerned about AI bias resulting in
unfair treatment and the environmental impact of
AI, however even these outcomes are reported
by a third of people surveyed.

Respondents across countries share similar
views and experiences regarding AI risks and
negative outcomes, highlighting these as areas
of universal concern. These negative outcomes
are not just ‘perceived risks’ but harms that are
being experienced or observed by a significant
proportion of people across the 47 countries
surveyed. These findings reinforce the need for
international cooperation and coordinated action
to prevent and mitigate AI risks and negative
impacts. The challenge is doing this in a balanced
way that does not undermine progress or hinder
the innovation required to realize the many
societal benefits of AI.


**The public expect AI regulation at both**
**the national and international level.**
**Yet the current regulatory landscape**
**is falling short of public expectations.**

There is a strong public mandate for AI regulation
to mitigate the societal risks and negative
impacts of AI: Seventy percent of people believe
AI regulation is required, including the majority
in almost all countries surveyed. This broad
public consensus on the need for regulation
supports national and international efforts in many
jurisdictions to develop and implement regulatory
and governance frameworks to support the safe
and responsible use of AI.

However, the current regulatory landscape is
falling short of public expectations: only two in
five believe that the existing laws and regulation
governing AI systems in their country are
adequate. Most people are unaware of laws,
legislation or government policy that apply to AI.

These findings reflect that most countries
and jurisdictions are still in an early stage
of designing or implementing regulatory
approaches. While some countries have adaptive
legislation that may apply to AI (e.g. consumer
or privacy laws), such laws are absent or weakly
enforced in some jurisdictions. This suggests
the need to clarify, develop or strengthen such
legislation where it is lacking and to educate
and raise public awareness of applicable laws.
The importance of effective, fit-for-purpose
regulation—and awareness of such regulation—
is underscored by our finding that the perceived
adequacy of AI regulation is a key predictor of
trust and acceptance of AI systems.

The majority of people expect a multipronged
national and international regulatory approach
to AI, with international laws and regulation the
most endorsed form of regulation and supported
by a clear majority in all countries. National
government regulation or a co-regulatory
approach between government and industry is
preferred in most countries over self-regulation
by industry or an independent AI regulator.
This highlights the public’s expectation that
government takes a central role in ensuring
effective governance and regulation of AI, as
well as the expectation that industry will work
with regulatory bodies and proactively align
their governance approach with the evolving
regulatory landscape.


© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **8**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

There is also a clear mandate for stronger
regulation of AI-generated misinformation:
87 percent of respondents want laws to combat
AI-generated misinformation and expect social
media and media companies to implement
stronger fact-checking processes and methods
that enable people to detect AI-generated
content. Our findings indicate that AI generated
misinformation is a key concern globally and is
undermining trust in online content and raising
concerns about the integrity of elections.

**AI literacy is lagging AI adoption yet is**
**critical for responsible and effective use**

Although AI tools are being widely used by the
public, employees and students, AI literacy
remains limited; about half of respondents say
they don’t feel they understand AI nor when or
how it is used. Half of respondents are unaware
that AI underpins common applications such as
social media, despite 90 percent saying they
use such platforms. This knowledge gap reflects
that only two in five people report any AI-related
training or education.

Despite low rates of knowledge and training,
three in five say they can use AI effectively.
This likely reflects the easily accessible
interfaces of many AI systems (e.g. using
natural language) and low barriers to use.
While this accessibility has benefits, it also
risks fostering complacency and overreliance
if not accompanied by meaningful levels of
understanding and literacy.

AI literacy is higher in emerging economies,
where three-quarters believe they can use
AI effectively, compared to half in advanced
economies, and half report AI training or education
compared to a third in advanced economies.

AI literacy consistently emerges in our findings
as a cross-cutting enabler: it is associated with
greater use, trust, acceptance, and critical
engagement, and more realized benefits from
AI use including more performance benefits in
the workplace.

The pattern of findings underscores that AI
literacy and training in responsible use is not only
a personal skillset, but can also be a strategic
capability for organizations and societies alike,


enabling people to recognize and seize the
capabilities of AI while recognizing their limitations
and guarding against harm. Investing in AI literacy
is a critical component of ensuring AI is used
safely, ethically, and to its full potential.

**There are notable differences between**
**countries with advanced and emerging**
**economies: People in emerging**
**economies report greater trust,**
**acceptance and adoption of AI, higher**
**levels of AI literacy, and more realized**
**benefits from AI**

One of the most striking insights from the survey
is the stark contrast in use, trust, and attitudes
toward AI between people in advanced and
emerging economies.

People in emerging economies report higher
adoption and use of AI both at work and for
personal purposes, are more trusting and
accepting of AI, and feel more positive about
its use. They report higher levels of AI training
and literacy, are more likely to expect and
realize the benefits of AI, and view AI benefits
as outweighing the risks. They are also more
confident in the development and use of AI by
commercial organizations and big technology
companies and more likely to view current
AI regulation and safeguards as adequate,
compared to people in advanced economies.
These differences hold even when controlling
for the effects of age and education.

These findings suggest that many countries
with emerging economies are leading the
way in terms of AI adoption. [7] In particular, six
countries with emerging economies strongly
and consistently show this pattern—India, China,
Nigeria, the UAE, Saudi Arabia and Egypt. Of the
advanced economies, Norway, Israel, Singapore,
Switzerland and Latvia have comparatively high
levels of AI adoption, trust, acceptance, and
positive attitudes toward AI.


© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **9**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

An implication is that these countries may
be uniquely positioned to rapidly accelerate
innovation and technological advantage through
AI. This has implications for global competitive
dynamics and may create shifts in the economic
landscape across countries in the future as AI
becomes a more prominent driver of productivity
and economic activity.

**Pathways to support the trusted and**
**responsible adoption of AI**

Our modeling supports four distinct yet
complementary pathways to trusted and
sustained AI adoption: a _knowledge pathway_
reflecting the importance of supporting people’s
AI literacy and efficacy through AI training and
education; a _motivational pathway_ reflecting
the importance of deploying AI in a humancentric way that delivers benefits to people;
the _uncertainty reduction pathway_ reflecting
the need to address concerns about the risks

associated with AI, and an _institutional pathway_
reflecting the adequacy of current safeguards,
regulation and laws to promote safe AI use,
and confidence in entities to develop and use
AI in the public interest.

Of these drivers, the institutional pathway
had the strongest influence on trust, followed
by the motivational pathway. This model also
holds at the organizational level where the
institutional pathway reflects appropriate levels
of organizational governance, strategy, and
training to support AI and its responsible use.

**AI adoption has increased markedly**
**since 2022, but trust in AI has declined**
**and worry has increased**

Our research program provided the unique
opportunity to compare data from the current
survey with our previous survey data collected from
17 countries in late 2022, just prior to the release
of ChatGPT. This comparison revealed a trend
of less positive attitudes toward AI, as adoption
has increased.

As expected, adoption of AI in the workplace
increased dramatically in all 17 countries: employee
reported organizational use of AI increased from
34 percent to 71 percent, and employees’ use of AI
at work increased from 54 percent to 67 percent.
The largest increases occurred in Australia,
Canada, the USA, and the UK.


However, this increased adoption is coupled with a
trend toward people feeling more concerned about
and less trusting of AI. People’s perceptions of the
trustworthiness of AI systems and their willingness
to rely on AI declined in most countries, as did
employee trust of AI at work in some countries.
This decline in trust likely reflects that increased
use and exposure, particularly to general-purpose
generative AI tools, has increased awareness of
both the capabilities and benefits of these tools,
and also their limitations and potential negative
impacts (e.g. hallucinations), prompting more
considered trust and reliance.

More people report feeling worried about AI
and concerned about the risks, and fewer view
the benefits of AI as outweighing the risks. For
example, in Brazil half of people reported feeling
worried about AI in 2022 compared to 75% in
2024, and the view that the benefits of AI outweigh
the risks fell from 71% to 44%. Excitement also

dampened over this time in several countries.

With this increase in concern, the importance of
organizational assurance mechanisms as a basis
for trust increased in all countries, suggesting
a greater need for reassurance that AI is being
used in a trustworthy and responsible way.

Attitudes toward the regulation of AI remained
stable and there was no overall change to the
perceived adequacy of regulation and laws.

Despite the rapid uptake of AI, we found no
discernible change in the public’s self-reported
understanding of AI, or their objective awareness
of AI use in common applications.

This pattern of findings suggests that the hype
of AI may be giving way to a more realistic and
measured assessment of AI’s capabilities and
limitations, benefits and risks, and heightened need
for reassurance around the trustworthy deployment
of AI and proactive mitigation of AI risks.

Collectively, the survey insights provide
evidence-based pathways for strengthening the
responsible use of AI systems and the trusted
adoption of AI in society and work. These
insights are relevant for informing responsible
AI strategy, practice and policy within business,
government, and education at a national level,
as well as informing AI guidelines, policy
and regulation at the international and pangovernmental level.


© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **10**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

##### **Introduction**

The motivation for this research is to provide an evidence-based understanding
of public trust, attitudes, and experiences of AI, and expectations of its
governance and regulation, as a resource to inform public policy and industry
and government practice.

Given the rapid advancement, widespread deployment and transformative
impact of AI technologies, it is important to regularly examine public trust,
attitudes, and expectations of AI. Equally important is documenting how
people use AI technologies and experience the impacts of AI in their lives,
work, and studies, and the implications this may have for organizations,
education providers, and society at large. To date, there has been limited
empirical insight addressing these critical issues, underscoring the relevance
of this research in promoting human-centered AI that meets evolving
societal needs and expectations.


This is the fourth survey in our program of
research examining public trust and attitudes
toward AI. Our current report examines the
perspectives of over 48,000 people from
47 countries covering all global geographic
regions, using nationally representative
sampling of the adult population based on
age, gender, and regional distribution. Taking
a global perspective is crucial, given that AI
systems are not bound by physical borders
and are rapidly being deployed and used
across the world.

Our program of research provided the unique
opportunity to benchmark and compare the
findings in this report to our previous survey
data collected from 17 countries in late 2022,
just prior to the release of ChatGPT. We
examine changes in public trust and attitudes
over time in these 17 countries and highlight
changes where relevant throughout the report i
(see ‘How we conducted the research’ for

more details).



© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **11**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

Our research insights are structured in three
sections. The first focuses on AI use broadly in
society examining the public’s use, understanding,
trust, attitudes and experience of AI systems and
their impact on society. These insights are based
on all respondents answering survey questions
asked about AI systems in general, as well as AI
use in the context of three common applications
which are likely to be used by or impact many
people: generative AI systems, AI in healthcare,
and AI in Human Resource applications.

In the second section, we delve deeper
into understanding how employees use and
experience AI impacts in the workplace. In the
third section, we examine student use of AI and
their perceptions of how AI impacts education.

Together, these sections provide evidence-based
insights on the following questions:

- To what extent do people use and understand

AI systems?

- To what extent do people trust and accept

AI systems?

- How do people view and experience the

benefits and risks of AI?

- What do people expect from the regulation

and governance of AI?

- What are the key drivers of AI trust and

acceptance in society?

- How is AI being used at work and with what

impacts?

- How is AI being used by students and with

what impacts?

The final section draws out the key conclusions
and implications from these insights for industry,
government, and the education sector. We next
outline the research methodology.

© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **12**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

##### **How the ** **research was
** **conducted**

**How the data was collected**

Data was collected in each country between
November 2024 and mid-January 2025 using
an online survey.

Countries were selected based on three criteria:

1) representation across global regions;
2) leadership in AI activity and readiness, [10] and
3) diversity on the Responsible AI Index. [11]
The sample size in each country ranged from
1,001 to 1,098 respondents.

Analysis of the data revealed a distinct pattern
of findings across countries with emerging
and advanced economies. We adopted the
International Monetary Fund’s (IMF) classification
of advanced and emerging economies. The
emerging economies surveyed are Argentina,
Brazil, Chile, China, [12] Colombia, Costa Rica,
Egypt, Hungary, India, Mexico, Nigeria, Poland,
Romania, Saudi Arabia, South Africa, Türkiye,
and UAE.

Surveys were conducted in the native language(s)
of each country with the option to complete
in English, if preferred. To ensure question
equivalence across countries, surveys were
professionally translated and back translated
from English to each respective language, using
separate translators. See Appendix 1 for further
method details.

© 2025 The University of Melbourne.

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

## **48,340**

people completed the survey
across 47 countries and

jurisdictions, covering all
global geographical regions [8] :

1. North America (Canada, United

States of America [USA])

2. Latin America and Caribbean

(Argentina, Brazil, Chile, Colombia,
Costa Rica, Mexico)

3. Northern and Western Europe

(Austria, Belgium, Denmark, Estonia,
Finland, France, Germany, Ireland,
Latvia, Lithuania, Netherlands,
Norway, Sweden, Switzerland,
United Kingdom [UK])

4. Southern Europe (Greece, Italy,

Portugal, Slovenia, Spain)

5. Eastern Europe (Czech Republic,

Hungary, Poland, Romania, Slovakia)

6. Africa (Egypt, Nigeria, South Africa)

7. Western Asia (Israel, Saudi Arabia,

Türkiye, United Arab Emirates [UAE])

8. Eastern, Southern and Central Asia

(China, [9], India, Japan, Republic of
Korea, Singapore)

9. Oceania (Australia, New Zealand)

Trust, attitudes and use of AI: A global study 2025 | **13**

**The 47 countries surveyed**


**Argentina**

**Australia**

**Austria**

**Belgium**

**Brazil**

**Canada**

**Chile**

**China**

**Colombia**

**Costa Rica**

**Czech Rep.**

**Denmark**


**Egypt**

**Estonia**

**Finland**

**France**

**Germany**

**Greece**

**Hungary**

**India**

**Israel**

**Italy**

**Ireland**

**Japan**


**Latvia**

**Lithuania**

**Mexico**

**Netherlands**

**New Zealand**

**Nigeria**

**Norway**

**Poland**

**Portugal**

**Republic**
**of Korea**

**Romania**

**Saudi Arabia**


**Singapore**

**Slovakia**

**Slovenia**

**South Africa**

**Spain**

**Sweden**

**Switzerland**

**Türkiye**

**United Arab**
**Emirates**

**United**
**Kingdom**

**United**
**States of**
**America**


© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **14**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**Who completed the survey?**

Representative research panels were used
to ensure the people who completed the
survey are representative of the population. [13]
This approach is common in survey research.

Samples were nationally representative of
the adult population on gender, age and regional
distribution matched against official national
statistics. In select countries, full representation
on these criteria was not obtainable (see Appendix 2
for further details on country sampling).

Across the total sample, the gender balance
was 51 percent women, 49 percent men and
<1 percent other gender identities. The mean age
was 46 years and ranged between 18 and 95 years.
Half the sample (51%) had a university education
and 20 percent a vocational or trade qualification.



**51%** **49%** **<1%**

Women Men Other genders


The sample represented the full range of income
levels, with the majority (72%) reporting middle
incomes (see Appendix 1 for details of the
income measure). [14]

Sixty-seven percent of respondents were currently
working full-time or part-time. These respondents
represented the diversity of industries and
occupational groups listed by the OECD and
International Labor Organization [15] and included
employees of small, medium, and large organizations,
business owners, and people who were selfemployed (e.g. sole traders and freelancers).

Five percent of respondents were students, with
the majority tertiary students enrolled in university
education (65%) or a vocational, trade or technical
program (16%), and the remainder in secondary
education (18%).

Further details of the sample representativeness,
including the demographic profile for each country
sample, are shown in Appendix 2.

**12%** **38%** **32%** **18%**

18-24 25-44 45-64 65-95


**Education**

**2%** **4%** **23%** **20%** **37%** **14%**
Primary Some secondary Secondary Vocation/trade Undergraduate Postgraduate


**15%** **72%** **13%**

Low Middle High


**52%** **15%** **28%**

Working full time Working part time Not working


**32%** **21%** **14%** **10%** **1%**
Professional & skilled **[22%]** Manager Administrative Manual Services & Sales Other


**77%** **7%** **16%**
Employed by Business owner Self-employed Small **[26%]**
an organization with employees (2-49 employees )


**32%**
Small **[26%]** Medium
(2-49 employees )


Large
(250+ employees)


**32%** **42%**

Medium Large
(50-249 employees)


**18%** Secondary education **[16%]** Vocation or trade **54%** Bachelor’s or equivalent **11%** Postgraduate **1%** Other

© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **15**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**How we asked about AI**

After asking a series of questions about
respondents’ understanding of AI, the following
description of AI, adapted from the OECD
definition, [16] was provided: _Artificial Intelligence_
_(AI) refers to machine-based systems that infer_
_from the input they receive and objectives_
_provided, how to generate outputs such as_
_predictions, content, recommendations, or_
_decisions. Different AI systems vary in their_
_levels of autonomy and adaptiveness._

As attitudes toward AI systems may depend on
their purpose and use, survey questions that
asked about the use of AI systems in society
referred to one of four AI use cases (randomly


allocated, see below): Generative AI (used
to create output and content in response to
user prompts); Healthcare AI (used to inform
decisions about how to diagnose and treat
patients); Human Resources AI (used to inform
decisions about hiring and promotion); and AI
systems in general.

These use cases were selected to represent
AI applications that are widely and increasingly
used and can impact many people, and were
developed based on expert input. Respondents
were provided with a description of the AI
use case allocated to them, before answering
questions related to AI systems.





© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **16**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**How the data was analyzed**

Statistical analyses were conducted to examine
differences between countries and economic

groups (e.g. countries with advanced and
emerging economies, as classified by the
IMF), and demographic factors (e.g. gender,
age, education, income, occupation). Relevant
differences are reported when statistically
significant and meaningful. Correlational analyses
and statistical models indicate associations

between concepts and do not infer causality.
Further details of the statistical procedures are
discussed in Appendix 1. An overview of key
indicators for each country sample are shown
in Appendix 3.

**How changes in trust, use and attitudes**
**over time were assessed**

To understand how trust, use, and attitudes
toward AI have shifted over time, a selection
of questions was asked in the same way in
the 2022 and 2024 surveys.

The 2022 survey included 17 countries:
Australia, Brazil, Canada, China, Estonia,
Finland, France, Germany, India, Israel, Japan,
Netherlands, Singapore, South Africa, Korea,
the UK, and the USA. [17]

While the samples collected in 2022 and 2024
are based on the same methodology and sample
representativeness, they are independent of each
other. As such, our analyses examine general
trends rather than a longitudinal analysis of the
same respondents over time. Relevant insights
on these changes are highlighted in call-out
boxes throughout the report (for an overview,
see Appendix 4).

© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **17**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

~~**SECTION**~~ ~~**ONE**~~
#### **Public attitudes ** **towards AI**



**To what extent do people use**
**and understand AI systems?**

To contextualize the findings and provide an indicator of overall public
adoption of AI and AI literacy levels, we first examine people’s use and
understanding of AI systems and how this varies across countries.
To identify levels of AI literacy, survey participants self-reported their
level of AI knowledge and efficacy together with AI-related education
and training. They were also asked about their objective understanding
of AI use in common technologies and interest in learning more about AI.

In subsequent sections of the report, employees’ and students’ use of AI
at work and for educational purposes are examined in more detail, together
with organizational support for AI literacy.


**Public adoption of AI is high: Two in**
**three people report intentional regular**
**use of AI tools for either personal,**
**work, or study purposes**

People were asked to report how often
they _intentionally_ use AI tools, clarifying
that this use is different from the passive
use of AI (e.g. when AI operates behind
the scenes in tools such as email filters
and search engines).

Two thirds of people (66%) report
intentionally using AI on a regular basis
for personal, work, or study reasons. As
shown in Figure 1, two in five (38%) people
report using AI on a weekly or daily basis,
whereas just over a quarter (28%) use AI
semi-regularly (i.e. every month or every
few months). One-third (34%) rarely or
never intentionally use AI.

Three in five (59%) use AI at least semiregularly for personal purposes, with those
not working or studying much less likely to
use AI (only 37%). Three in five (58%) people
who work intentionally use AI regularly for
work purposes, while four in five (83%)
students regularly use AI in their studies.


This high level of adoption reflects the
ease with which AI systems—particularly
general-purpose generative AI tools—can
be accessed and used by a diverse range
of people and applied to a broad variety of
tasks. This sets AI apart from many other
advanced technologies that have greater
barriers and constraints on access and

use by individuals.
# **38%**

of people report
using AI on a weekly
or daily basis.


© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **19**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**Figure 1: Frequency of intentional use of AI tools for personal, work,**
**or study purposes**

‘In your personal life (work/studies), how often do you intentionally use AI tools, including generative
AI tools?'

% Overall AI use







Never Few times Every few Monthly Weekly Daily
a year months

Daily = ‘most days’ or ‘multiple times a day’


**There are notable differences across**
**countries in people’s adoption of AI, with**
**emerging economies leading the way**

There is a distinct pattern of findings between
countries with advanced and emerging
economies, with the use of AI tools notably
higher in countries with emerging economies.
On average, four in five (80%) people in emerging
economies intentionally use AI tools on a regular
or semi-regular basis, compared to three in five
(58%) in advanced economies.


As shown in Figure 2, levels of AI use in most
emerging economies exceed 70 percent of the
population, with India and Nigeria reporting the
highest regular or semi-regular usage (92%). Two
emerging economies located in Eastern Europe—
Hungary and Romania—have notably lower AI
use compared to the other emerging economies.

In contrast, AI use levels in most advanced
economies fall below 70 percent of the population,
with the lowest usage reported in the Netherlands
(43%) and the highest in Singapore (73%).


© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **20**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**Figure 2: Regular use of AI systems across countries**

% Advanced Economy % Emerging Economy

**Overall**

**India**

**Nigeria**

**United Arab Emirates**

**Egypt**

**China**

**Saudi Arabia**

**Costa Rica**

**South Africa**

**Brazil**

**Türkiye**

**Mexico**

**Argentina**

**Colombia**

Singapore

**Chile**

Latvia

Norway

Estonia

Lithuania

**Poland**


**66**

**77**

**75**

**74**

**73**

**72**

**72**

**71**

**70**

**69**

**69**

**68**

**66**

**65**


**92**

**92**

**91**

**90**

**89**

**88**

**87**


**83**

**82**

**81**


Israel

Slovenia

Switzerland

Portugal

**Romania**

Greece

Korea

Italy

Spain

Denmark

Ireland

Finland

Austria

Slovak Republic

United States of America

United Kingdom

France

Germany

Australia

Canada

**Hungary**

Japan

New Zealand

Czech Republic

Sweden

Belgium

Netherlands


**43**


**63**

**62**

**61**

**61**

**60**

**60**


**57**

**56**

**55**

**53**

**53**

**53**

**52**

**51**

**51**

**50**

**50**

**50**

**50**

**50**

**49**

**49**

**47**


% Using AI on a semi-regular
or regular basis: 'every few
months’, ‘monthly’, ‘weekly’
or ‘daily’


© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **21**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**Most people have no AI training and half**
**don’t feel they understand AI, yet 3 in 5**
**believe they can use AI effectively**

Despite high levels of adoption, the majority of
people report they have not received any form of
AI training or education. Only two in five (39%)
report some form of AI training, such as workbased AI training, formal or informal AI training
outside of work, or completing a university-level
course related to AI (such as computer science
or data analytics; see Figure 3).

In line with these low levels of AI training,
almost half (48%) report limited knowledge
about AI, indicating that they do not feel they
understand AI nor when or how it is used. [18]
As shown in Figure 4, only one in five people
report high levels of knowledge, and about a
third report a moderate level.

Despite low levels of AI education, training and
knowledge, 60 percent of people believe they
can use AI effectively. This includes their ability to
choose, use and communicate with AI systems
to support everyday activities, and evaluate the
accuracy of AI output (see Figure 5). This is likely
because many AI tools and systems are designed

**Figure 5: Self-reported AI efficacy**

‘To what extent do you agree with the following? I can…’


**Figure 3: AI-related training or education**

% AI training

**39**

% No AI training

**61**

**Figure 4: Self-reported AI knowledge**

‘To what extent do you...
(a) Feel you know about AI?
(b) Feel informed about how AI is used?
(c) Think you understand when AI is being used?
(d) Feel you have the skills and knowledge necessary
to use AI tools appropriately?’



% Low

% Moderate

**48**
% High


% Low = 'Not at all' or 'To a small extent’

% High = ‘To a large extent' or 'To a very large extent'

to be intuitive to use and accessible to a broad

range of people (via a mobile phone application,
for example, and by using natural language to
make requests), enabling these tools to be used
widely with limited or no training. For example, AI
voice assistants can be used simply by conversing
with these tools.


% Disagree % Neutral % Agree

Skillfully use AI applications to help with daily work

or activities

% Disagree = 'Strongly disagree', 'Disagree', 'Somewhat disagree'
% Agree = 'Somewhat agree', 'Agree', 'Strongly agree'

© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **22**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**AI training, knowledge, and efficacy**
**are lowest in the advanced economies**

In line with the distinct differences in the use

of AI across economic groups, there are also
pronounced differences between advanced and
emerging economies when it comes to levels of
AI training, knowledge, and efficacy.

As shown in Figure 6, half of the people surveyed
in emerging economies report having completed
AI-related training or education, compared
to less than a third in advanced economies.

Similarly, almost two-thirds of people in emerging
economies report moderate or high knowledge
about AI, compared to less than half in advanced
economies. Around three-quarters of those in the
emerging economies feel they can use AI tools
and systems effectively, compared to only half in
advanced economies.


As shown in Figure 7, AI training, knowledge,
and efficacy are particularly high in Nigeria,
Egypt, the UAE, India, China and Saudi Arabia.
These six countries also rate highest on AI
use (see Figure 2). In contrast, AI training and
knowledge are particularly low in Germany,
the Czech Republic and Japan.


**Figure 6: AI training, knowledge and AI efficacy across economic groups**










© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **23**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**Figure 7: AI knowledge, efficacy, and training across countries**

AI knowledge AI efficacy AI training

20% 25% 30% 35% 40% 45% 50% 55% 60% 65% 70% 75% 80% 85%

**Nigeria**
**Egypt**

**United Arab Emirates**

**India**

**China**

**Saudi Arabia**

**Cost Rica**

**South Africa**

**Colombia**

Lithuania

**Argentina**

**Brazil**

**Mexico**

Estonia

Switzerland

Singapore

Slovenia

Chile

Norway

Israel

Spain

Latvia

Korea

Greece

**Türkiye**
Italy

Denmark

**Romania**

Portugal

Ireland

Finland

**Poland**

Austria

USA

United Kingdom
Slovak Republic

Sweden

New Zealand

Netherlands

France

Canada

Belgium

Australia

Japan
Czech Republic
Germany
**Hungary**

% AI knowledge = ‘% To a moderate extent’, ‘% To a large extent’, ‘% To a very large extent’
% AI efficacy = ‘% Somewhat agree’, ‘% Agree’, ‘% Strongly agree’
% AI training = ‘% Selected University level course in AI’, ‘% Selected Work-based training’,
or ‘% Selected Formal or informal training outside work’
Bolding indicates countries with emerging economies. Ordered by AI training.

© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **24**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**A third are unaware that AI enables**
**common applications they use: half**
**don’t know AI is used in social media**

As an indicator of people’s objective awareness
of AI use, respondents were asked if they use
the three common technologies shown in Figure
8, and whether these technologies are enabled
by AI (i.e. whether these technologies rely on
AI to function). Seventy-nine percent of people
use these common AI-enabled technologies—
highlighting the prevalence of AI technologies
in people’s lives—but over a third (36%) are
unaware that these technologies use AI.

Use of the technology does not necessarily
translate into an increased understanding of
whether AI is part of it. For example, while the


majority (90%) of the sample reports using social
media, nearly half (47%) of all respondents are
unaware of AI’s role in social media. As shown in

Figure 8, this pattern of using technology without
realizing it relies on AI is particularly strong for
social media, but also evident in facial recognition
and virtual assistants—prompting the question of
whether the awareness of AI’s central role in these

technologies would change how people engage
with them.

People in emerging economies are more likely
to be aware that AI is used in these technologies
than those in advanced countries (70% vs.
61%), and they are also more likely to use these
common AI-enabled technologies (88% vs. 74%).


**Figure 8: Use of common technologies and awareness that they involve AI**

‘For each technology below, please indicate if you have used it and if it uses AI’

% Unaware this technology uses AI % Who use this technology



**Overall**

Social media




Virtual

assistants

Facial
recognition






**Self-reported understanding of AI has not changed over time and many are still**
**unaware that AI is used in common applications like social media**

Despite the rapid uptake of AI since 2022, there has been no overall substantive change in selfreported knowledge of AI (M=2.6 in 2022; M=2.6 in 2024). However, increases were found in
four countries, Estonia, Brazil, China and South Africa, with the largest increases in Estonia (26%
vs. 50%, M=2.1 vs. 2.8) and Brazil (38% vs. 63%, M=2.5 vs. 3.0).

Although use of AI in common technologies such as social media, facial recognition, and virtual
assistants has tended to remain constant or increased in most countries, many are still unaware that
these technologies rely on AI to function. For example, social media use has remained constant and
high over time across countries (88% use at both time points), yet many are still unaware that
AI is used in social media platforms (2022: 44% vs. 2024: 46%).

© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **25**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**Four in five want to learn more about**
**AI, with interest highest in emerging**
**economies**

Most people (83%) are interested in learning
more about AI, ranging from almost all (97%)
in Nigeria to three in five (59%) in Australia.

In most emerging economies, over 90 percent
of people express a desire to learn more about
AI. In contrast, respondents in seven advanced
economies (Australia, New Zealand, the USA,
Canada, the UK, Japan and Finland) have
considerably lower interest (ranging from 5967%), compared to other countries. Australia
and Finland are notably low, with two in five
(41%) people reporting no or low interest in
learning more about AI.

~~**I**~~ **n summa** ~~**ry**~~


People with AI knowledge and efficacy tend
to be more interested in learning more about
AI (r=.48), suggesting a virtuous cycle where
those who are already knowledgeable and
confident in using AI are more eager to
learn and thus more likely to deepen their
understanding further. In contrast, those
with low knowledge and efficacy may fall
further behind.

In most emerging
economies, over
# **90%**

of people express a desire
to learn more about AI


Taken together, these findings indicate high rates of AI adoption by the
public, coupled with comparably low levels of AI training and literacy. Low
levels of AI literacy may limit people’s ability to recognize the capabilities
and applications of AI and thus fully realize benefits, and importantly, the
ability to recognize the limitations of AI systems, critically evaluate their
outputs, and guard against harm. For instance, social media users that
are unaware of how algorithms shape content may fail to question the
credibility or biases of algorithmically curated content and face increased
vulnerability to misinformation and manipulation.

The findings also reveal accelerated uptake of AI tools and higher levels
of AI literacy amongst people in emerging economies compared to
advanced economies. This may be explained in part by the increasingly
important role that emerging and transformative technologies play in
the economic development of these countries. [19] As discussed in the
next sections, people in emerging economies also tend to be more
trusting, accepting, and positive about AI and experience the most
benefits from its use, compared to those in advanced economies.

© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **26**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**To what extent do people trust**
**and accept AI systems?**

To answer this question, respondents were asked about their trust and
acceptance of a range of AI systems, and the extent to which they perceive
them to be trustworthy. They were also asked about the emotions they feel
when it comes to AI applications.

Our approach to measuring trust in AI aligns with the following common
definition of trust: a _willingness to be vulnerable_ to an AI system (e.g. by
relying on system recommendations or output or sharing personal data)
based on _positive expectations_ of how the system will operate (such as
accuracy, helpfulness, data privacy and security). [20 ]


**People have more trust in the technical**
**ability of AI systems to provide a helpful**
**service but are more skeptical of its**
**safety, security and impact on people**

While most people use AI tools, many people
have reservations about the trustworthiness of

AI systems and their use in society.

On average, 58 percent of people view AI
systems as trustworthy. [21] People have more faith
in the technical ability of AI systems to provide
accurate and reliable output and services (65%)
than in their safety, security, impact on people,
and ethical soundness (e.g. that they are fair,
do no harm, and uphold privacy rights; 52%).

This difference is consistent across countries,
as shown in Figure 9. To illustrate, in Finland—a
country where trustworthiness is very low—half
of the respondents view AI systems as providing
a helpful service, yet only a third agree that
these systems are safe and secure to use. By
contrast, in Egypt—where AI is perceived as highly
trustworthy—83 percent believe AI systems are
accurate and provide a helpful service, while 72
percent agree that they are safe and secure to use.


Trust is important because it underpins the
acceptance and sustained adoption of AI.
This is confirmed by our research: trust is
associated with the acceptance and approval
of AI systems (r=.70) and the use of AI (r=.48).
People who trust AI systems are more likely
to use them frequently.

**How trust in AI was measured**

To understand how people view the
trustworthiness of AI systems, we asked about
two key components: the _technical ability_ of AI
(e.g. to provide accurate and reliable output and a
helpful service), and _safe and ethical use_ (e.g. to
be safe and secure to use and ethically sound).

We also examined two primary ways people
demonstrate trust in AI systems: _Reliance_
assesses people’s willingness to rely on an AI
system’s output, such as a recommendation
or decision (i.e. to trust that it is accurate).
_Information sharing_ relates to the willingness
to share information or data with an AI system
(e.g. to provide personal information to enable
the system to work or perform a service).


© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **27**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**Figure 9: Perceptions of the trustworthiness of AI systems**

Perceived trustworthiness Ability Safe and ethical use

35% 40% 45% 50% 55% 60% 65% 70% 75% 80% 85% 90%

**Nigeria**

**India**

**China**

**Egypt**
**Türkiye**

**United Arab Emirates**

**Saudi Arabia**

**South Africa**

**Brazil**

**Costa Rica**

**Mexico**

Singapore

**Romania**

**Chile**

**Colombia**

Spain
Norway
**Hungary**

Lithuania

Latvia

**Argentina**
Italy

Korea

Switzerland

**Poland**

Estonia

Portugal

Slovenia

Greece

United Kingdom

Israel

Czech Republic
Japan

Ireland

Denmark

USA

Belgium

Austria

Germany
Slovak Republic

France

Australia

Canada

Sweden

New Zealand

Netherlands

Finland

% Agree = 'Somewhat agree', 'Agree', 'Strongly agree'. Ordered by perceived trustworthiness.
Bolding indicates countries with emerging economies.

© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **28**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**Most people are ambivalent or unwilling**
**to trust AI systems but accept their use**

The concern about the safety and security of AI
and its impact on people helps explain why a little
over half (54%) of people are wary about trusting
AI systems, reporting either ambivalence or an
unwillingness to trust (see Figure 10). Only 46
percent are willing to trust AI systems.

As people’s trust in AI may vary depending on the
application of AI, we asked about trust in different
AI use cases. As shown in Figure 10, there are
similar levels of trust in generative AI tools, AI use
in Human Resources, and AI systems in general
(42-45% are willing to trust, Ms=3.9-4.0).

One difference is that people are more trusting
of AI use in healthcare (52% willing, M=4.3),
with healthcare the most trusted application in
42 of the 47 countries surveyed (see Figure 11).


This difference likely reflects the direct benefit
that increased precision of medical diagnoses
and treatments affords people, combined
with generally high levels of trust in medical
professionals in most countries. [22] These findings
reinforce that people’s trust of AI systems is
contextual and can depend on the use case
and their confidence in the organization that is
deploying the AI system.

Most people report low or moderate acceptance
and approval of the use of AI systems (see
Figure 10), with moderate acceptance indicating
a level of ambivalence in their acceptance of AI
use. In contrast, a third report high acceptance
and approval. Taken together, these findings
show that the majority (72%) have at least some
level of acceptance of AI use.


**Figure 10: Trust and acceptance of AI systems**

‘How willing are you to trust AI [specific application]?’

% Unwilling to trust % Ambivalent % Willing to trust


**Trust in AI overall**


~~**35**~~ **19** **46**


AI in general 36 19 45

Generative AI 37 19 44

Human Resources AI 39 19 42

Healthcare AI 30 18 52

% Unwilling to trust = 'Somewhat unwilling', 'Unwilling', or 'Completely Unwilling'
% Ambivalent = 'Neither willing nor unwilling'
% Willing to trust = 'Somewhat willing', 'Willing', or 'Completely willing'

‘To what extent do you accept/approve the use of AI [specific application]?’

%Low acceptance %Moderate %High acceptance

Acceptance ~~28~~ ~~39~~ 33

% Low acceptance = 'Not at all' or 'Slightly'
% High acceptance = 'Highly' or 'Completely'

© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **29**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**Figure 11: Trust in AI applications across countries**

AI Generative AI Human Resources AI Healthcare Al

20% 25% 30% 35% 40% 45% 50% 55% 60% 65% 70% 75% 80%

**Nigeria**

**India**

**Egypt**

**China**

**United Arab Emirates**

**Saudi Arabia**

**South Africa**

**Türkiye**

**Brazil**

**Hungary**
Norway

**Costa Rica**

Spain

Israel

**Mexico**

Singapore

Latvia

Switzerland

Greece

Estonia

**Argentina**

**Romania**

**Colombia**

**Chile**

Korea

United Kingdom

Denmark

**Poland**

USA

Italy

Austria

Slovenia

Ireland

Portugal

Sweden

Australia

Slovak Republic
Belgium

Lithuania

New Zealand

Canada

Netherlands

France

Germany
Czech Republic
Japan

Finland

% Willing to trust based on ‘Somewhat willing’, ’Mostly willing’ and ‘Completely willing’. Ordered by % Willing.
Bolding indicates countries with emerging economies.

© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **30**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**Trust and acceptance of AI is lower in**
**advanced economies**

As shown in Figure 12, trust and acceptance of
AI systems are consistently lower in advanced
economies compared to emerging economies.
In advanced economies, two in five are willing to
trust AI systems by relying on their output and
sharing information with these systems. Half view
AI systems as trustworthy, and two-thirds report
at least moderate levels of acceptance.

In contrast, people in emerging economies have
more trust in AI systems, view them as more
trustworthy, and have higher levels of acceptance
and approval of their use. It is notable, however,
that 43 percent of people in emerging economies
remain ambivalent or unwilling to trust AI
systems, highlighting that trust cannot be taken
for granted.

To illustrate this distinction at the country level,
as shown in Figure 13, over half of the people
surveyed trust AI systems in 12 of the 17
emerging economies (ranging from


41 percent in Poland to 79 percent in Nigeria).
Trust and acceptance are particularly high in the
six emerging economies of Nigeria, India, Egypt,
China, the UAE, and Saudi Arabia—with over
60 percent of people willing to trust AI and at
least 49 percent reporting high acceptance.
These countries also have the highest levels
of AI use and AI literacy, as previously reported.

In contrast, less than half trust AI systems in 25
of the 29 advanced economies. Of the advanced

economies, trust is highest in Norway, [23] Spain,
Israel, and Singapore (all over 50 percent willing
to trust). In contrast, Finland and Japan rate the
lowest on trust (25-28%) while New Zealand and
Australia (15-17% high acceptance) rank lowest
on acceptance.

The higher trust and acceptance of AI in emerging
economies is reflected in the accelerated uptake
of AI in these countries. [24]


**Figure 12: Trust and acceptance of AI systems across economic groups**

% Global % Advanced Economy % Emerging Economy











Trust = % 'Somewhat willing', 'Mostly willing', 'Completely willing'
Trustworthy = % 'Somewhat agree', 'Agree', 'Strongly agree' trustworthy
Acceptance = % 'Moderately', 'Highly', 'Completely' accept

© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **31**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**Figure 13: Trust and acceptance of AI systems across countries**

% Trust % High Acceptance


**79**

**76**


**71**

**68**

**65**

**62**

**62**


**66**

**63**

**61**

**69**


**54**

**52**

**49**


**56**

**55**

**54**

**54**

**52**

**51**

**51**

**51**

**50**

**47**

**47**

**46**

**46**

**46**

**45**

**45**

**45**

**44**

**42**

**41**

**41**

**41**

**40**

**40**

**40**

**38**

**36**

**36**

**36**

**36**

**35**

**34**

**34**

**34**

**33**

**33**

**32**

**31**

**28**

**25**


**Nigeria**

**India**

**Egypt**

**China**

**United Arab Emirates**

**Saudi Arabia**

**South Africa**

**Türkiye**

**Brazil**

Norway

**Hungary**

**Costa Rica**

Spain

**Mexico**

Israel

Singapore

Latvia

Estonia

Switzerland

Greece

**Argentina**

**Romania**

**Colombia**

**Chile**

Korea

United Kingdom

USA

**Poland**

Denmark

Slovenia

Italy

Austria

Ireland

Sweden

Slovak Republic

Portugal

Australia

Belgium

New Zealand

Lithuania

Canada

Netherlands

France

Germany

Czech Republic

Japan

Finland


**29**

**32**

**38**

**38**

**31**

**36**


**44**

**44**

**43**

**43**


**27**

**28**


**34**

**36**

**35**

**36**

**32**

**33**


**22**

**21**

**20**

**22**


**30**

**33**

**28**

**30**

**26**


**25**

**24**

**17**

**21**

**15**

**19**

**18**

**24**

**22**

**18**

**20**


**35**


**29**


% Trust = ‘Somewhat willing’, ‘Mostly willing’ or ‘Completely willing’
% High acceptance = 'Highly' or 'Completely’
Bolding indicates countries with emerging economies.

© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **32**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**People have mixed emotions about AI:**
**both optimism and worry prevail**

People feel a range of emotions about AI
applications. As shown in Figure 14, the majority
feel optimistic and excited, while also worried—
demonstrating a degree of emotional ambivalence.

People in emerging economies report more
positive emotions toward AI and a clear
divergence between positive and negative

**Figure 14: Emotions associated with AI**


sentiment. Optimism and excitement are
dominant emotions in emerging economies,
experienced by 74-82 percent of people.
Significantly fewer (56%) feel worried.

In contrast, people in advanced economies feel
both worried and optimistic in almost equal
measure (61-64%), with just over half (51%)
feeling excited.


'In thinking about AI [specific application], to what extent do you feel…'

% Global % Advanced Economy % Emerging Economy









Reinforcing this pattern, Figure 15 shows
emotions about AI applications at the country
level. People in many advanced economies feel
more worried than optimistic or excited, whereas
optimism and excitement dominate in most
emerging economies. To illustrate, 70 percent of
people in Japan feel worried and only 37 percent


are excited. In contrast, over 80 percent of people
in China feel optimistic and excited about AI
applications, while only 43 percent feel worried.

At least half of respondents feel worried about
AI in all but three countries, underscoring that
worry about AI often coexists with optimism
and excitement in many countries.


© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **33**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**Figure 15: Emotions toward AI across countries**

% Optimistic % Worried % Excited

35% 40% 45% 50% 55% 60% 65% 70% 75% 80% 85% 90%

**India**

**China**

**Nigeria**
**Türkiye**
**Egypt**

**United Arab Emirates**

**Saudi Arabia**

**Costa Rica**

**Brazil**

**South Africa**

**Romania**

Lithuania

Latvia

Norway

**Chile**

**Mexico**

**Colombia**

Israel

**Argentina**

**Poland**

Singapore

Slovenia

Estonia

Korea

Spain
**Hungary**
Switzerland

Italy

Greece

Portugal
Germany
Slovak Republic

France

Denmark

Austria

Ireland

Czech Republic
Belgium

Sweden

USA

United Kingdom

Canada

Japan

Finland

Netherlands

New Zealand

Australia

% based on: % Moderately, % Very and % Extremely. Ordered by % optimistic.
Bolding indicates countries with emerging economies.

© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **34**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

~~**I**~~ **n summa** ~~**ry**~~

Overall, the findings reveal considerable ambivalence toward the use
of AI systems in society, stemming from the tension that people are
less trusting of the safety and security of using AI systems and their
impact on society, but are more trusting of their technical ability to
provide a helpful service. This tension is reflected in low and ambivalent
trust of AI, moderate acceptance, and the coexistence of optimism with
worry, particularly for people in advanced economies. Moreover, trust in
AI has declined over time, while worry has increased. The next section
examines how this ambivalent trust is shaped by perceptions and
experiences of the benefits and risks of AI systems.

© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **36**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**How do people view and experience**
**the benefits and risks of AI?**

To help answer this question, we asked the extent to which people perceive
and have observed or experienced beneficial or negative outcomes from AI,
and if they feel the benefits of AI applications outweigh the risks.


**People expect and are experiencing**
**a broad range of benefits from AI**

Most people (83%) believe the use of AI will
result in a wide range of benefits, as shown
in Figure 17. Importantly, 73 percent of people
are personally _experiencing_ or _observing_
these benefits. [25]

The most commonly expected benefits are
also some of the most realized, with over three
quarters reporting they have experienced or
observed improved efficiency and effectiveness,
reduced time spent on mundane or repetitive
tasks and improved levels of accessibility to
information or services. [26] Increased fairness

due to the use of AI (e.g. by reducing human
bias) is the least commonly realized benefit,
but it is still experienced or observed by over
half of respondents (54%).

The utility of AI and people’s lived experience
of its benefits help explain the widespread
use, adoption and qualified acceptance of
AI technologies, despite the trust concerns.
The positive benefits experienced are largely
performance oriented—in line with our finding
that people are more trusting of AI’s ability to
provide a helpful service and output.

People who expect and experience or observe
benefits from AI are more likely to trust (r=.42.57), accept (r=.41-.63), and use AI (r=.40-.41).
They are also more likely to have AI training or
education (r=.25), AI knowledge (r=.31-.38),
and AI efficacy (r=.38-.45).



© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **37**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**Figure 17: Expected and experienced benefits of AI use**

‘I expect the use of AI [specific application] will result in these potential positive outcomes’

%Low % Moderate to High % Personally experienced or observed


**Overall benefits**

Reduced time spent on mundane or

repetitive tasks

Improved efficiency

Improved accessibility

Improved effectiveness

Enhanced precision or personalization

Reduced costs or better

use of resources

Innovation

Enhancing what people can do

Enhanced decision-making and

problem-solving

Improved outcomes for people

Enhanced creativity

Increased fairness












% Low = 'Not at all' or 'To a small extent’

% Moderate to High = 'To a moderate extent’, 'To a large extent' or 'To a very large extent'


**People in emerging economies are**
**more likely to expect and realize the**
**benefits of AI**

Ninety percent of people in emerging economies
expect benefits from AI applications, compared
to 79 percent in advanced economies. As shown
in Figure 18, people in emerging economies have
the most positive expectations of the benefits of
AI. For instance, 95 percent of people in Nigeria
expect a wide range of benefits. In contrast, fewer
people expect benefits from AI in several advanced
economies, particularly Australia, Canada, Finland,
Japan, New Zealand, the UK and the USA.


The majority of people in emerging economies are
also more likely to have observed or experienced
AI benefits (82% vs. 65% in advanced economies).
The largest differences between economies relate
to the benefits of increased fairness (66% vs 43%),
enhanced creativity (80% vs 59%), and improved
outcomes for people (84% vs 64%).

AI systems may be perceived and experienced as
more beneficial in emerging economies because
of their ability to fill critical resource gaps and
provide greater relative opportunities to people.
For instance, the use of AI systems in healthcare
has the potential to enhance service delivery and
improve health outcomes in areas where there is
limited access to medical professionals.


© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **38**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**Figure 18: Expected benefits of AI across countries**

Reduced time spent on repetitive Improved efficiency Improved accessibility
or mundane tasks

Reduced costs or better
Improved effectiveness Enhanced precision or personalization
use of resources


Innovation Enhancing what people can do


Enhance decision-making
or problem solving


Improved outcomes for people


Enhanced creativity Increased fairness


40% 45% 50% 55% 60% 65% 70% 75% 80% 85% 90% 95% 100%

**Nigeri** ~~**a**~~
**Türkiy** ~~**e**~~
**Egy** ~~**pt**~~

**Mexic** ~~**o**~~

Ita ~~ly~~

**Indi** ~~**a**~~

**Chil** ~~**e**~~

**United Arab Emirat** ~~**es**~~

**Saudi Arabi** ~~**a**~~

Isra ~~el~~

Greec ~~e~~

**Costa Ric** ~~**a**~~

**Colombi** ~~**a**~~

**Chin** ~~**a**~~

**Braz** ~~**il**~~

**Argentin** ~~**a**~~
Spai ~~n~~

**South Afric** ~~**a**~~

Portug ~~al~~

**Polan** ~~**d**~~

Kor ~~ea~~

Singapor ~~e~~

**Romani** ~~**a**~~

Franc ~~e~~

Denma ~~rk~~

Norw ~~ay~~
Germa ~~ny~~

Swede ~~n~~

Lithuan ~~ia~~

Eston ~~ia~~

Czech Republ ~~ic~~
Belgiu ~~m~~

Sloven ~~ia~~

Slovak Republ ~~ic~~

Netherlan ~~ds~~

**Hungar** ~~**y**~~

Austr ~~ia~~

Switzerlan ~~d~~

Latv ~~ia~~

Irelan ~~d~~

Cana ~~da~~

US ~~A~~

United Kingdo ~~m~~
Japa ~~n~~

New Zealan ~~d~~

Austral ~~ia~~

Finlan ~~d~~

Based on % Moderate to High = 'To a moderate extent', 'To a large extent' or 'To a very large extent'
Ordered by 'Reduced time spent on repetitive or mundane tasks'. Bolding indicates countries with emerging economies.

© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **39**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**Figure 19: Experienced benefits of AI across countries**

Reduced time spent on repetitive Improved efficiency Improved accessibility
or mundane tasks

Reduced costs or better
Improved effectiveness Enhanced precision or personalization
use of resources


Innovation Enhancing what people can do


Enhance decision-making
or problem solving


Improved outcomes for people


Enhanced creativity Increased fairness


25% 30% 35% 40% 45% 50% 55% 60% 65% 70% 75% 80% 85% 90% 95%

**Nigeria**
**Egypt**

**Chile**

**United Arab Emirates**

**Saudi Arabia**

Singapore

**Mexico**

**India**

**Costa Rica**

**Romania**

**Colombia**

Greece

**China**

**Argentina**
**Türkiye**

**Brazil**

Spain

**South Africa**

Israel

Italy

Lithuania

Korea

Latvia

Japan
Germany
Portugal

Estonia

Switzerland

Slovenia

Denmark

**Poland**

**Hungary**
Slovak Republic

Canada

Austria

United Kingdom
Norway

Sweden

Ireland

France

Australia

USA

Czech Republic
Belgium

New Zealand

Finland

Netherlands

Based on % Yes. Ordered by ‘Reduced time spent on repetitive or mundane tasks’.
Bolding indicates countries with emerging economies.

© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **40**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**People are concerned about a range of**
**negative outcomes from AI use and two in**
**five are experiencing negative outcomes**

While many of those surveyed are experiencing
significant benefits from AI use, the majority
(79%) are also concerned about a broad range
of risks and negative outcomes from AI use (see
Figure 20). Many of these risks are at the societal
level, impacting society broadly rather than having
isolated impacts on the individuals who use AI. [27]

Cybersecurity risk (e.g. from hacking or malware)
is a dominant concern raised by 85 percent
of people, together with the loss of human
interaction and connection (e.g. losing the option
to speak with a human service provider). Other
risks raised by over 80 percent of people include
misinformation and disinformation (e.g. AI used
to spread misleading or false information and
deepfakes), manipulation or harmful use, loss
of privacy or intellectual property (IP), deskilling
and dependency, and job loss.


In comparison, people are less concerned about
the risk of bias or unfair treatment from AI use

or the environmental impact (68-69%). This may
reflect a lack of awareness of the potential for AI
systems to codify existing biases in datasets, and
the high energy usage required to develop some
AI systems and power the data centers they rely
on. Although the percentages are lower, bias and
environmental impact remain clear concerns for
more than two thirds of people.

In addition to being concerned about the risks
of AI applications, two in five have personally
experienced or observed these negative
outcomes (43%; see Figure 20). The loss of
human interaction and connection, inaccurate
outcomes, and misinformation and disinformation
are the most commonly experienced negative
outcomes from AI (52-55%). Bias or unfair
treatment is the least commonly experienced or
observed outcome, but it was still experienced
by almost a third of people.


**Figure 20: Perceived risks and experienced negative outcomes from AI use**

‘How concerned are you about these potential negative outcomes of AI [specific application]?’

%Low % Moderate to High % Personally experienced or observed

% Low = 'Not at all' or 'To a small extent’

% Moderate to High = 'To a moderate extent’, 'To a large extent' or 'To a very large extent'

© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **41**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**The risks of AI are viewed and experienced**
**in a comparable way across countries**

In contrast to the differences across countries in
how people view the benefits of AI, there are few
differences across countries in people’s concerns
about the risks: the same proportion of people
are concerned about negative outcomes from AI
in both advanced and emerging economies (79%
and 78%, respectively) and the majority of people
in all countries report moderate or high concern
about these risks (ranging from 67% in China to
87% in Greece).

As shown in Figure 21, the top concerns in
almost all countries are either cybersecurity risks
or the loss of human connection. China, Egypt,
Nigeria, Saudi Arabia and South Africa are the
exceptions, where job loss is the primary or an
equal concern. There are also commonalities in
what people are least concerned about, with
either the environmental impacts of AI or the
potential risk of bias from AI ranking last in
every country.

The experience or observation of negative
outcomes is also similar across economies

(Emerging: 46% vs. Advanced: 40%). However,
as shown in Figure 22, there is a trend for people
in emerging economies to be more likely to have
experienced or observed job loss due to AI
(46% vs. 34% in advanced economies).


**People in emerging economies are**
**more likely to believe the benefits of AI**
**outweigh the risks: opinion is divided in**
**advanced countries**

Globally, 42 percent of people believe the
benefits of AI outweigh the risks, compared to
32 percent who believe the risks outweigh the
benefits, and 26 percent who believe benefits
and risks are balanced. This aligns with the
finding that more people report experiencing
benefits from AI than negative outcomes.

However, there are significant country
differences in how people perceive the balance
between AI risks and benefits. Half of people in
emerging economies believe benefits outweigh
risks, but opinions are more divided in advanced
economies, where 38 percent believe the
benefits outweigh risks and an almost equal
number (37%) believe the risks outweigh the
benefits. This aligns with the previously reported
finding that more people in emerging economies
expect and experience benefits from AI.

As shown in Figure 23, over 60 percent believe
benefits outweigh risks in Nigeria, China, and
Egypt (from 61% in Egypt to 74% in Nigeria).
In contrast, a third or less agree that the
benefits outweigh the risks in Australia,
New Zealand, the Netherlands, Sweden,
Finland, Canada, Ireland, and France.

Although perspectives on AI vary across
economies, in no country does the belief that
AI risks outweigh the benefits reach 50 percent.
This suggests that, despite concerns, most
people in all countries acknowledge the benefits
of AI systems.


© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **42**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**Figure 21: Concerns about the risks of AI across countries**

Cybersecurity risks Loss of human interaction and connection Misinformation or disinformation

Deskilling and dependency Loss of privacy or intellectual property Manipulation or harmful use


Job loss System failure


Human rights being undermined


Inaccurate outcomes

Bias or unfair treatment


Disadvantage due to Environmental impact
unequal access to Al


55% 60% 65% 70% 75% 80% 85% 90%

Port ~~ugal~~

Netherl ~~ands~~

**Me** ~~**xico**~~

Sw ~~eden~~
Singa ~~pore~~

Den ~~mark~~
Czech Rep ~~ublic~~
**Colo** ~~**mbia**~~
**Argen** ~~**tina**~~
**South A** ~~**frica**~~
Slovak Rep ~~ublic~~

Ger ~~many~~

Bel ~~gium~~

United King ~~dom~~
**Rom** ~~**ania**~~

Aust ~~ralia~~

Slov ~~enia~~

New Zea ~~land~~

**Costa** ~~**Rica**~~

**Hun** ~~**gary**~~

Nor ~~way~~

Lithu ~~ania~~

**United Arab Emir** ~~**ates**~~

Switzer ~~land~~

**Saudi Ar** ~~**abia**~~

Based on % Moderate to High= 'To a moderate extent', 'To a large extent' or 'To a very large extent'
Ordered by %'Cybersecurity risks’. Bolding indicates countries with emerging economies

© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **43**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**Figure 22: Experienced negative outcomes from AI use across countries**

Cybersecurity risks Loss of human interaction and connection Misinformation or disinformation

Deskilling and dependency Loss of privacy or intellectual property Manipulation or harmful use


Job loss System failure


Human rights being undermined


Inaccurate outcomes

Bias or unfair treatment


Disadvantage due to Environmental impact
unequal access to Al


20% 25% 30% 35% 40% 45% 50% 55% 60% 65%

Latvi ~~a~~

**Indi** ~~**a**~~

Singapor ~~e~~
**United Arab Emirate** ~~**s**~~

**Colombi** ~~**a**~~

**Saudi Arabi** ~~**a**~~

**Romani** ~~**a**~~

**Chin** ~~**a**~~

**Türkiy** ~~**e**~~
Estoni ~~a~~

**Egyp** ~~**t**~~
**Costa Ric** ~~**a**~~

**Chil** ~~**e**~~

**Nigeri** ~~**a**~~
**Mexic** ~~**o**~~

**Argentin** ~~**a**~~
Greec ~~e~~

Switzerlan ~~d~~

Finlan ~~d~~

Lithuani ~~a~~

Sloveni ~~a~~

Slovak Republi ~~c~~
Denmar ~~k~~

**Brazi** ~~**l**~~

Belgiu ~~m~~
Austri ~~a~~

Irelan ~~d~~

**South Afric** ~~**a**~~

Kore ~~a~~

Spai ~~n~~
Norwa ~~y~~
Australi ~~a~~

US ~~A~~

Portug ~~al~~
Japa ~~n~~
Isra ~~el~~

**Hungar** ~~**y**~~
Czech Republi ~~c~~
Swede ~~n~~

Finlan ~~d~~

Franc ~~e~~

Netherland ~~s~~

Canad ~~a~~

Ital ~~y~~
United Kingdo ~~m~~
Germany
New Zealan ~~d~~

Based on % Yes. Ordered by ‘Cybersecurity risks’. Bolding indicates countries with emerging economies

© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **44**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**Figure 23: Perceptions across countries that AI benefits outweigh risks**

‘For you personally, how do the benefits of AI [specific application] compare to the risks?’


**Overall**

**Nigeria**

**China**

**Egypt**

**India**

**Saudi Arabia**

**United Arab Emirates**

**South Africa**

**Türkiye**

Korea

**Costa Rica**

Singapore

Lithuania

Norway

**Argentina**

**Brazil**

**Chile**

Spain

Switzerland

Israel

Latvia

**Mexico**

**Poland**

Estonia

Italy

Japan

**Romania**

Denmark

Slovak Republic

**Colombia**

Greece

Slovenia

Czech Republic

Germany

**Hungary**

United Kingdom

Austria

Portugal

USA

Belgium

France

Ireland

Canada

Finland

Sweden

Netherlands

New Zealand

Australia










‘Benefits slightly outweigh the risks’,
‘Benefits outweigh the risks’, and
‘Benefits strongly outweigh the risks’.
Light bars and bolding indicate countries


© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **45**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

~~**I**~~ **n summa** ~~**ry**~~

Taken together, the extensive range of benefits and negative outcomes
experienced from AI use highlights the paradoxical impacts of AI
systems on individuals and society. For example, depending on how it
is implemented and for what purpose, AI systems can either increase
fairness or augment bias, facilitate accurate information or contribute
to misinformation, enhance what people can do or deskill people.

As with all powerful technologies that augment capabilities and offer
transformative opportunities for advancement and growth while also
augmenting risks and negative outcomes, AI systems require careful
management and governance, together with guardrails and guidance
to ensure appropriate and responsible use and prevent harm.

It is with this in mind that we turn next to examine the public’s
expectations of the regulation and governance of AI.

© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **46**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**What do people expect from the**
**regulation and governance of AI?**

Given the risks and benefits associated with AI, we asked people about
their expectations of the regulation and governance of AI including whether
regulation is necessary, whether current regulation and institutional
safeguards are sufficient, and who should regulate AI. We also explored
who is trusted to develop and use AI, and the role of governance and
assurance mechanisms in supporting trust in AI.


Before presenting findings on public perceptions
of AI regulation, it is important to recognize that
regulatory approaches vary significantly across
jurisdictions. For example, the European Union
has adopted the comprehensive EU AI Act,
while other jurisdictions are at different stages
of maturity—ranging from developing AI-specific
frameworks to relying primarily on guidelines
or existing regulation. This diversity highlights
the absence of a unified global approach and
provides important context for interpreting
public perceptions of AI regulation.

**The majority in almost all countries**
**surveyed believe AI regulation**
**is required**

Given the perceived and experienced risks
and impacts of AI, it is not surprising that
70 percent of people across countries globally
believe AI regulation is required. Only 17 percent
believe that AI regulation is not needed, with
the remaining 13 percent unsure. This finding
corroborates our prior survey findings, and
other independent surveys indicating strong
public desire for the regulation of AI. [28]


As shown in Figure 24, the majority of people in
all countries view AI regulation as a necessity.
India is the exception, where just under half (48%)
agree regulation is needed. In all other countries,
the percentage reporting that AI regulation is
needed ranges between 57 percent in the UAE
to 86 percent in Finland.

This broad public consensus of the need to
regulate AI supports the many national and
international efforts to regulate and govern AI to
minimize negative societal outcomes and harm.


© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **47**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**The current regulatory landscape is not**
**meeting public expectations: Only two**
**in five people believe current regulation**
**and laws governing AI are sufficient**

The majority (57%) of people disagree or
are unsure that current regulation, laws and
safeguards are sufficient to make AI use safe
and protect people from harm (see Figure 25).
Only two in five (43%) believe that regulation
and laws governing AI systems are sufficient.
This finding aligns with prior surveys [29] indicating
people want more effective regulation of AI.

This pattern is strongest in the advanced
economies, where only 37 percent view current
regulation and laws as adequate. As evidenced
in Figure 25, a third or less view regulation
as adequate in the advanced economies of
New Zealand, Finland, Japan, Sweden, Canada,
the USA, Australia, Ireland, France, the UK,
and Germany.

In contrast, 55 percent of people in emerging
economies view the safeguards around AI as
sufficient. This predominantly reflects the six
countries where a significant majority believe
current safeguards are sufficient, namely India,
Nigeria, China, Saudi Arabia, the UAE,
and Egypt.


To further understand the adequacy of current
regulation and laws, respondents were asked if
there is _too much_ regulation of AI.

In the advanced economies, the dominant
response is to disagree (45%), followed by those
who are neutral or don’t know (35%). Only one
in five (20%) agree that there is already too
much regulation of AI. People in the emerging
economies, are more evenly split, with about a
third (32%) disagreeing that there is too much
regulation, another third (30%) neutral or reporting
that they don’t know, and 38 percent agreeing.

The country-level data shows that the only
countries where the majority believe there is too
much AI regulation are India, Egypt, Saudi Arabia,

and the UAE.

The strong association between perceived
adequacy of AI regulation with trust (r=.67),
acceptance (r=.64), and use of AI (r=.45), and
confidence in organizations to develop and use
AI in the public interest (r=.51) highlights the
importance of developing an effective regulatory
framework to underpin AI adoption.


© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **48**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**Figure 24: Need for AI regulation across countries**

‘Regulation of AI [specific application] is needed’ % Agree Advanced Economy


**Overall**

Finland

Spain

New Zealand

Portugal

United Kingdom

**Hungary**

**Chile**

Australia

Ireland

Netherlands

Israel

Canada

Italy

**Colombia**

Sweden

Slovenia

Belgium

USA

**Nigeria**

Norway

**Argentina**

Czech Republic

Denmark

Greece

Germany

France

Lithuania

**Romania**

**Mexico**

Japan

Singapore

**South Africa**

Slovak Republic

**Brazil**

**Costa Rica**

Estonia

Austria

**Türkiye**

Switzerland

Korea

**China**

Latvia

**Saudi Arabia**

**Egypt**

**Poland**

**United Arab Emirates**

**India**





% Agree Emerging Economy

% Agree = ‘Somewhat agree’,
‘Agree’, and ‘Strongly agree’
Light bars and bolding indicate
countries with emerging economies.





© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **49**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**Figure 25: Perceived adequacy of current regulation and laws to make AI use safe**


**Overall**

**India**

**Nigeria**

**China**

**Saudi Arabia**

**United Arab Emirates**

**Egypt**

**Brazil**

Latvia

Singapore

**South Africa**

**Costa Rica**

Norway

Lithuania

**Türkiye**

Estonia

**Poland**

**Romania**

Switzerland

**Mexico**

**Hungary**

Italy

**Chile**

Spain

Slovenia

**Colombia**

**Argentina**

Austria

Slovak Republic

Korea

Czech Republic

Israel

Denmark

Greece

Netherlands

Belgium

Portugal

Germany

United Kingdom

France

Ireland

Australia

USA

Canada

Sweden

Japan

Finland

New Zealand






% Disagree

% Neutral

% Agree

To what extent do you agree
with the following...

There is adequate regulation of
AI [specific application]

The current law helps ensure the
use of AI (specific application) is safe

There are sufficient governance
processes in place to protect
against problems from the use
of AI (specific application)

There are enough safeguards to
make me feel comfortable with
the use of AI (specific application)

_Country responses represent_
_amalgamated percentages of all_
_four items_

% Disagree = ‘Somewhat disagree’,
‘Disagree’, or ‘Strongly disagree’

% Neutral = ‘Neutral’

% Agree = ‘Somewhat agree’,
‘Agree’, or ‘Strongly agree’.

Bolding indicates countries
with emerging economies.

















© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **50**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**Most people are not aware of laws,**
**regulation or policy that apply to AI**

These views about the adequacy of regulation
and laws may reflect, in part, low awareness of
the regulatory landscape, given four in five people
(83%) are not aware of any laws, regulation or
government policy that apply to AI within their
respective country.

There is significant variation across countries,
ranging from 5 percent awareness of AI regulation
in the Czech Republic to 49 percent in China.
Awareness is highest in the emerging economies
of Nigeria, Costa Rica, Saudi Arabia, Egypt, the
UAE, India and China (ranging from 27 percent
to 49 percent aware). Amongst the advanced
economies, awareness is notably highest
in Norway (32%), followed by Estonia, Latvia,
Singapore and Switzerland (24% respectively), and
under 17 percent in other advanced economies. [30 ]


**Figure 26: Expectations of who should**
**regulate AI**

‘I think AI systems [specific application] should be
regulated by...’

% Disagree % Neutral % Agree


% Disagree = ‘Somewhat disagree’, ‘Disagree’,
or ‘Strongly disagree’
% Agree = ‘Somewhat agree’, ‘Agree’, or ‘Strongly agree’
##### **83%**

As shown in Figure 27, international laws and

are not aware of any regulation was the most endorsed form of
laws, regulation or regulation in most countries. A clear majority
policy that apply of people in all countries support having
to AI in their country


People who have AI training or education, or
higher levels of AI literacy (AI knowledge or
AI efficacy), report greater awareness of laws
and regulations that apply to AI (r=.34-.42).
This suggests that one pathway to lift regulatory
awareness is through AI literacy programs.

**There is a strong public mandate for**
**international and national regulation of AI**

As shown in Figure 26, a clear majority of people
(between 64% and 76%) support multiple forms
of regulation. Three in four expect international
laws and regulation and seven in ten expect coregulation by industry, government, and existing
regulators, and independent oversight from their
country’s government and existing regulators.
Just under two thirds expect governance from
industries that use or develop AI systems and a
dedicated, independent AI regulator.


As shown in Figure 27, international laws and
regulation was the most endorsed form of
regulation in most countries. A clear majority
of people in all countries support having
international laws and regulation, with agreement
ranging from 60% to 86%. This may reflect an
appreciation that many AI platforms operate
across borders and are often developed and used
by multinational organizations headquartered
outside of one’s own country, requiring laws and
regulation at the international level to ensure
oversight and application across jurisdictions.

In addition to international laws and regulation,
people in most countries express a preference for
national government regulation or a co-regulatory
approach between government and industry, over
self-regulation by industry or an independent AI
regulator. However, it is notable that a majority in
almost all countries endorse each of these forms

of regulation, in line with the broad reach, uptake
and impact of AI across multiple sectors and
levels of society.

These findings indicate the public has a strong,
shared expectation of a multipronged regulatory
approach at international and national levels to
govern AI, with active involvement from both
government and industry.

.


© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **51**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**Figure 27: Expectations of who should regulate AI across countries**


International law and regulations


The government and/or existing regulators Industry that uses or develops Al


Co-regulation by industry, government and existing regulators A dedicated, independent Al regulator

40% 45% 50% 55% 60% 65% 70% 75% 80% 85% 90%

**India**

Spain

**Egypt**

Netherlands

Italy

Ireland

Greece

Singapore

United Kingdom

**United Arab Emirates**

Portugal

**Türkiye**

**Saudi Arabia**

**Romania**

**Hungary**

**Chile**

**Poland**

Norway

**China**

Belgium

Finland

Switzerland

**South Africa**

**Nigeria**

New Zealand

Australia

**Mexico**

Germany

France

Austria

Sweden

Lithuania

Latvia

Estonia

Denmark

**Colombia**

Canada

Slovenia

Korea

**Costa Rica**

Israel

**Brazil**

**Argentina**

Slovak Republic

Japan

Czech Republic

USA

Dots represent % Agree = ‘Somewhat agree’, ‘Agree’, or ‘Strongly agree’.
Ordered by ‘International laws and regulation’. Bolding indicates countries with emerging economies.

© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **52**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**A clear public mandate for stronger**
**regulation of AI-generated misinformation**

We further examined impacts and expectations
related to AI-generated misinformation and
disinformation. As shown earlier in the report
(see Figure 20), this is a key concern for the
majority of people.

Our findings suggest that AI-generated
misinformation is eroding trust in online content,
with ripple effects for trust in elections. As shown
in Figure 28, 70 percent of people are unsure
if online content can be trusted because they
don’t know if content is real or AI-generated,
and 64 percent are concerned that elections are


being manipulated by AI-powered bots and AIgenerated content. This is further exacerbated
by the fact that over half of people do not feel
they can identify AI-generated misinformation.

Given these concerns, almost nine in ten
respondents say they want stronger laws and
actions to combat AI-generated misinformation.
A large majority agree that there should be
laws to prevent the spread of AI-generated
misinformation. They want news and social media
companies to implement stronger fact-checking
processes to combat AI-generated misinformation,
and methods (such as watermarking) to allow
people to detect when content is AI generated.


**Figure 28: Impacts and management of AI-generated misinformation**

‘To what extent do you agree with the following?’

% Agree

**Impacts of misinformation**

I find it hard to trust information online as I don’t know

if content is real or AI-generated



I am concerned that elections are being manipulated
by AI-generated content or bots

**Actions to combat misinformation**

I am confident in my ability to identify AI-generated
misinformation

There should be laws to prevent the spread
of AI-generated misinformation

News and social media companies should implement
stronger fact checking processes to combat
AI-generated misinformation

News and social media companies need to ensure
people can detect when content is AI-generated
(e.g. text, images, audio or videos)

% Agree = ‘Somewhat agree’, Agree’, and ‘Strongly agree’







© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **53**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**Organizational assurance mechanisms**
**enhance trust in AI systems**

In addition to external rules, laws, and
safeguards, we asked about a range of assurance
mechanisms available to organizations to support
and signal their trustworthy and responsible
use of AI. These mechanisms range from
monitoring system reliability to human oversight
and accountability, responsible AI policies and
training, adhering to international AI standards,
and independent third-party AI assurance
systems (see Figure 29).

**Figure 29: AI assurance mechanisms**


Four out of five (83%) report they would be
more willing to trust an AI system when such
assurance mechanisms are in place.

Each of these assurance mechanisms is viewed

as important for trust across all countries (ranging
from 69% in Japan to 89% in Türkiye and Nigeria).
This indicates that these mechanisms can play a
key role in strengthening trust in organizational AI
use across diverse markets.


‘I would be more willing to trust an AI system (specific application) if…’

% Agree

**Assurances overall**

People have the right to opt out of having their data
used by the system

Its accuracy and reliability are monitored

Organizations using the system train employees on responsible

and safe use

It allows for human intervention to correct, override,
or challenge recommendations and output

Laws, regulations or policies are in place to govern
responsible AI use

It adheres to international AI standards

It is clear who is accountable if something goes wrong
with the system

It is assured by an independent third party

% Agree = ‘Somewhat agree’, Agree’, and ‘Strongly agree’











© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **54**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**There has been no change in the perceived**
**adequacy of AI safeguards over time, however**
**the importance of organizational assurance**
**mechanisms for trust has increased**

The belief that AI regulation is needed has
remained constant over time (71% in 2022
vs 71% in 2024; M=2.5 vs. 2.6), as has the
perceived adequacy of current regulations and
laws (M=4.0 at both time points). However,
there is a trend towards fewer people viewing
current AI regulations as adequate in nine
countries, largest reduction evident in Germany
(41% agree in 2022 vs. 31% in 2024).

**People have most confidence in**
**universities and healthcare organizations**
**to develop and use AI**

As shown in Figure 30, people have the most
confidence in their country’s universities, research
institutions, and healthcare organizations to
develop and use AI in the best interests of the
public. Between 78 percent and 88 percent report
moderate to high confidence in these entities in
advanced and emerging economies, respectively.

People are less confident in their government’s
use of AI. Between 58 percent and 65 percent
report moderate to high confidence in their
national government to develop and use AI in
the best interests of the public in advanced and
emerging economies, respectively. However, two
in five (40%) report low confidence. Addressing
this low confidence in governmental use of AI
will be important going forward to realize the
many beneficial applications of AI use in public
sector service delivery, including enabling
equitable access to government services and
enhancing the personalization, effectiveness
and efficiency of service delivery.

There is significant variation across countries in
people’s confidence in government. Half or more
(50 to 67%) lack confidence in their government
to develop and use AI in the public's best interest
in Argentina, Italy, Spain, Ireland, Japan, the USA,
Colombia, Hungary, Slovenia, Romania, Greece,


Given the increase in perceived risks of AI
previously reported, it is not surprising that
the importance of organizational assurance
mechanisms has increased over time. Eighty
percent of people in 2024 reported they
would be more likely to trust AI systems when
organizational assurance mechanisms are in
place, compared to 72 percent in 2022 (M=5.6
vs. M=5.0). There were significant increases
in all 17 countries, with the largest in Canada,
the UK and Finland (ranging from 69-74% in
2022 to 81-84% in 2024).

the Czech Republic and Slovakia. In contrast,
as shown in Figure 31, most people in Norway,
Singapore, India, the UAE, Saudi Arabia and
China have confidence in their government
(ranging between 65% and 90%).

People in emerging economies report greater
confidence in big technology companies,
like Apple, Facebook/Meta, Google/Alphabet,
Huawei, OpenAI and Tencent (84% vs 64%
confident) and commercial organizations, such
as retailers and banks (75% vs 60%), than those
in advanced economies. For example, as shown
in Figure 31, over 90 percent of people in China,
Nigeria, India, Egypt, and Saudi Arabia have
moderate to high confidence in big technology
firms. In comparison, countries with advanced
economies tend to have lower confidence in
big technology firms, such as France, the UK,
Sweden, the USA, Denmark, Canada, Australia
and New Zealand (ranging from 60% in France
to 46% in New Zealand).

This highlights the potential opportunity for
commercial organizations, big technology firms,
and government to collaborate with universities
and research institutions in the development of AI.

When people are confident in entities to develop
and use AI, they are more likely to trust (r=.54)
and accept AI systems (r=.52), accept AI
systems (r=.52), and use AI (r=.40).


© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **55**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**Figure 30: Confidence in entities to develop and use AI**

‘How much confidence do you have in the following entities to develop and use AI in the best
interests of the public?’

% Low confidence % Moderate confidence % High confidence

**Universities and**

**research institutions**


Advanced economies

Emerging economies

**Healthcare**

**institutions**

Advanced economies

Emerging economies

**Big technology**
**companies**

Advanced economies

Emerging economies

**Commercial**
**organizations**

Advanced economies

Emerging economies

**Government**

Advanced economies

Emerging economies


~~35~~ ~~26~~ 39


% Low confidence = ’Very low confidence’ and ‘Low confidence’
% High confidence = ‘High confidence’ and ‘Very high confidence’

© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **56**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**Figure 31: Confidence in entities to develop and use AI across countries**


The country government


Commercial organizations Country healthcare institutions


Big technology companies Country universities and research institutions

30% 35% 40% 45% 50% 55% 60% 65% 70% 75% 80% 85% 90% 95% 100%

New Zealand

Australia

Canada

Denmark

Ireland

Slovenia

Finland

USA

Sweden

Austria

United Kingdom

Netherlands

France

Belgium

Greece

Czech Republic

Germany

Portugal
Spain

Slovak Republic

Switzerland

**Hungary**

**Poland**

Italy

Estonia

Israel

Lithuania

**Romania**

Norway

**Argentina**

**Türkiye**

**Colombia**

**Chile**

Japan

Latvia

Singapore

**Mexico**

**Costa Rica**

**Brazil**

**South Africa**

Korea

**United Arab Emirates**

**Saudi Arabia**

**Egypt**

**India**

**Nigeria**

**China**

% based on ‘Moderate confidence’, ‘High confidence’ and ‘Very high confidence’ (5 point scale)
Ordered by Big technology companies. Bolding indicates countries with emerging economies.

© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **57**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

~~**I**~~ **n summa** ~~**ry**~~

Taken together, these findings reveal a clear public desire for stronger
regulation and governance of AI systems that is fit-for-purpose in
supporting safe and trustworthy use. The majority expect robust
international and national regulation, but many do not believe that the
current safeguards around AI are sufficient. There is also widespread
support for stronger legislation and action that specifically targets AIgenerated misinformation.

The low level of public awareness of laws governing AI likely reflects
that many jurisdictions are still in an early phase of designing and
implementing regulatory frameworks. However, it also suggests
a need to support people to understand if and how existing and
emerging laws and regulation apply to AI.

At the organizational level, the findings highlight that organizations
can strengthen trust in their use of AI systems by putting in place
governance and assurance mechanisms that signal trustworthy
and responsible use. In the next section we further examine key
pathways for supporting trust and acceptance of AI systems.

© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **58**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**What are the key drivers of trust and**
**acceptance of AI systems?**

In the preceding sections, we identified that AI literacy and training,
perceptions of the benefits and risks of AI, and the perceived adequacy of
AI regulation and confidence in entities to use AI, are each associated with
people’s trust and acceptance of AI systems used in society. To identify the
most important predictors, we used a statistical technique called structural
equation modeling. [31]

The model examines four distinct pathways—reflecting knowledge,
motivational, uncertainty, and institutional drivers—testing and comparing
their importance in predicting trust and acceptance of AI. We show the
model in Figure 32, together with notes on interpretation.


**Trust is central to AI acceptance**

The model shows that trust is a key driver of AI
acceptance (B=.43 [32] ), empirically supporting why
trust in AI matters: if people are willing to trust AI
systems, then they are more likely to accept and
approve their use in society.

As explained below, the model further shows that
trust acts as a central mechanism through which
other drivers impact AI acceptance.

**AI literacy influences trust and acceptance**

The knowledge pathway is based on evidence
that knowledge, efficacy, and training—which
each relate to AI literacy—help to enhance trust
in technology. [33]

The model shows that people are more likely
to trust AI systems when they believe they
understand AI and when and how it is used

in common applications and have received AI
education or training (B=.11). The knowledge
pathway also has a direct impact on acceptance
(B=.12).


These relationships indicate the importance of
providing people with opportunities to enhance
their AI literacy.

**The perceived benefits of AI foster**
**increased trust and acceptance**

The motivational pathway to trust is grounded
in evidence that the more people perceive
benefits, utility, and positive outcomes from
the use of technologies, the more they will
be motivated to trust and accept them. [34]

Expecting AI systems to produce benefits
(B=.23) has a relatively strong influence on
trust, as well as on levels of acceptance (B=.22).
This relationship highlights the importance of
designing and using AI systems in a way that
delivers benefits to a broad range of people.


© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **59**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

l


l


**Figure 32: A model of the key drivers of trust and acceptance of AI use in society**

l


l


l


l


l


l

|Col1|Col2|Col3|Col4|
|---|---|---|---|
|.|0|1||
|.||||

|Col1|Col2|
|---|---|
|.0|3|
|||


l


l


l


l


l


l


l


l


l


l


l


l


l


l


l


l


l


l


l


l


Predictors also have a
direct effect on acceptance
after accounting for their
influence via trust:

- Knowledge: .12

- Motivational: .22

- Uncertainty: -.05

- Institutional: .17

l


**How to read the model**

When reading the model, follow the arrows from left to right. The left boxes show the four drivers of trust and
acceptance, with notes explaining each driver in the boxes below the model. The values on the arrows indicate the
relative importance of each driver in infuencing trust and acceptance: the larger the number, the stronger the effect.
l
The positive values for institutional safeguards and confidence, benefits, and knowledge, indicate that when these
drivers increase, so do trust and acceptance. The negative value for uncertainty indicates that when perceived risks
increase, trust and acceptance decrease.
The model is based on all data (across countries and AI applications). All relationships shown are significant (p<.001).

© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **60**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**The perceived risks of AI create uncertainty**
**and reduce trust and acceptance**

The uncertainty pathway is based on the view
that it is more difficult to trust technologies
in contexts of risk or when the outcomes and

impacts of the technologies are uncertain. [35]

The model shows that the more concerned

people are about the risks and potential negative
outcomes of AI use in society, the less likely they
are to trust the systems (B=-.08) or accept them
(B=-.05). The impact of risk concern is notably
smaller than that of benefit expectation, which
helps to explain why people are willing to trust
and accept AI systems in society and use them
personally to gain benefits, despite concerns
they may have about the risks.

This finding demonstrates the importance of
proactively working to mitigate the perceived
risks associated with AI systems at multiple
levels and to effectively communicate the
mitigation strategies that are in place to help
reduce uncertainty, reassure people and
support their trust in AI.

**Institutional factors are the strongest**
**drivers of trust, and also impact**
**acceptance**

The institutional pathway reflects evidence that
institutional safeguards and control mechanisms
(e.g. laws, rules, standards) and confidence in
the institutions deploying technologies reassure
people of the safety, reliability and trustworthiness
of technologies. [36]

~~**I**~~ **n summa** ~~**ry**~~


Our findings indicate that people are more
trusting of AI systems when they believe current
regulation and laws are sufficient to make
AI adoption and integration into society safe
and are confident in a range of entities—from
government, big tech companies, commercial
organizations, research institutions, and health
organizations—to develop and use AI in the
public’s best interests (B=.62). The influence
of institutional factors on acceptance is
comparatively smaller (B=.17), suggesting
that much of the influence of these factors
on acceptance occurs via trust.

The model shows the institutional pathway is
the most important pathway to trust. However,
the broader survey results indicate that (a)
many are not convinced that current laws and
regulation are sufficient, and (b) perceptions of
the adequacy of AI regulation have not shifted
markedly over time. This stable perception
of existing regulation highlights an ongoing
challenge for policymakers when it comes to
reassuring the public that there are appropriate
laws, regulation and safeguards in place.

The model’s predictors explain 79 percent of the
variance in trust and 72 percent of the variance
in acceptance. The similarity of these findings to
the model that was tested and validated in our

prior research report [37] reinforces the importance
of these drivers and the robustness of the model

when tested in a larger, more diverse sample.


In summary, the modeling indicates that each of the four pathways
play a significant and complementary role in supporting trust and
acceptance of AI use in society.

© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **61**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**How do demographic factors influence**
**trust, attitudes and use of AI?**

To understand how attitudes and experiences with AI systems vary
across demographic groups, we examined the influence of age, income,
education, AI training, and gender on trust, acceptance, and the key drivers
in our model.

The analyses reveal that AI training and income consistently have the
strongest effects. It is notable that there are no differences between men
and women on any of the key indicators.


**Younger people, higher income earners,**
**the university-educated, and those with AI**
**training are more trusting and accepting of**
**AI systems, have higher levels of AI literacy,**
**and are more likely to use AI**

Analyses reveal that four subgroups are more
trusting and accepting of AI, more likely to have
higher levels of AI knowledge and efficacy, and
more likely to use AI. As shown in Figures 3335, this applies to:

- People with AI-related training or education

(vs. those without)

- People with high household incomes (vs.

middle- and low-income categories)

- Younger people, notably those aged 18-34

years, compared to the oldest category of
respondents (55+)

- People with a university education (vs. no

university education)

As shown in Figure 33, those with AI-related
education or training are almost twice as likely
to trust and accept AI technologies compared to
those without. Similarly, high-income earners are
twice as likely to trust AI and three times more
likely to have high acceptance of AI compared to
those with lower incomes.


Over
# **80%**

of people under 35, people
with AI training, and those
with high incomes use AI
tools, compared to less than
# **50%**

of those 55 years of age and
older, those who do not have
AI training, and people with
low incomes.


© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **62**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

In relation to the use of AI tools, over 80 percent
of people under 35, people with AI training, and
those with high incomes use AI tools on a regular
basis, compared to less than 50 percent of those
55 years of age and older, those who do not have
AI training, and people with low incomes (see
Figure 34).

These findings likely reflect that younger people,
those with higher incomes, and the universityeducated are more likely to have completed AI
training or AI-related education and have higher
levels of AI knowledge and efficacy (see Figure
35). For instance, 71 percent of young adults
report moderate to high levels of AI knowledge,
compared to 33 percent of older adults.
80 percent of high-income earners feel confident
about using AI, compared to 44 percent of lowincome respondents. Strikingly, 70 percent
of those with high income report having AI
education or training, compared to 38 percent
of middle-income earners and just 18 percent of
those who report low income. Over 9 in 10 (92%)
of high-income earners are interested in learning
more about AI, compared to just 42 percent of
low-income earners.

**People with AI training and high-income**
**earners report more benefits from AI**

Individuals with AI training and high-income
earners are more likely to expect a range of
benefits from AI compared to low-income earners
and those with no AI training or education (High
income: 90%, vs. middle: 83%, vs. low: 74%;
AI education or training: 89%, no AI education
or training: 79%) and report experiencing more
positive outcomes (High income: 80%, middle
income: 72%, low income: 60%; AI training or
education: 79%, no AI education or training:
63%). Higher AI literacy and use, together with
greater access to resources, may uniquely
position these groups to seize the benefits of AI
use, and protect them from negative outcomes.

Regarding the experience of specific benefits,
80 percent of people who report high income
have experienced enhanced decision-making,


compared to 70 percent of middle-income
earners and just 59 percent of those with low
income. Those with AI education or training
are particularly more likely to have experienced
reduced costs or better use of resources (75%
vs. 53%), enhanced creativity (76% vs. 54%),
and enhancing what people can do (80% vs.
65%). Concerns about negative AI outcomes and
experiences of such outcomes are consistent
across all subgroups.

**Those with AI training, high-income**
**earners and younger people are more likely**
**to view AI regulation and laws as sufficient**

People with AI training, high-income earners
and younger people are less likely to believe
AI regulation is necessary. Only 54 percent
of high-income respondents agree that AI
regulation is required, compared to between
72 percent and 75 percent of middle- and lowincome respondents. Similarly, 61 percent of the
youngest age group believe that AI regulation is
required, compared to 70 percent in the middleage range (35-44 years) and 79 percent in older
age categories (55+ years).

These groups are also more likely to view existing
AI regulation as sufficient, with 69 percent of
high-income earners agreeing, compared to just
28 percent of low-income earners.

Over
# **9 in 10**

high-income earners are interested
in learning more about AI, compared
to just 42 percent of low-income

earners.


© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **63**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**Figure 33: Trust and acceptance of AI systems by age, income, education,**
**and AI training**

% Trust % High acceptance

**Age**








69


61


45


32


32


18–34 year olds

35–54 year olds

55 and older

**Income**

High

Middle

Low

**Education**

University education

No university education

**AI training**

AI training

No AI training


20

26

23


52


40


60


39

37


50


% Trust in AI= 'Somewhat willing', 'Mostly willing', 'Completely willing'
% High acceptance = ‘Highly’ or ‘Completely’ accept

**Figure 34: Use of AI and AI training by age, income, and education**



% AI Use **Age** % AI training

18–34 year olds






35–54 year olds

55 and older

**Income**

High

Middle

Low

**Education**

University education

No university education

**AI training**

AI training

No AI training









74


50


57


27



100



% AI use = ‘Occasionally (every few months)’ to ‘Always (multiple times a day)’
% AI training = ‘% Selected University level course in AI’, ‘% Selected Work-based training’, or
‘% Selected Formal or informal training outside work’

© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **64**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**Figure 35: AI knowledge and AI efficacy by age, income, and education**





% AI knowledge **Age** % AI efficacy

18–34 year olds

35–54 year olds










55 and older

**Income**

High

Middle

Low

**Education**

University education

No university education

**AI training**

AI training

No AI training







78


78


36


48


% AI knowledge= 'To a moderate extent', 'To a large extent', 'To a very large extent'
% AI efficacy = 'Somewhat agree', 'Agree', 'Strongly agree'

~~**I**~~ **n summa** ~~**ry**~~

Taken together, the pattern of findings suggests that people who are
younger and university educated, and particularly those with AI training
and higher incomes, are better positioned to use and realize the benefits
from AI. This is likely due to their higher levels of AI literacy and resources.

In the next two sections, we examine how employees and students
use, experience and trust AI in their work and education, and their
perceptions of how their organizations govern and support AI adoption
and responsible use. These sections are based on the subset of survey
respondents who identified as working or studying, respectively.

© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **65**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

~~**SECTION**~~ ~~**TWO**~~
#### **Employee attitudes ** **towards AI at work**



**How is AI being used by employees at work?**


**The age of working with AI is here:**
**3 in 5 employees report intentional**
**regular use of AI at work**

The rapid adoption of AI in the workplace,
augmented by the release of generative AI
tools such as ChatGPT, is evident.

As shown in Figure 36, 77 percent of
employees report that AI is being used by
their organization. Almost half (47%) report
their organization uses AI to a moderate to
very large extent across a range of areas
and tasks, and thirty percent report limited
use in isolated areas or specific tasks. Just
under one-quarter of employees report their
organization does not use AI.

Fifty-eight percent of employees report
intentionally using AI tools and systems in
their work on a regular basis. Less than half
of employees report any form of training or
education in AI or related fields (47%) or have
at least a moderate level of AI knowledge
(46%), and only half (51%) believe they
can use AI effectively.

Figure 37 shows that frequency of use
varies; about a third (31%) use AI on a
weekly or daily basis, about a quarter (27%)
use it semi-regularly (i.e. every month or
few months) and two in five (42%) rarely
or never use it.


**Figure 36: Organizational use of AI**
**(employee reported)**

‘To what extent is AI used in the organization you work for?’




% Not at all

% To a small extent

% To a moderate extent

% To a large or very large extent


% Not at all = 'Not at all'

% To a small extent = 'To a small extent'

% To a moderate extent = 'To a moderate extent'

% 'To a large or very large extent = 'To a large extent',
'To a very large extent'
# **58%**

of employees report
intentionally using AI tools in
their work on a regular basis.


© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **67**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**Figure 37: Frequency of intentional use of AI at work**

‘In your work, how often do you intentionally use AI tools,
including generative AI tools?’

% selected






Never A few

times

a year


Every
few

months


Monthly Weekly Daily


Daily = ‘most days’ or ‘multiple times a day’

The one-quarter (27%) of employees who
never intentionally use AI at work were asked
to indicate the reasons why. The top reasons
included [39] :

- AI tools are not helpful, required or used for

their work (58%)

- A preference to work without the involvement

of AI tools (19%)

- Not understanding how to use AI tools (14%)

- AI tools are not approved or allowed (14%)

- Not trusting AI tools (12%)

- Lack of access or not wanting to pay for AI

tools (12%)


In several advanced economies—notably the
USA, Australia, Switzerland, Sweden, New
Zealand, and the Netherlands—a lack of trust in
AI tools was one of the top three reasons for
not using AI (reported by 15-20%). Compared
to those in emerging economies, employees
working in advanced economies are more likely
to say that they did not use AI tools because
they are not helpful or required for their work.

These findings provide insight into the potential
barriers of AI adoption at work, reinforcing the
importance of supporting AI literacy amongst
employees, providing access to AI tools, and
facilitating understanding of how AI can be used
for a range of work applications to create value.
It also highlights the importance of respecting
employees’ choice about the use of these tools
in their work.


© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **68**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **69**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**Most employees use free, public**
**generative AI tools at work, yet only**
**a minority report their organization**
**has a policy governing its use**

Employees that report using AI were asked to
identify the main AI tools they use for work (see
Figure 39). By far the most common tools—
used by almost three in four employees—are
general-purpose generative AI tools, such as
ChatGPT. Voice-based AI assistants, such as
Siri and Google Assistant, are the next most
common, used by just under half of employees,
followed by image, video and audio generators.
These high-use levels likely reflect the broad
accessibility of these tools, including the ability
to use these tools through a natural language


interface, combined with their wide utility across a
range of work tasks and functions, and immediate
usability without AI training or education.

Comparatively fewer employees use AI tools with
a more specialized focus or specific purpose—
such as Grammarly or predictive analytics
tools—or AI systems developed or customized
specifically for their organization. Even fewer
use robots and physical autonomous systems.

Employees were also asked how they access
these tools (see Figure 40). The majority (70%)
say they use publicly available AI tools that are
free to use, with a much lower proportion using
public AI tools that require payment to access.
Two in five report using AI tools that are provided
or managed by their employer.


**Figure 39: Types of AI tools intentionally used at work**

‘What are the main types of AI tools you use intentionally for work? Select all that apply’

% Using

General-purpose generative AI tools
(e.g. ChatGPT, Copilot, Claude)



Voice-based AI assistants
(e.g. Siri, Alexa, Google Assistant)

Image/video/audio generators
(e.g. DALL-E, Canva)

Specific-purpose generative AI Tools
(e.g. Grammarly, Github)

Other specific-purpose AI tools
(e.g. for predictive analytics, workflow automation)

AI systems developed or customized
for your organization

Robots and physical autonomous systems
(e.g. manufacturing robots)








© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **70**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**Figure 40: Access to AI tools used at work**

‘How do you access AI tools used for work?’

% selected





I use free,
publicly available

AI tools


I use AI tools

provided by
my employer


I use publicly
available AI tools

that I pay to access


Despite the extensive use of generative AI
tools in the workplace, employees report that
limited policies are in place to guide and outline
appropriate use.

As shown in Figure 41, only two in five report
that their organization has a policy or provides
guidance on the use of generative AI tools
at work. It is notable that almost one in five
do not know if their organization has a policy,
highlighting a significant gap between use and
knowledge of workplace policies on generative
AI tools.

**Emerging economies are leading in**
**workplace adoption of AI**

As shown in Figure 42, more employees in
emerging economies report using AI at work
compared to those in advanced economies
(72% vs. 49% using AI at least semi-regularly).
Similarly, those working in emerging economies
are more likely to report that their organization
uses AI (81% vs. 66% in advanced economies)
and does so more extensively (57% vs. 36%
have moderate to extensive use). [40]


**Figure 41: Organizational policy or**
**guidance on generative AI at work**
**(employee reported)**

‘Has your organization put in place a policy or provided
guidance on the use of generative AI at work?’


% with policy guiding
use of Gen AI

% with policy banning
use of Gen AI

% No

% Don't know





© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **71**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**Figure 42: Frequency of intentional use of AI at work**

‘In your work, how often do you intentionally use AI tools, including generative AI tools?’

% Global % Advanced Economy % Emerging Economy













Never A few times Every few Every Every Daily
a year months month week

Daily = ‘most days’ or ‘multiple times a day’


To illustrate, as shown in Figure 43, 80 percent
or more employees report using AI at work on
a regular basis in the emerging economies of
India, China, Nigeria, the UAE, Saudi Arabia and
Egypt. This compares to less than 50 percent
in the majority of the advanced economies.
We find an almost identical pattern of findings
across countries for the organizational use
of AI. [41]

A few countries with advanced economies
deviate from this trend. Norway, Singapore, and
Switzerland have comparatively high workplace
adoption of AI compared to other advanced
economies, with more than 60 percent of
employees using AI at least every few months
or more, and over 75 percent reporting that
their organization uses AI. This likely reflects the
previously reported high levels of AI training,
literacy, trust and acceptance of AI amongst
people in these countries compared to those in
other advanced economy countries (see Figures 7
and 13).

**One in two employees trusts AI at work**

Respondents were asked how willing they are
to trust AI systems for work purposes either
by relying on the information and output AI


provides to inform their work and decisions or in
sharing relevant information and data to enable AI
tools to perform tasks for them.

About half (53%) report trusting AI tools for work
purposes, which is similar to the proportion of
employees that use AI on a regular basis (58%).
There are clear differences among countries,
ranging between 31 percent in Japan to 81 percent
in India and Nigeria (see Figure 43).

Trust is highest in the emerging economies, with
an average of 63 percent of employees in these
countries trusting AI for work, compared to an
average of 45 percent in advanced economies.

Employees’ trust in AI for work purposes is
associated with their frequency of AI use at
work (r=.46) and experiencing positive impacts
of AI use at work (r=.53), highlighting the
important role of trust in adoption. Trust in
AI for work purposes is also associated with
AI knowledge, efficacy, and AI training or
education (r=.23-.45).


© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **72**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**Figure 43: Intentional use of AI at work and trust of AI at work**

% Using AI at work % Trust AI at work

% Using AI on a semi-regular or regular basis: 'every few months’, ‘monthly’, ‘weekly’ or ‘daily’
% Trust AI at work = % Willing
Countries sorted by % Using AI at work
Bolding indicates countries with emerging economies

© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **73**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **74**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**Many employees are using AI in**
**complacent and inappropriate ways,**
**augmenting risks for both organizations**
**and individuals**

A notable finding is the extent to which
employees report using AI at work in complacent
and inappropriate ways (see Figure 44).

Almost one in two employees who use AI admit
to doing so in ways that contravene organizational
policies and guidelines. For example, about half
(48-49%) of employees report that they have
uploaded sensitive company information, such
as financial, sales, or customer information, or
copyrighted material, into public AI tools. Such
behaviors are most common of employees who
report their organization has banned generative
AI (67%) or has a policy guiding generative AI use
(56%), compared to those in organizations without
such policies (33%) or those who are unsure if
there is a policy (38%). This suggests outright bans
may be ineffective, and that simply having policies
does not guarantee compliance; clear guidance
and education on responsible AI use is needed.

Employees also report using AI in ethically
ambiguous ways. Almost half (47%) say they
have used AI in ways that could be considered
inappropriate and even more indicate that they
have seen or heard other employees using
AI tools in inappropriate ways (63%). Fifty-six
percent say they have used AI tools at work
without knowing if it is allowed.

Over half (57%) of employees also admit that
they have used AI in non-transparent ways,
including presenting AI-generated content as
their own or avoiding revealing when they have
used AI tools to complete their work. This nontransparent use makes it even more challenging
for leaders and managers to govern and manage
employees’ use of AI at work.

The complacent use of AI may also reduce the
quality and accuracy of work. Over half (56%)
report they have made mistakes in their work
from AI use. This likely reflects using incorrect
or ‘hallucinated’ AI-generated content from
generative AI tools and may also include
misinterpretation of AI recommendations or
output. Two-thirds of employees report having
relied on AI output at work without critically
evaluating the information it provides (66%) and
putting less effort into their work due to AI (72%).


A contributing factor to this complacent use
may be a sense of pressure to use AI tools,
with almost half (48%) of employees feeling
concerned about being left behind if they do
not use AI at work. In support of this view,
there is a positive association between the
extent employees feel strain at work and their
complacent use of AI (r=.31).

While the survey was anonymous to encourage
honest responses from the participants, these
findings may underreport the actual extent of
complacent and inappropriate use of AI in the
workplace, given social desirability bias. [42 ]
# **48%**

of employees report
that they have uploaded
company information,
such as financial, sales,
or customer information,
into public AI tools.


© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **75**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**Figure 44: Inappropriate and complacent use of AI at work**


‘At your work, how often have you…’


% Never % Rarely % Sometimes to very often


**Overall**

**Contravening policies**

Uploaded copyrighted material or IP to a Gen AI tool

Uploaded company information into a public AI tool

Used AI in ways that contravene policies or guidelines

**Ethically ambiguous**

Seen or heard of people using AI tools inappropriately

Used AI tools at work without knowing whether it is allowed

Used AI tools in ways that could be considered inappropriate

**Non-transparent use**

Avoided revealing when you've used AI tools in your work

Presented AI-generated content as your own

**Quality issues**

Put less effort into your work knowing you can rely on AI

Relied on AI output without evaluating the information

Made mistakes in your work due to AI

% Sometimes to very often = ‘Sometimes’, ‘Often’, or ‘Very often’

**Figure 45: Critical engagement with AI at work**

‘In your work, how often do you…’






% Never % Rarely to sometimes % Most of the time to always


**Overall**

Verify the accuracy of AI output before using it

Critically evaluate the output of AI

Consider an AI tool's limitations when making
decisions based on its output

Reflect on whether you are using AI appropriately

Think about the ethical implications of using
AI-generated content

% Rarely to sometimes = 'Rarely', 'Sometimes'
% Most of the time to always = 'Most of the time', 'Always’

This inappropriate and complacent use of AI may
in part reflect a lack of critical engagement in the
way employees are using AI. As shown in Figure
45, on average, only half of employees say they
regularly engage critically with AI at work. Rather,
most employees do not routinely evaluate the


output of AI or consider the limitations of AI tools
when making decisions based on its output, or
the ethical implications of using AI content. Most
employees infrequently reflect on whether they
are using AI tools appropriately or weigh up the
benefits and risks of using them.

















© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **76**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**What are the impacts of AI use at work?**


**Employees experience performance**
**benefits from AI, but also mixed**
**impacts on workload, stress,**
**collaboration, compliance, and**
**surveillance**

Employees in organizations that use AI were
asked how AI has impacted a range of work
processes and outcomes. They report a
range of beneficial impacts on performance,
contrasted with other complex, mixed
impacts which potentially augment risks
for organizations and employees.

Focusing on the beneficial impacts, as shown
in Figure 46 (see blue bars), a majority of
employees (54-67%) report that the use of AI
tools in their workplace is delivering a range of
benefits including increased levels of efficiency,
improved access to accurate information,
enhanced innovation and idea generation,
higher work quality and decision-making, better
use and development of skills and abilities,
and improved knowledge sharing. Almost half
(46%) report the use of AI tools has increased
revenue generating activity in their organization.
These findings highlight the significant
performance benefits from AI.

However, the positive benefits of using AI
tools are not guaranteed. A quarter to a third
of employees report that the use of AI tools at
work has not had an impact on these desired
outcomes. For example, a similar proportion


of employees report AI has had no impact
on revenue generation as those reporting an
increase. Furthermore, about one in ten report
that the use of AI has actually reduced some of
these desired outcomes. Whether or not AI use
delivers beneficial outcomes is likely dependent
on a combination of factors, including the
nature of the work, the purpose and types
of AI tools used, how AI is implemented and
integrated into work design and organizational
strategy, and the level of employees’ AI literacy
and capabilities.

Employees also report that the use of AI is
having mixed impacts on workload, time spent
on repetitive tasks, and stress and pressure at
work (see Figure 46). While about two in five
(36-40%) employees have experienced positive
reductions, between one-quarter and twofifths (26-39%) report increases in workload,
repetition, stress and pressure from using AI
tools. This is not surprising given the evidence
that technological advancements can result
in the intensification of work, highlighting the
need for appropriate work redesign and change
management. [43]

AI training (r=.24), knowledge (r=.42), efficacy
(r=.41), and perceptions of organizational
support for AI and responsible use (r=.56)
are positively associated with experiencing
beneficial impacts of AI use at work.


© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **77**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

AI use is also having mixed impacts on workplace
communication and collaboration. While about
two in five report that AI tools have increased
communication and collaboration, close to a fifth
report that AI use has reduced it.

A third (35%) of employees report that the use of
AI tools has resulted in increased compliance and
privacy risks, such as contravening rules, policies
and local laws. Since most employees report
using free, publicly available generative AI tools,
this may result in instances of uploading private,
confidential or copyrighted material into public
AI systems. One fifth of employees say using
AI tools helps reduce compliance and privacy
risks, which may reflect the growing use of AI for
monitoring and managing cybersecurity threats
as well as ensuring employee compliance with
organizational policies.

It is also notable that two in five report increased
monitoring and surveillance of employees using AI


technologies. This increase may have implications
for trust in the workplace: while in some work
contexts, monitoring and surveillance is required
and beneficial for ensuring safe and trustworthy
conduct and adherence to laws and governance
policies, these control mechanisms can contribute
to decreased levels of trust at work if perceived as
signaling management distrust of employees. [44]

Most employees report that AI use in their
workplace has either had no impact on job security
or has increased it, with just under one in five
reporting it had reduced job security.

This complex mix of impacts underscores the
importance of understanding, managing and
monitoring the implementation, use and impacts
of AI at work, investing in appropriate work
redesign, and building employee capabilities to
support effective and balanced levels of humanAI collaboration.


**Figure 46: Impacts of AI use in the workplace as reported by employees**

'In your experience, how has the use of AI tools in your workplace impacted:'

%Reduced/negative impact %No impact % Increased/positive impact

%Reduced/positive impact %No impact % Increased/negative impact

Privacy and compliance risks
(e.g., breaking policies or laws)

% Reduced = ‘Slightly reduced’, ‘Reduced’, or ‘Greatly reduced’
% Increased = ‘Slightly increased’, ‘Increased’, or ‘Greatly increased’

© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **78**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**The adoption of AI has changed how and**
**by whom work is done with employees**
**rapidly becoming dependent on AI and**
**human-AI collaboration**

The data suggests that about half of employees
rely heavily on AI tools and collaboration with AI
to perform their work, with two in five employees
indicating that they sometimes or often cannot
complete work without the help of AI (see Figure
47). This reliance is likely to increase over time
given that half of employees say they regularly
rely on AI to perform tasks rather than learning
the skills to do so themselves.

These findings underscore the risk of employee
skill degradation over time and align with our
finding that deskilling and dependency on AI
are key societal concerns and notable negative
outcomes of AI adoption. This reinforces the need
for thoughtful work design to ensure AI empowers
humans to retain critical skills as well as focus

on higher-skilled, meaningful work.

Our findings also reveal that about half of
employees surveyed regularly choose to use AI to
complete work, rather than collaborating with peers
or supervisors. This has implications for achieving a
diversity of inputs, as well as the development and
retention of collaborative capabilities and processes
in the workplace. It also highlights concerns about
diminishing human interactions and connections
from increased reliance on AI tools (previously
reported in Figure 20).

**Most prefer AI involvement in managerial**
**decision-making with human oversight**

Further evidence of employees’ support for
human-AI collaboration comes from their views

of the use of AI in managerial decision-making.
Respondents were asked to choose the most
acceptable weighting between human and AI
involvement in decision-making related to work
and resource allocation, hiring, promotions, and
pay rises. [45]

As shown in Figure 48, most believe that AI
should aid managerial decision-making, but
want humans to retain most or equal control.
Nearly half consider a 75 percent human and
25 percent AI decision-making split to be the
most acceptable balance. The next most popular
preference is an even 50/50 split, supported by
just under a third of respondents.


Only ten percent believe AI should dominate
managerial decision-making, and even fewer
support a fully AI-driven approach where there is
no human involvement. This highlights the lack of
support for fully automated managerial decisionmaking or AI taking precedence over humans in
important workplace decisions.

**Figure 47: Employee reliance**
**on AI at work**

‘At your work, how often have you…’

% Never %Rarely %Sometimes to very often

Relied on AI to do a task rather than learning
how to do it yourself













% Sometimes to very often = ‘Sometimes’,
‘Often’, or ‘Very often’

**Figure 48: Preference for human–AI**
**involvement in managerial**
**decision-making**

‘Which of the following proposals do you find most
acceptable for managerial decision-making activities?’

% selected






Humans

100%


Humans

75%,
AI 25%


Humans

50%,
AI 50%


Humans AI

25%, 100%
AI 75%


© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **79**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**Organizational support for AI and**
**its responsible use is lagging behind**
**adoption, particularly in advanced**
**economies**

The extent of complacent and inappropriate use of
AI within the workplace highlights the importance
of organizational support and governance of
responsible AI use. Employees in _organizations_
_that are actively using AI_, were asked whether
their organization: a) has an AI strategy and culture,
b) supports AI literacy and responsible use by
employees, and c) has responsible AI governance
practices in place, such as regular monitoring of AI
systems, accountability systems to oversee AI use,
and data privacy and security measures.

We find substantial variation between advanced
and emerging economies (see Figure 49).
In advanced economies, just over half of
employees report that their organization has
mechanisms in place to support AI adoption and
responsible use, including a strategy and culture
conducive to responsible AI adoption, adequate
employee training, and governance processes.
Only 55 percent believe there are adequate
safeguards within their organization to ensure
responsible AI use. While these findings are
based on employee perceptions and awareness
of these organizational support mechanisms,
they suggest that just under half of organizations
in advanced economies may be using AI without
adequate support and governance.

In contrast, in emerging economies, about
70 percent say their organization has a clear
AI strategy, offers responsible AI training,
and 65 percent report AI governance policies.
Furthermore, 71 percent feel assured that
sufficient safeguards exist for responsible AI
use. This higher level of organizational support
for AI aligns with the greater reported employee
use of AI and higher levels of AI education and
training, AI knowledge, and efficacy reported in
emerging economies.

These findings are based on employees who report
working in organizations that are actively using AI.
We anticipate considerably lower organizational
support for responsible AI in organizations that are
considering but have not yet actively taken steps
to integrate AI into their operations.

© 2025 The University of Melbourne.

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.


Only
# **55%**

of employees in advanced
economies feel there are
adequate safeguards within
their organization to ensure
responsible AI use.

This suggests
that just under
# **half**

of organizations in advanced
economies may be using AI
without adequate support
and governance.

Trust, attitudes and use of AI: A global study 2025 | **80**

**Figure 49: Perceived organizational support for AI and responsible AI use**

‘In relation to your organization, to what extent do you agree with the following?’

% Agree advanced economies % Agree emerging economies


**AI strategy and culture overall**

AI adoption is considered strategically important

There is an AI strategy

Efforts to integrate AI into the organization are recognized

People are encouraged to use AI at work







**Support for AI literacy overall**

Employees are supported to understand AI systems

Employees support each other to learn
and integrate AI tools at work

The organization supports employees in understanding
the responsible use of AI systems

Training in the responsible use of AI
is provided to employees

**Responsible AI governance overall**

There are policies and practices to govern
the responsible use of AI

People are informed when AI is being used to make
or inform decisions about them

AI systems are regularly monitored to ensure
they operate as intended

Data privacy and security measures are in place
to protect people’s data

There are people accountable for overseeing
the organization’s use of AI

% Agree = ‘Somewhat agree’, ‘Agree’, ‘Strongly agree’.
Based on employees working in organizations that are actively using AI.

Country-level data further illustrate these
differences (see Figure 50).

Over 70 percent of employees in India,
Nigeria, Egypt, China, the UAE, Saudi Arabia,
Türkiye, South Africa, and Brazil report strong
organizational support for responsible AI. Among
advanced economies, Singapore, Switzerland,
























the UK, Norway, Italy, and Denmark lead, with at
least 60 percent of employees reporting robust
organizational support. In contrast, employees
in Portugal, Slovenia, the Czech Republic, and
Finland report some of the lowest levels of
organizational support.


© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **81**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**Figure 50: Organizational support for AI and responsible use across countries**

% AI strategy and culture % Support for AI literacy % Responsible AI governance

40% 45% 50% 55% 60% 65% 70% 75% 80% 85% 90%

**India**

**Nigeria**

**Egypt**

**China**

**United Arab Emirates**

**Saudi Arabia**

**Türkiye**

**South Africa**

**Brazil**

Singapore

**Costa Rica**

Switzerland

United Kingdom

**Mexico**

**Colombia**

Norway

Italy

Denmark

USA

**Argentina**

**Chile**

**Romania**

Estonia

Australia

**Poland**

Ireland

**Hungary**

Canada

Belgium

Spain

Latvia

Sweden

France

Lithuania

Japan

Germany

Austria

Greece

Israel

Netherlands

Slovak Republic

New Zealand

Korea

Finland

Czech Republic

Slovenia

Portugal

% Agree = ‘Somewhat agree’, ‘Agree’ and ‘Strongly agree’; [7 point scale]. Based on employees working in organizations
that are actively using AI. Bolding indicates countries with emerging economies.

© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **82**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**AI’s impact on work and jobs: Only one**
**in three believe AI will create more jobs**
**than it will eliminate**

Employees are conscious of the potential impact
of AI on work and jobs (see Figure 51). Over half
agree that the way they do their daily work will
change because of AI.

In terms of job impacts, less than a third believe
AI will create more jobs than it will eliminate.
Rather, almost half believe the opposite—that
AI will eliminate more jobs than it will create.
This aligns with our earlier-reported finding that
the potential for job losses from AI technology
implementation is a key societal concern and is
experienced or observed by two in five people. [46]

Employees are split in their views on whether
AI can perform key aspects of their work and
will replace jobs in their specific area of work.
This likely reflects the diverse range of jobs,
occupations and industries represented in the
survey sample, and the extent to which AI
systems and capabilities are useful in these jobs.

**Figure 51: Perceived impact of AI on jobs**

‘To what extent do you agree with the following?’


Our earlier finding that one in five employees
report reduced job security from the use of AI
suggests that a minority are directly experiencing
AI-related job insecurity.

People in emerging economies are more optimistic
about job creation from AI, with 39 percent
agreeing AI will create more jobs than it will
eliminate, compared to 23 percent of those in
advanced economies. This is not blind optimism.
Employees in emerging economies are also more
likely than those in advanced economies to agree
that key aspects of their work could be performed
by AI (53% vs. 35%), how they do their work will
change due to AI (64% vs. 48%), and more are
concerned about being left behind if they don’t
use AI (56% vs. 42%).

We next examine what encourages employee
use of AI at work, and, importantly, what predicts
critical engagement with AI tools.


The way I do my daily work will change because of AI

Key aspects of my work could be performed by AI

AI will replace jobs in my area of work

AI will create more jobs than it will eliminate


% Disagree % Neutral % Agree












% Disagree = ‘Somewhat disagree’, ‘Disagree’, or ‘Strongly disagree’
% Agree = ‘Somewhat agree’, ‘Agree’, or ‘Strongly agree’

© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **83**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**What predicts the use and critical**
**engagement of AI at work?**

The findings on employee use of AI highlights
that organizations must navigate a complex
balance between promoting AI adoption to realize
the benefits, while simultaneously encouraging
thoughtful, critical engagement with AI tools that
underpins responsible use.

To help inform how this balance can be achieved,
we conducted statistical modelling to identify the
key predictors of AI use and critical engagement
with AI at work, using the same techniques
explained in the section ‘What are the key drivers
of trust and acceptance of AI systems?’


The predictors examined align with the four
pathways discussed earlier: AI literacy (knowledge
pathway), perceived performance benefits of AI at
work (motivation), perceived negative impacts of
AI use (uncertainty), and organizational support for
AI, AI literacy, as well as responsible AI governance
(institutional pathway). Additionally, the impact of
trust in AI at work was examined. These models

were tested using data from employees in
organizations that use AI.

Our analysis revealed that each of the four
pathways predicts both the frequency of AI use
at work and critical engagement with AI, but in
different ways.




These combined results highlight that AI literacy
is a key lever, as the strongest predictor of both
AI use and critical engagement. Experiencing
positive performance benefits from AI also
motivates both use and critical engagement.
In contrast, experiencing negative impacts from
AI reduces adoption but can prompt employees
to adopt a more critical and discerning stance.

Our findings show that trust in AI systems
encourages employee adoption, but its negative
impact on critical engagement highlights the


need for organizations to avoid fostering blind,
uncritical trust in AI tools. Instead, employees
should be supported to calibrate their trust based
on the technology’s trustworthiness and reliability.

Cultivating an AI-friendly culture and strategy can
help to encourage employees to use AI more
frequently, whereas responsible AI governance
mechanisms help to prompt deeper critical
reflection when using AI tools. We explore the
implications of these findings further in the
Conclusions and Implications section.


© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **84**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**How do demographic factors influence**
**use and perceptions of AI at work?**

There are notable differences between subgroups of employees in their
use, trust, perceptions and realized benefits from AI use in the workplace,
all of which have implications for the management of AI. We note at the
outset that there are no gender differences in AI use or attitudes toward
AI at work.


**Employees who are younger, AI trained,**
**university-educated, higher-income**
**earners and managers are more likely**
**to use and trust AI at work and believe**
**AI will change aspects of their work**

As shown in Figure 52, younger people (aged
under 35), those with AI training, universityeducation, or higher incomes, and managers
are more likely to use AI for work purposes and
to trust AI in the workplace. These groups are
also more likely to report that their organization
uses AI, fosters an AI-driven culture, and
supports responsible AI use. The largest
differences are seen in relation to AI training
and income.

This pattern mirrors our previously reported
findings that these groups are more trusting
and accepting of AI use in society and have
higher levels of AI literacy, (see Figures 33-35).

These groups are also more likely to agree
that AI will perform key aspects of their job
and agree that AI will change the way they do
their daily work (67% AI trained vs. 43% no
AI training; 62% university educated vs. 44%
no university education; 41% managers vs.
21%-29% other occupations). Managers and
high-income earners are also more likely to
agree that AI will create more jobs than it will
eliminate (65% managers vs. 36-56% other
occupations; 54% high-income vs. 26% and
17% of middle- and low-income respondents,
respectively).


Taken together, these findings suggest that
these groups are better positioned to integrate AI
into their work and realize performance benefits
(see below). Conversely, employees without
these attributes—namely older, lower-income
employees, those without AI training or university
education—may be at risk of being left behind
and experience what has been called ‘AI divide’ in
terms of progression, opportunities and benefits.

**High-income earners, those with AI**
**training and managers report the most**
**positive impacts from AI at work**

As shown in Figure 53, higher-income earners,
those with AI training, and people in managerial
positions are more likely to report experiencing
positive impacts from AI at work compared to
middle- and low-income earners, employees
without AI training and those in non-managerial
occupations.

To illustrate specific positive impacts, highincome earners are more likely to have
experienced increased quality or accuracy of
work (72%) compared to middle- (54%) and lowincome respondents (44%). Those with AI training
and managers are more likely to report increased
efficiency due to AI (76% vs. 56% without AI
training; 75% of managers vs. 55-67% in other
occupations) and increased revenue-generating
activity from AI (55% vs. 34% without AI training;
59% of managers compared to 40-43% in other
occupations).


© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **85**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**Younger employees, those with**
**AI training, and those with higher**
**incomes are more likely to engage in**
**inappropriate and complacent use of**
**AI at work**

It is notable that some of these groups are also
the most likely to use AI inappropriately. After
accounting for frequency of AI use at work in
analyses, [48] younger employees, those with AI
training, and higher-income earners are more
likely to use AI in their work in inappropriate or
complacent ways.

As shown in Figure 53, 65 percent of younger
employees (aged under 35) report engaging in
complacent and inappropriate use behaviors,
compared to half or fewer of older employees
([effect size] n²=.04). [49 ] Similarly, employees with
AI education or training report higher rates of
complacent and inappropriate use (63% vs.
46%; n²=.03), though they are also more likely
to engage critically with AI in their work (53% vs.
40% most of the time or always; n²=.03).

Income also plays a role, with higher-income
earners (70%) being the most likely to report
complacent or inappropriate AI use. Notably, they
are also more likely to engage in AI behaviors that
contravene AI policies than other income groups
(n²=.03).

While frequency of use explains some of the
variation in inappropriate and complacent use,
it does not fully account for the observed
differences in these groups. Other underlying
factors such as understanding of AI, workplace
norms, or training, may shape how AI systems
are being used. For example, these groups may
have developed ways of using and relying on AI
in their work before guidelines were established,
leading to the formation of unhealthy complacent
norms. The higher trust levels in AI among these
groups may also influence them to over-trust
and rely on these technologies more than other
groups. In addition, these groups may feel that
their heightened understanding of AI or seniority
gives them license to decide how best to use AI.


**Employees working in the IT, finance**
**and insurance, and media and**
**communications sectors report the**
**highest AI adoption and those in**
**government and public administration**
**report the lowest adoption**

We sampled employees in each of the 18
sectors shown in Figure 54. [50] Sampling was
naturally occurring rather than representative of
each industry and ranged from 527 employees
in the real estate industry to 3,415 employees
in the manufacturing sector and are based on
employee perceptions and experiences. As such,
the findings should be interpreted as indicative
of broad trends.

Our analysis revealed statistically significant
differences between industries on a range of
indicators, most notably:

- Employees in the Information Technology (IT),

Media and Communications, and Financial and
Insurance sectors report the highest use of AI
at work (72-85%, see Figure 54) and greatest
organizational adoption of AI (90-94%).

- Employees in the IT and Financial and

Insurance sectors also report the greatest
organizational support for AI (75-76%), trust
in the use of AI at work (62-67%), beneficial
impacts from AI use (63-66%), and job impacts
from AI (68-72%).

- In contrast, employees in the Government and

Public Administration, Healthcare and Social
Assistance, and Transport and Logistics sectors
report the lowest employee adoption of AI
(43-47%), organizational adoption (61-63%),
organizational support for AI and its responsible
use (55-60%), and the least beneficial impacts
from AI (48-52%).

- Employees in the Arts, Entertainment and

Recreation Services and Healthcare and Social

Assistance sectors report the lowest trust in
AI at work (46-48%) and are the least likely to
believe that AI can perform key aspects of their
work (33-35% agree).

- After accounting for frequency of use in

analyses, there are no differences in complacent
or inappropriate use between industries.


© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **86**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**Figure 52: Demographic differences in trust and use of AI at work**

% Using AI at work % Trust at work

**Age**


72



56



40























18–34 year olds

35–54 year olds

55 and older

**Income**

High

Middle

Low

**AI training**

AI training

No AI training

**Education**

University education

No university education

**Occupation**

Manager

Professional and skilled

Clerical, service and sales

Manual



% Trust at work = 'Somewhat willing', 'Mostly willing', 'Completely willing'
% AI use at work = ‘Occasionally (every few months)’ to ‘Always (multiple times a day)’

**Figure 53: Demographic differences in complacent use and positive impacts of AI**



70



% Positive AI impacts % Complacent use at work

**Income**

High

Middle



53


45


47





Low

**AI training**

AI training

No AI training

**Occupation**

Manager

Professional and Skilled

Clerical, Service, and Sales

Manual

**Age**

18–34 year olds

35–54 year olds

55 and older











% Complacent use at work = ‘Rarely', ‘Sometimes’, ‘Often’, and ‘Very often’
% Positive AI impacts = ‘Slightly increased', 'Increased', 'Greatly increased'

© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **87**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**Figure 54: Industry differences in use of AI and organizational support for AI**

% AI use at work **Industry** % Organizational support


85


76

75


75

72


66

66

65

64

59

59

58

54

54

54

51

47

46

45

43


Information Technology

Media and Communications

Financia and Insurance Activities

Power, Energy, Utilities,
Mining and Natural Resources

Scientific and Technical Services

Education and Training

Real Estate Activities

Arts, Entertainment and Recreation Services

Professional Services

Construction

Administrative and Support Services

Agriculture, Forestry and Fishing

Manufacturing

Retail or Wholesale Trade

Transport, Logistics, Storage and Postal

Accommodation and Food Services

Health Care and Social Assistance

Government, Public Administration,
Defense, and Safety


61

65

61

61

66

61

60

60

58

55


59

58


67

66

65

65


% AI use at work = ‘Occasionally (every few months)’ to ‘Always (multiple times a day)’
% Organization Support = ‘Somewhat Agree’, ‘Agree’, ‘Strongly Agree’

~~**I**~~ **n summa** ~~**ry**~~

Taken together, these findings reveal a complex and nuanced picture of AI
use in the workplace. A majority of employees are intentionally using AI
at work, and experiencing positive impacts, particularly performance and
efficiency benefits. However, there are mixed effects from AI integration,
particularly on workload, stress, and collaboration. Many employees are
using AI in ways that are inappropriate or complacent, with organizational
support and governance for responsible AI use perceived to be lagging,
particularly in advanced economies. These factors, combined with the
insight that most employees use free, publicly available generative AI tools
in organizations that lack clear policies on its use, opens up substantial
organizational risk. While most employees trust AI at work and accept its
involvement in managerial decision-making, rapid adoption is reshaping
workflows and deepening dependency on human-AI collaboration.

In the final empirical section, we examine AI use by students who represent
the workforce of the future.

© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **88**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

~~**SECTION**~~ ~~**THREE**~~
#### **Student attitudes ** **towards AI in ** **education**



**How is AI being used by students?**


**Four in five students regularly use AI in**
**their studies**

Most students (83%) use AI in their studies
on a regular basis, with half using it weekly or
daily. Eighty-three percent of students also use
AI for personal, non-study-related purposes at
least semi-regularly.

Students are more likely to use AI in their
studies than employees are in their work (83%
vs. 58% use AI regularly or semi-regularly; see
Figure 55).

About half of students (53%) report trusting AI
tools in their studies, which mirrors the finding
for employees (52% trust).

While about half (53%) report receiving AI
education or training, 72 percent indicate that
they have at least moderate knowledge about
AI and feel they can use AI tools effectively.


Collectively, these results suggest most
students feel confident in their knowledge
and ability to use AI systems.

Of the few students who do not use AI in their

studies (8%, n=195), the most common reasons
are that they prefer to do their work without AI
(55%), followed by the belief that AI tools are
not helpful or required (34%), and that AI will
have a negative impact on their learning (31%).

**Freely available general-purpose**
**generative AI tools are most used**
**by students**

Mirroring the pattern for employees, students
are most likely to use general-purpose
generative AI tools (89%) and voice-based
AI assistants (42%) in their studies, (see
Figure 56), and are much more likely to use
free, publicly available tools (89%) than tools
provided by their education provider (26%), or
those that require payment to access (12%).


**Figure 55: Frequency of student use of AI compared to employee use of AI for work**

'In your studies/work, how often do you intentionally use AI tools, including generative AI tools?'

% Student % Employee

40


35

30

25

20

15

10

5

0





a year months month week

Daily = ‘most days’ or ‘multiple times a day’

© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **90**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**Many students use AI inappropriately**
**or complacently**

Only half of students (52%) who use AI in their
studies critically engage with it on a regular basis,
for example by evaluating AI output or verifying
its accuracy before using it, and considering the
limitations of an AI tool when making decisions
based on its output. Rather, many students report
using AI in complacent or inappropriate ways
(see Figure 57).

Almost three in five (59%) students report
having used AI in ways that contravene their
education provider’s policies or guidance.
Over half (56%) say they have used AI tools
in ways that could be considered inappropriate,
and 84 percent state that they have seen
or heard of other students using AI tools in
inappropriate ways.

Most report using AI in ethically ambiguous and
non-transparent ways, such as using AI tools
without knowing whether it is allowed, avoiding
revealing when they have used AI tools in their
coursework, and presenting AI-generated content
as their own.


The findings also suggest that students are
becoming increasingly dependent and over-reliant
on AI tools in their studies, with implications for
learning. Over three quarters say they have relied
on AI to complete tasks rather than learning how
to do them themselves, or felt unable to complete
their coursework without its help (see Figure 57).
Eighty-one percent say they have put less effort
into their studies or assessment knowing they can
rely on AI, and two-thirds have made mistakes in
their work due to AI.

One potential contributor to the inappropriate
and complacent use of AI may be a sense of
competitive pressure to use AI tools, with half of
students indicating they are concerned about being
left behind if they don’t use AI tools in their studies.
Such competitive pressure could lead to increased
use and greater dependence on AI, potentially
cascading into complacent use.

Students are more likely to report inappropriate
or complacent AI use and over-reliance on AI in
their studies than employees are in their work.
For example, around three quarters (76%) of
student AI users say they have relied on AI output
without evaluating the information or felt unable to
complete their work without AI (77%), compared
to two thirds (66%) of employees.


**Figure 56: Types of AI tools intentionally used for study, compared to employees**

'What are the main types of AI tools you use intentionally for study? Select all that apply'

% Study


General-purpose generative AI tools
(e.g., ChatGPT, Copilot, Claude)

Voice-based AI assistants (e.g., Siri, Alexa,
Google Assistant)

Image / video / audio generators
(e.g., DALL-E, Canva)

Specific-purpose generative AI Tools
(e.g., Grammarly, Github)

AI systems developed or customized
specifically for your education provider

Other specific-purpose AI tools (e.g., for
predictive analytics, workflow automation)

Robots and physical autonomous systems
(e.g., manufacturing robots)









© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **91**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**Figure 57: Inappropriate and complacent use of AI in education**

'As a student, how often have you...'

% Never % Rarely % Sometimes to very often


**Overall**

**Contravening policies**

Used AI in ways that contravene policies or guidelines

Uploaded copyrighted material or IP to a generative AI tool

**Ethically ambiguous**

Seen or heard of people using AI tools in their course in
inappropriate ways

Used AI tools in your course without knowing whether
it is allowed

Used AI tools in ways that could be considered inappropriate

**Non-transparent use**

Avoided revealing when you've used AI tools in your work

Presented AI-generated content as your own

**Quality Issues**

Put less effort into study or assessment tasks knowing
you can rely on AI

Relied on AI output in your studies without evaluating
the information

Made mistakes in your work due to AI

**Overreliance**

Asked AI a question instead of your teacher or lecturer

Relied on AI to do something rather than learning how
to do it yourself

Used AI rather than collaborating with or involving others
to get work done

Felt you could not complete your work without the help of AI

% Sometimes to very often = ‘Sometimes’ or ‘Often’ or ‘Very often’



























© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **92**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**What are the impacts of AI use in education?**


**Students experience positive impacts**
**of AI use in education, but AI’s influence**
**on social dynamics, critical thinking,**
**and fairness and equity is mixed**

Figure 58 shows the impacts of AI use in
education. The purple bars show a positive impact,
for example by increasing efficiency and decreasing
stress and pressure. The blue bars indicate a
negative impact, for example by reducing critical
thinking and increasing time on mundane tasks.

As shown in this figure (see purple bars), the
majority of students report notable positive impacts
from the use of AI in their education, including
increased efficiency, quality and accuracy of
work, idea generation and innovation, and the
personalization of learning. Over half also report
reduced workload and stress and pressure.

However, there are also mixed impacts. A quarter
to a third of students (27-36%) report the use
of AI in education has reduced critical thinking,


as well as communication, interaction, and
collaboration with instructors and peers, trust
of students by instructors and peers, and the
fairness and equity of assessments, while similar
proportions report AI has had a positive impact
on these outcomes. There are also mixed impacts
on skill development and time spent on mundane
tasks, with almost half of students reporting
positive impacts and a quarter to a third reporting
negative impacts.

These findings suggest that while AI can offer
substantial advantages—particularly for completing
tasks—AI use may also inadvertently hinder key
essential interpersonal and cognitive skills, and as
well documented, raise challenges for the fairness
and equity of assessment.

Students’ perceptions of the impacts of AI
on jobs and the world of work broadly mirror
those reported for employees. Fewer than one
in three believe AI will create more jobs than
it will eliminate, with almost half disagreeing.


© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **93**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**Figure 58: Impacts of AI use in education as reported by students**

‘In your experience, how has the use of AI tools in your workplace impacted:’

% Reduced/negative impact % No impact % Increased/positive impact


Efficiency of work

Access to accurate information

Quality or accuracy of work

Idea generation and innovation

Personalization of learning

Use and development of skills and abilities

Critical thinking

Fair and equitable assessment of student work

Communication, interaction, or collaboration with
teachers/lecturers and peers

Trust in students





















% Reduced/positive impact % No impact % Increased/negative impact


Workload

Stress and pressure

Time spent on repetitive or mundane tasks
including searching for information

% Reduced = ‘Slightly reduced’, ‘Reduced’, or ‘Greatly reduced’
% Increased = ‘Slightly increased’, ‘Increased’, or ‘Greatly increased’
Purple bars indicate positive impacts.

**Support for responsible AI use in**
**education is lagging adoption: only half of**
**students report their education provider**
**has a policy guiding generative AI use**

Despite the pervasive use of AI by students, only
half of the students surveyed (49%) believe their
education provider has appropriate safeguards in
place to make them feel comfortable with the use
of AI in learning and teaching.

Only half report their education provider supports
responsible AI use by having policies in place to
ensure equitable use in learning and assessment
and providing students access to training and
resources on responsible use (see Figure 59).
This low investment may reflect that only half
of students report that their education provider








encourages students to use AI in their learning
and supports them to innovate with AI.

Given the high use of generative AI by students,
it is notable that less than a third report that
their education provider has a policy in place to
guide the responsible use of generative AI by
students, and one in five indicate that there are
policies banning generative AI use (see Figure
60). A quarter of students do not know if there is
a policy in place, suggesting a lack of awareness
may be contributing to complacent use.

These student-reported insights suggest many
education providers are not adequately supporting
students in the responsible use of AI or are not
making students sufficiently aware of relevant
policy, training and resources.


© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **94**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**Figure 59: Education provider support for responsible AI use as reported by students**

‘In relation to your education provider, to what extent do you agree with the following?’

% Disagree % Neutral % Agree


There are policies to ensure the responsible
use of AI in learning and assessment

Students have access to training and resources
to help them use AI systems responsibly

The education provider supports students in
understanding the responsible use of AI systems

Students are supported to
understand AI systems

Students are supported to use AI to
innovate and do things differently

Students are encouraged to use AI to
support their learning

% Disagree = ‘Somewhat disagree’, ‘Disagree’, or ‘Strongly disagree’
% Agree = ‘Somewhat agree’, ‘Agree’, or ‘Strongly agree’








**Figure 60: Education providers’ guidance on generative AI use for students**

‘Has your education provider put in place a policy or provided guidance on the use of generative AI for students?’




% with policy guiding the use of GenAI

% with policy banning use of GenAI


% No

% Don't know




~~**I**~~ **n summa** ~~**ry**~~

These findings highlight that while most students are using and benefiting from
AI, the complacent and inappropriate use of AI in education is widespread, and
students are experiencing mixed impacts from AI use. Furthermore, education
providers appear to be lagging in providing adequate training, resources and
policy guidance to support and enforce the responsible use of AI by students.
These findings have implications for the effective development of critical skills
and the integrity of assessment in the AI age, and the future of work, as these
students become the workforce of the future.

We next discuss the implications of these findings and the broader
research insights.

© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **95**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

##### **Conclusion and ** **im lications** **p**

The research insights from this global survey highlight the current and future opportunities
and challenges of responsibly stewarding AI into work, education, and society.

Our findings reveal rapid adoption of AI despite substantial public ambivalence toward its
use. Although a clear majority recognize the technical competence, utility, and benefits
of AI, fewer are assured of its safety and security, and many are concerned about the
societal impacts. This ambivalence manifests in the cautious acceptance of AI coupled
with limited trust, and optimism about its benefits coupled with worry about the risks. [51 ]

Underlying this ambivalence is the tension between realizing the benefits of AI and ensuring its
responsible use. This tension is evident at multiple levels:

  - At the societal level, governments seek to realize the national economic and productivity gains from

AI and harness its potential to transform social services and address societal challenges, whilst also
exercising their responsibility to protect societal values and safeguard citizens from harm and unfair
treatment. Some jurisdictions view ongoing and significant investment in responsible AI as key to
fostering competitive advantage. [52]

  - Citizens want to benefit from the promise of AI, while feeling safe and avoiding manipulation, fraud,

privacy and IP loss, bias, and the damaging societal consequences of prolific mis- and disinformation.

  - At the organizational level, leaders seek to realize enhanced productivity, innovation, value creation

and competitive advantage from AI, whilst mitigating material and reputational risks and building
sustained stakeholder trust.

  - At the individual level, employees and students seek to enhance efficiency, quality and creativity

in work and study, while avoiding deskilling, loss of jobs and the erosion of meaningful human
connection. Some feel they have little choice but to adopt AI, fearing that not using it risks them
becoming uncompetitive and left behind.


This tension helps explain why the pace of AI
adoption in the quest for performance gains has
often outstripped AI literacy, training, governance,
and regulation. It is also why there is a public
mandate for stronger regulation and governance
of AI, and growing desire for assurance of its
trustworthy use.

Effectively navigating this tension is one of the
grand challenges of our time [53 ] and will require


proactive and sustained action and effort from
multiple actors at all levels.

To help address this challenge, we draw out the
insights and implications of the research for key
groups at the forefront of AI adoption, integration,
governance and regulation. These include
government policymakers, regulators and citizens;
organizational leaders, managers and employees;
and education providers and students.


© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **96**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**Emerging economies are leading**
**in public and employee AI adoption,**
**trust, acceptance and realized benefits**

A key insight is the notable difference in adoption
and sentiment toward AI between countries with

emerging and advanced economies. People in
emerging economies report accelerated adoption
and a pattern of greater trust, acceptance, and
positive attitudes toward AI. They also selfreport higher levels of AI literacy and training,
realized benefits both in work and society,
and organizational support and governance for
responsible AI use. [54] This pattern is particularly
strong in countries such as India, China, Nigeria,
Egypt and the UAE.

This pattern may be due to the increasingly
important role that transformative technologies
play in the economic development of these
countries [55] and the greater relative benefits and
opportunities AI affords people in emerging
economies. [56] For example, AI systems may
help fill critical resource gaps in these countries
by enabling access to quality information and
services where access is limited.

AI may provide augmented opportunities for
people and organizations in emerging economies
to overcome economic disadvantage and barriers.
By bridging gaps in language, skills, information
or networks, people in these countries may
enhance their competitiveness and be able to
seize a broader range of work and economic
opportunities, including access to global markets.
This potential may encourage a growth mindset that
motivates trust, acceptance, and use of technology
as a means to accelerate economic progress,
prosperity, and quality of life. It may also motivate
investment in AI training and literacy as a foundation
for realizing and augmenting the benefits.

The greater levels of trust and acceptance seen
in emerging economies can be explained, in part,
by the pathways in our model. Higher levels of AI
literacy (knowledge pathway), greater perceived
and experienced benefits (motivational), and more
favorable views of the adequacy of regulation
and confidence in industry to develop and use AI
responsibly (institutional) help to reduce concerns
about risks (uncertainty pathway) and shape the
view that benefits outweigh risks.

Similar pathways also help to explain why
emerging economies are leading in AI workplace
adoption and trust at work, with employees
reporting more beneficial outcomes from
organizational AI use, as well as higher levels


of AI training and literacy and more perceived
organizational support for responsible use, which
helps mitigate risks and uncertainty.

These insights raise the question of whether
governments and organizations operating in
advanced economies need to augment investment
and support in AI training and literacy, as well as
strategic use and governance of AI to help realize
benefits and support adoption.

Looking ahead, the nations that accelerate
in responsible adoption may be uniquely
positioned to gain long-term competitive and
strategic advantage if AI becomes a central
driver of productivity, innovation, and progress
on societal challenges, such as climate change.
This potential advantage, combined with the
increasing importance of AI for national security,
could prompt new dynamics in international
relations, including debates around access to
AI technologies and whether restrictions might
emerge in response to perceived strategic or
economic gains.

While a challenge to AI adoption, we caution
against viewing the lower trust and acceptance
in advanced economies as a deficit. Rather it can
be viewed as appropriate rational caution based
on the perceived state of AI use in society, the
current levels of governance, regulation and
standards supporting it, coupled with low levels
of AI literacy. Well-placed trust in AI systems is
grounded in informed and accurate assessments
of their benefits, limitations, and safeguards.

Interventions to strengthen trust and acceptance
can focus on enhancing the adequacy of regulation
and investing in initiatives to mitigate negative
outcomes from AI use, designing and deploying
AI systems to maximize beneficial outcomes and
reduce risks (e.g. privacy by design), strengthening
organizational assurances and governance of
trustworthy use, and systematically improving AI
literacy, through public and employee AI education
programs, for example.

Our findings further suggest the high trust
and acceptance levels in emerging economies
are not based on blind optimism: people in
these economies perceive and experience
negative outcomes of AI in a similar way to
those in advanced economies. Rather, they
experience augmented benefits, which offset
these risks. However, it is important to guard
against overconfidence and complacency that
can stem from high trust by encouraging critical
engagement, for example.


© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **97**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

While there is a distinct pattern between advanced
and emerging economies, it is important to
recognize that countries within these broad
categories vary substantially in their economic,
cultural, political, and historical contexts, and there
are country exceptions that don’t fall neatly into
these patterns.

**There is a public mandate for AI**
**regulation with the current regulatory**
**landscape falling short of expectations:**
**implications for policymakers**

The 47 countries surveyed represent a variety
of approaches and stages in AI regulation and

governance.

At the time of data collection, countries such as
Singapore and China stood out for the breadth
of regulatory and governance measures that
had been implemented. Other jurisdictions,
such as the European Union and Korea, had
designed comprehensive AI laws and regulatory
frameworks and were in the process of
implementation. Countries such as Australia,
India, and Canada were debating proposed AIspecific legislative frameworks. Similarly, a range
of countries—including emerging economies
such as Saudi Arabia, Türkiye, and the UAE—had
implemented or proposed AI guidelines without
adopting comprehensive legislation. The UK
and the USA (including individual US states)
had launched multiple initiatives but lacked a
unified regulatory approach. Notably, after data
collection, the US Government scaled back its
approach to AI regulation, and while 58 countries
signed the Paris AI Action Summit agreement,
the USA and UK did not. [57]

In the context of this lack of a globally
consistent regulatory approach, our findings
provide important insights and evidence on
public expectations surrounding the regulatory
landscape for AI.

They reveal a clear public mandate for robust, fitfor-purpose AI regulation underpinned by globally
shared concerns surrounding the societal risks
and negative outcomes from AI, and low public
trust in the safety and security of AI use.

The majority of people in all countries expect a
multipronged regulatory approach, supporting
both international and national laws and

regulation, and expecting government and existing


regulators to play a leading role. They also expect
industry to be involved, working together with
government and regulatory bodies through coregulation, and aligning organizational governance.

The near universal endorsement and preference
for international-level laws and regulation
indicates public recognition that AI is not bound
by national borders and is often developed by
multinational companies who operate crossborder, which can constrain the ability of a
national government or regulatory body to
develop and enforce regulation. International
standards (e.g. from the International Standards
Organization [ISO]) can provide governments
and industry with interoperable frameworks for
regulation and governance.

In contrast to these expectations, the majority
view the current regulatory landscape as
inadequate and falling short in making AI use safe.

This gap between public expectations and the
current regulatory landscape likely reflects
the early stage of regulatory design and
implementation in many jurisdictions. It may also
partly reflect low public awareness of existing
applicable laws in countries where these exist.

To consider and remedy this gap, policymakers
need to not only design, implement and enforce
appropriate AI regulation, but also to educate
and raise public awareness of these laws. This
includes clarifying and raising awareness of how
existing laws (e.g. privacy and consumer laws)
apply to AI in countries where these are in place,
and the rights and responsibilities that each
individual has, as well as the responsibilities of
organizations and governments to manage and
enforce the laws. [58] For example, some people
may not know that under the EU AI Act they have
a right to know when they are interacting with
certain AI applications (e.g. chatbots).

When people believe there are adequate
regulatory safeguards, they are considerably
more likely to trust and accept the use of AI,
underscoring the importance of having an
effective regulatory framework in place and
ensuring it is communicated widely to those that
are governed by it. A clear and effective regulatory
framework and coordinated international

responses provides industry with certainty
and supports sustained safe use and adoption,
as well as interoperability across countries.


© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **98**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

Our findings reinforce that AI-generated
misinformation is a key concern globally [59] and is
undermining trust in online content and raising
concerns about the integrity of elections. There
is strong public support for legislative measures
to combat AI-generated misinformation, with the
public also expecting media and social media
companies to implement stronger fact-checking
and techniques to enable the detection of AIgenerated content (e.g. watermarking). These
expectations stand in contrast with moves by
some social media companies to reduce factchecking on their platforms. [60]

Combatting misinformation and supporting
the public’s ability to detect content generated
and spread by AI bots is critical to supporting
well-functioning democratic processes and
societal cohesion. The widespread adoption of
increasingly sophisticated generative AI tools is
likely to make fake content easier to produce and
disseminate, yet harder to detect.

**The age of working with AI is here:**
**implications for organizational leaders**
**and employees**

Our findings indicate that the age of working
with AI is here, with high rates of self-reported
employee and organizational adoption particularly in
emerging economies, and a preference for humanAI collaboration in managerial decision making.

The use of AI at work is delivering clear
performance-related benefits ranging from
productivity gains, better resource utilization,
greater access to information, enhanced
innovation and knowledge sharing, and increased
revenue-generation opportunities. These benefits
are indicators of the return on investment that

can be realized from adopting AI technology.

However, our research indicates that these
benefits are not guaranteed and are often
accompanied by a concerning pattern of
complacent, inappropriate, and non-transparent
use of AI by employees, which augment
material and reputational risks for organizations,
leaders, and employees alike. Compounding
this complacent use is lagging organizational
governance and support for responsible AI use. [61
]

For example, while most employees are using
public generative AI tools, many organizations do
not provide any policy to guide their use, despite
the risks these public tools pose for privacy


**Key considerations for policymakers**
**and regulators**

  - Analyze gaps in current regulation

and laws.

  - Accelerate the development and

implementation of effective and
enforceable AI regulation at the national
and international level.

  - Collaborate with trusted technical experts

to ensure regulation is effective and
enforceable.

  - Support international coordination and

cooperation to ensure consistent global
standards, interoperability, and mitigation
of AI risks.

  - Communicate and raise public

awareness of legal rights, protections and
responsibilities that relate to common
applications of AI.

  - Invest in public AI training and education to

support AI literacy and responsible use.

  - Invest in methods to combat mis- and

disinformation.

**Key actions for media and social**
**media companies**

  - Invest in fact-checking and other

mechanisms to combat mis- and

disinformation.

  - Develop and use tools that enable and

support users to identify AI-generated

content.

and data leakage, loss of IP, and cybersecurity
concerns. Even when policies are in place, a
worrying number of employees say they are
using these tools in ways that contravene
policies and rules, put company and customer
data at risk, and raise quality issues. The invisible
nature of much of employees’ individual AI work
practices limits the ability to understand and
harness the benefits and manage the risks.

While many organizations are still at an early
stage of their journey with AI [62], these findings
suggest a significant gap between employee
individual adoption and organizational awareness
and preparedness. There is an urgent imperative
to close this gap.


© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **99**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

Our research suggests that organizations can
encourage AI adoption, while simultaneously
promoting critical engagement with AI tools
to combat complacent use, by cultivating an AI
strategy and culture, implementing responsible
AI governance mechanisms, and supporting
employee AI training, literacy and understanding
of AI capabilities, limitations and standards of
responsible use. Each element is critical: the
benefits of AI adoption and integration are more
likely to be realized when organizations have each
of these strategic, cultural, governance and training
mechanisms in place.

There are many resources to help organizations
support the development and implementation of
robust AI governance systems, including several
ISO AI Standards. [63] By simultaneously encouraging
experimentation and mandating responsible
oversight, organizations can foster a sustainable
ecosystem of innovation and performance benefits
without sacrificing the reflection and scrutiny that
is critical for responsible use.

Transparency and accountability are critical to
combat inappropriate use. This requires clear
guidance, policy, training, and oversight, and
also a psychologically safe environment where
employees feel comfortable to openly share
how and when they are using AI tools in their
work. This psychological safety not only enables
better oversight and risk management but
can also support a culture of shared learning,
experimentation, continuous improvement, and
the responsible diffusion of innovation across
the organization (e.g. through communities
of practice), helping to realize more of the
performance benefits offered by AI technologies.
Achieving this requires investment in structures
and strategies to meaningfully engage with,
listen to, and have honest conversations with
employees about AI use and deployment. [64]

Our findings further reinforce that high levels
of trust and use of AI are not simply end goals.
Rather, employees can be supported to develop
appropriate levels of trust based on an informed
understanding of the capabilities, limitations and
risks of the AI system, and its appropriateness
to the task at hand. Fully integrating training
and guidance on responsible AI practices into
everyday workflows—including onboarding
processes, project work, and performance
reviews—can help set healthy workplace


norms around responsible AI use and support
employees to develop well-calibrated trust.

Most employees surveyed want to learn more
about AI, which can serve as a springboard to
upskilling. Our research also suggests employees
with low levels of AI literacy—such as older
employees and those with lower incomes and
no university education—may be at risk of
experiencing what has been termed the ‘AI divide’:
being left behind due to a lack of access or ability to
use AI and benefit from the opportunities it offers.

AI adoption in the workplace is also having mixed
impacts on human collaboration, stress and
workload, employee surveillance, deskilling, and
job security. Proactive management is required
to help ensure that AI integration enhances
rather than undermines trust, wellbeing, and skill
development at work. For example, through work
design that incorporates human–AI collaboration
while preserving human relationships, strategic
workforce planning and reskilling to support job
security, and the ongoing development of human
capabilities to mitigate deskilling and overreliance.

A critical way organizations can help to strengthen
stakeholder trust is by designing and using AI
in ways that create demonstrable benefits and
value for stakeholders, as well as by investing
in assurance mechanisms that support and
signal trustworthy use. The research indicates
that people are more willing to trust AI systems
when assurance mechanisms are in place, such
as meaningful human oversight and accountability
that enables over-riding or challenging AI
recommendations, monitoring of system reliability,
adhering to international AI standards,
and independent third-party AI assurance.

To date, much of the governance of AI has focused
on the integration of AI into services, products
and operations, and ensuring the principles of
trustworthy AI (such as those reflected in the
assurance mechanisms), are put into practice. [65]
The research highlights the need to complement
this governance with greater attention to
employee use of AI and the impacts on work.
Specifically, they highlight a need for organizations
to better govern _how employees are using AI_
_tools and systems_ in their everyday work to create
greater accountability and transparency, and to
proactively _manage and monitor the impacts_
_of AI integration in the workplace_ .


© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **100**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**Key actions for organizational leaders:**

  - Invest in AI literacy to enhance human-AI

collaboration skills, critical engagement,
responsible use and appropriate trust in AI.

  - Establish governance frameworks

that support oversight, accountability,
transparency, and risk management.

  - Embed responsible AI practices into

operational routines and decision-making.

  - Create psychologically safe environments

that support transparent and accountable

use.

  - Create structures to meaningfully

engage with, listen to, and have honest
conversations with employees about AI
use and deployment.

  - Invest in strategic workforce planning

and reskilling to prepare for job and
work changes.

  - Understand, manage, and monitor the

impacts of AI use on employees and
the workplace.

  - Ensure trust is earned, not assumed,

by demonstrating responsible
organizational AI use.

**Educating for an AI-augmented future:**
**Implications for education providers,**
**students, and employers**

The findings reveal that AI use among students
is pervasive, frequent, and driven primarily by
freely available general-purpose generative
AI tools. Students are clearly benefiting from
from increased efficiency, enhanced access
to information, greater innovation, more
personalized learning, and reduced workloads
and stress. However, students also report mixed
cognitive, social-relational, and fairness impacts,
and widespread inappropriate or complacent
use of AI.


**Key actions for managers:**

- Model responsible AI use and set clear

norms and guidelines on appropriate use.

- Encourage ongoing dialogue about AI use,

including where it adds value, where it
introduces risk, and what support is needed.

- Balance innovation with risk management

by supporting safe experimentation while
ensuring compliance with organizational
policies.

**Key actions for employees:**

- Be transparent about when and how AI

tools are being used in work.

- Take initiative in developing AI literacy,

particularly in understanding the strengths,
limitations, and appropriate use cases for
AI tools.

- Critically engage with AI tools and validate

output when important for work.

- Stay informed about organizational policies

on AI use and ensure they are followed.

- Support peers in responsible adoption by

sharing learning, best practice, and raising
concerns about inappropriate use.

The implications of these mixed impacts are
profound. While AI helps content production and
efficient completion of learning and assessment
tasks, it may also weaken the development of
critical thinking, interpersonal skills, and social
dynamics such as collaboration and interaction—all
of which are critical life skills. Without intervention

and management, students—the workforce
of the future—are likely to be tech-savvy with
well-developed AI capabilities, yet potentially
underprepared for work that requires collaboration,
strong interpersonal skills, critical thinking and
completion of work without AI assistance.


© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **101**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

For education providers, these findings emphasize
the need for robust and explicit AI governance
frameworks, as well as educational programs
that develop students’ critical engagement with
AI technologies. The findings suggest many
educational providers are lagging behind in
establishing clear guidance for their students,
highlighting the need to proactively develop,
integrate, and communicate AI policies and
provide appropriate training to support responsible
use and preserve the core educational outcomes
essential to students’ long-term success.

More broadly, the rise of AI is challenging
conventional teaching models, suggesting
a need for ongoing curriculum adaptation to
ensure students are equipped with the skills
to navigate an AI-augmented world, while
continuing to develop their uniquely human
capabilities. Educators must equip students for a
workplace where AI is a ubiquitous tool, ensuring
they develop both human-AI collaboration
proficiency, together with the essential human
skills that underpin leadership, innovation,
collaboration, and ethical decision-making.

Simply banning AI use is not a viable option.
Instead, teaching students how to question,
verify, and critically engage with AI tools is a
critical skillset for the future of work. Ultimately,
the proliferation of student AI use leaves
education providers little choice but to reimagine
a new educational paradigm. This may require
prioritizing collaborative assignments and inperson engagement to ensure interpersonal
skill development and redesigning assessment
methods towards more interactive, processoriented evaluations (e.g. oral exams, in-class
problem-solving tasks) and AI-assisted but
human-verified work.Fostering a culture of
academic integrity—where students see AI
as an aid rather than a shortcut to developing
their skills, knowledge and capabilities—will
be equally crucial.

These insights may also have implications for
the workplace. It will pose a significant challenge
for employers if students—as the workforce of
the future—bring with them engrained norms of
inappropriate AI use and ways of working that are
at odds with organizational responsibilities. This
reinforces the need for AI education, literacy and
critical engagement with AI technologies to start
early and be core to educational programs.


**Key actions for education providers:**

  - Develop and communicate robust

governance frameworks for the
responsible use of AI in learning
and assessment.

  - Develop curricula and pedagogy that

integrate AI literacy, human-AI collaboration
skills, and critical evaluation of AI systems
balanced with the development of uniquely
human capabilities such as collaboration,
teamwork, problem solving, and
ethical reasoning.

  - Use assessment methods that preserve

academic integrity and skill development.

  - Collaborate with industry to ensure

educational curricula prepares students
for the future of work.

**Key actions for students:**

  - Engage with AI tools ethically,

transparently, and in accordance with
institutional guidelines.

  - Take initiative to learn how AI systems

work, understand their limitations, and
critically evaluate their outputs.

  - View AI as a tool to support learning, not

a shortcut: use it purposefully to develop
skills, knowledge and capabilities.

Loss of human interaction due to AI is a
significant societal concern. It is experienced
by most people, including employees and
students who report using AI rather than
collaborating with others to complete work,
raising the question of how human connectivity
can be retained in AI-augmented workplaces,
educational environments, and society at large.
This particular challenge is less amenable to
training, governance, or technical solutions. It
leaves organizations to grapple with building and
preserving meaningful connectivity, purpose, and
belonging amidst increasingly virtual work and
service delivery environments and a drive toward
enhancing efficiency through AI-empowered
technological solutions. Deliberate strategies


© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **102**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

to maintain human connections will become

increasingly essential, not only for attraction and
retention of employees, but also for fostering a
culture of collaboration and shared responsibility
that underpins meaningful work, sustained
performance, and broader societal wellbeing.
There is no easy fix: addressing these challenges
demands sustained organizational commitment
and intentional strategies to balance technological
efficiency with human-centric practices.

Education providers and employers have a shared
interest in ensuring people use AI effectively,
responsibly, and in ways that enhance human
potential and have positive societal outcomes.
Education providers can lay the foundation by
socializing students in responsible use and
critical evaluation of when, where and how to
appropriately use it. Organizations can reinforce
and build upon this understanding through
workplace practices, norms, governance, training
and professional development. A cross-sectoral
approach—rooted in shared responsibility and
mutual learning among students, education
providers, leaders, and employees—is important
to ensure the next generation enters the
workforce not only AI-capable, but also AI-wise.

**Re-imagining the AI-enabled society:**
**stewarding the responsible integration**
**of AI requires a shared commitment**

The public’s shared concerns about AI stem
broadly from three sources: AI malfunctions
(e.g. bias, inaccurate outcomes, system failure),
malicious or misleading use (e.g. misinformation
and disinformation, manipulation or harmful use,
cybersecurity risks), and inappropriate, reckless
or overuse (e.g. deskilling and dependency, loss
of human interaction, loss of privacy or IP). [66]


Addressing and mitigating these root
causes requires a range of technical, social,
organizational, regulatory, and individual actions,
highlighting the need for a coordinated approach
at multiple levels. While our survey suggests the
negative outcomes from AI are experienced less
than the benefits, there is an argument that even
the lowest experienced negative outcomes (i.e.
bias and unfair treatment; experienced by a third)
is unacceptable, and there is a moral obligation to
do better.

These negative outcomes are being experienced
or observed by a significant proportion of people
across each of the 47 countries, indicating that
these are no longer ‘potential’ risks: rather they
are realized impacts. These negative impacts are of
universal concern across the countries surveyed,
and there is broad support for international
cooperation and efforts to address them.

The tension between the undeniable positive
benefits from AI and the realized negative impacts
raises questions about the kind of society and
organizations we want to achieve with AI. Our
survey shows that we are reaping the rewards of
efficiency, effectiveness, innovation, and resource
savings, but are also experiencing loss of human
connection, privacy, mis- and disinformation,
deskilling and dependency. We do not yet fully
understand the long-term impacts, underscoring
the importance of considered choices at every
level about how AI is integrated into society
and work.

We hope this research will support individuals
and organizations to make choices that practically
resolve this tension in favor of AI’s benefits and
inform a clearer vision of how an AI-enabled

society can meet the needs and expectations of
the public and support people and communities
to thrive.


© 2025 The University of Melbourne. Trust, attitudes and use of AI: A global study 2025 | **103**

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

###### **Appendix 1:
** **Methodological and ** **statistical notes**


In this section, we explain the
research methodology and
statistical approach

**Survey piloting, translations and**
**procedure**

The research was approved by and adhered to
the Guidelines of the ethical review process of
The University of Queensland and the National
Statement on Ethical Conduct in Human Research.

The survey was divided into five sections
with questions in each section focused on
the respondent’s: a) demographic details;
b) understanding of AI; c) use and attitudes
toward AI systems (including trust, acceptance,
risks, benefits, impacts and emotions); d)
attitudes toward AI regulation, governance and
management; e) attitudes, use and impacts of
AI at work (only completed by those working) or
in education (only completed by those studying).
At the end of the survey, respondents were
asked a series of open-ended questions.

After completing the first section on use
and understanding of AI, participants read the
definition of AI adapted from the OECD (see
page 16), followed by a description of common
ways AI is used to ensure understanding:
“ _AI is used in a range of applications that do_
_things such as generate text, images, and_
_videos, predict what customers will buy,_
_identify credit card fraud, identify people_
_from their photos, help diagnose disease,_
_and enable self-driving cars_ .”

© 2025 The University of Melbourne.

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.


Questions in sections c and d of the survey
referred to one of three specific AI applications or
referred to ‘AI systems in general’. Respondents
were randomly allocated to one of these AI
applications, providing equivalent numbers of
responses across each. Before answering these
questions, respondents read a brief description
of the AI application, including what it is used
for, what it does and how it works (see full
descriptions on page 16). The research team
developed these descriptions based on a range of
in-use systems with input from domain experts
working in healthcare, Human Resources, and
generative AI.

The survey was extensively piloted and refined
before launch to ensure clarity and construct
validity and reliability. [67] To ensure survey
equivalence across countries, we conducted
translation and back-translation of the English
version of the survey into the native language(s)
dominant in each country, using separate
professional translators. Respondents could also
opt to complete the survey in English if preferred.

To enhance the rigor and quality of the research,
we applied established techniques to filter out
inattentive survey responses. [68] Individuals with
rapid completion times suggestive of insufficient
engagement were removed. We included
attention checks at two points in the survey.
Respondents were excluded if they failed these
checks or failed one while also exhibiting straightlining behaviors (e.g. consistently selecting
the same response across multiple survey
items), nonsensical open-ended responses, or
implausible answers across related question sets.

Trust, attitudes and use of AI: A global study 2025 | **104**

**Survey measures**

Where possible, we used or adapted existing
validated measures from academic research

[(e.g. Haesvoets et al., 2021;](https://psycnet.apa.org/doi/10.1016/j.chb.2021.106730) [Harmon-Jones et](https://doi.org/10.1371/journal.pone.0159915)
[al., 2016; McKnight et al., 2002,](https://doi.org/10.1371/journal.pone.0159915) [2011; Lee &](https://doi.org/10.1145/1985347.1985353)
[Park, 2023; Wang et al., 2023; Zhang & Moffat,](https://psycnet.apa.org/doi/10.1007/s12144-024-05723-0)
[2015) or from previous public attitude surveys](https://www.sciencedirect.com/science/article/pii/S0301420715000069)
(e.g. Ipsos, 2017; [Zhang & Dafoe, 2019).](https://governanceai.github.io/US-Public-Opinion-Report-Jan-2019/)

_Trust_ in each specific AI application was
measured using a reliable 7-item scale adapted
[from Gillespie (2012) and validated in our prior](https://psycnet.apa.org/record/2010-10839-017)
surveys. Example items are: “How willing
are you to… Rely on information or content
provided by an AI system” (willingness to rely);
“Share relevant information about yourself to
enable an AI system to perform a service or
task for you” (willingness to share information);
“Trust AI systems” (direct trust). _Perceived_
_trustworthiness_ was measured using a 9-item
measure assessing positive expectations toward
[the AI system, adapted from McKnight et al.](https://pubsonline.informs.org/doi/abs/10.1287/isre.13.3.334.81)
[(2002). Example items include “I believe most](https://pubsonline.informs.org/doi/abs/10.1287/isre.13.3.334.81)
AI applications: Produce output that is accurate”
(ability); “Are safe and secure to use” (safe and
ethical use).

AI literacy was assessed using two indicators.
_AI knowledge_ was measured with four items
adapted from Ipsos (2017) that assessed people’s
belief that they feel informed about how AI is
used, understand when AI is being used, feel
they know about AI, and feel they have the skills
and knowledge to use AI appropriately. AI efficacy
was assessed with a 6-item measure adapted
[from validated subjective AI literacy scales (Lee &](https://psycnet.apa.org/doi/10.1007/s12144-024-05723-0)
[Park, 2023; Wang et al., 2023). Three items relate](https://psycnet.apa.org/doi/10.1007/s12144-024-05723-0)
to the ability to use AI effectively (e.g. “I can…
Skillfully use AI applications or products to help
me with my daily work or activities”) and three
to the ability to use AI responsibly (e.g. “Identify
potential ethical issues associated with the use
of AI applications”). This was supplemented with
an objective measure of people’s knowledge
of AI use in common applications by asking
respondents whether three common AI
applications (social media, virtual assistants, and
facial recognition) use AI (yes, no or don't know).

_Income_ was measured with a simplified version
of the income question used by the World
[Values Survey (WVS; see Haerpfer et al., 2022).](https://doi.org/10.14281/18241.24)

© 2025 The University of Melbourne.

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.


Specifically, we asked: “Please indicate which
income group best describes your household
income (counting all wages, salaries, pensions and
other income sources).” Responses were provided
on a 1-10 scale, where 1 = Lowest income group,
5 = Middle income group, and 10 = Highest income
group. There was also a ‘Prefer not to say’ option.

Most survey measures used either a 5 or 7-point
Likert scale (e.g., ranging from strongly disagree
(1) to strongly agree (7)). The psychometric
properties of all multi-item constructs were
assessed to examine reliability and dimensionality.
Each measure met the criteria for reliability,
with Cronbach alphas ranging from .81 (critical
engagement with AI) to .96 (organizational support
for responsible AI).

**Data analysis, statistical testing**
**and reporting**

For ease of interpretation, percentages are
reported in most places rather than means.
When percentages did not add up to 100 percent
due to rounding, we distributed the remaining value
based on decreasing order of the values’ decimal
part, as per the Largest Remainder Method.

Some survey response scales provided a
‘don’t know’ option. When 5 percent or more of
respondents selected this option, we include it
in the reporting of percentages. When less than
5 percent, we remove these responses for ease
of interpretation and recalculate percentages
based on the remainder of the data.

Correlational analyses and structural equation
modeling were conducted to examine
associations between concepts. All correlations
reported in-text are significant at p<.001.
Reported relationships are based on theoretical
or hypothesized relationships. Given the data
is cross-sectional and self-reported, causality
between concepts cannot and should not
be inferred.

Our reporting of between-country, betweenapplication, between-people and within-person
differences was based on statistical testing and
adhered to well established benchmarks for

interpreting between- and within-subject effect
sizes (see Cohen, 1988; Lakens, 2013).

Trust, attitudes and use of AI: A global study 2025 | **105**

We used one-way analysis of variance (ANOVA)
to examine differences between countries,
AI applications and people (e.g. age category
differences). We took several steps to ensure
the responsible reporting of only meaningful
differences in the data. First, we adopted a
stringent cut-off of p<.001 to interpret statistical
significance. Where there were statistically
significant differences between groups, we
examined the partial eta-squared effect size
to determine the magnitude of differences
between the groups. Given the large sample
size, trivial effects can reach statistical
significance; thus, we report only those findings
with effect sizes of .03 or greater to focus on
relationships that are substantively meaningful.
This threshold ensures that reported findings
reflect meaningful differences. [69]

We performed paired-sample t-tests to examine
within-person differences (for instance, the
variability in perceptions of the technical ability
of AI systems and their safe and ethical use).
We used a measure of effect size to determine
the magnitude of statistically significant effects.
Specifically, we used Hedges’ g with a cutoff of .30 to indicate a robust and practically
meaningful difference. [70]

© 2025 The University of Melbourne.

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.


Changes over time in the 17 countries surveyed
in both 2022 and 2024 are based on survey
questions asked about three common AI use
applications: AI in general, Healthcare AI, and
Human Resources AI. As such, comparative
data presented is based only on the three AI
applications. Questions about generative AI
were only asked in 2024. Additionally, some
measures were modified between 2022 and
2024, with items added or removed. For these
measures, composite values were recalculated
using only items that remained the same or
were substantively similar across both surveys. [71]

Because the samples at each time point are
independent rather than longitudinal, changes
over time should be interpreted as indicative of
broad trends. We report statistically significant
differences (p<.001) in Appendix 4 and illustrate
the largest changes in the main text. While we
use stringent effect size thresholds (e.g. n² ≥ .03)
in cross-sectional analyses to ensure that only
substantively large differences are highlighted,
in repeated cross-sectional studies even small
but statistically significant changes can signal
consistent and informative population-level trends.

Trust, attitudes and use of AI: A global study 2025 | **106**

###### **Appendix 2:
** **Country samples**

**Overall and country demographic profiles**

The demographic profile of each country sample
was nationally representative of the population
on age, gender and regional location, within a
5 percent margin of error, based on official national
statistics within each country. The few exceptions
are noted below.

Across countries, the gender balance was
51 percent women, 49 percent men and <1
percent other genders, with Costa Rica, Latvia,
and Portugal having the highest representation
of women (54%) and UAE the lowest (32%).

The mean age across countries was 46 years and
ranged from 35 years (Costa Rica and Saudi Arabia)
to 53 years (Japan). There was difficulty in reaching
over-65-year-olds in eight countries: China (over 65s
expected: 17%, achieved: 10%), Egypt (expected:
9%, achieved: 5%), Greece (expected: 27%,
achieved: 16%), Israel (expected: 18%, achieved:
11%), Lithuania (expected: 25%, achieved: 14%),
Portugal (expected: 27%, achieved: 17%), Slovenia
(expected: 27%, achieved: 11%), and Türkiye
(expected: 13%, achieved: 7%). Respondents from
China, Egypt, and Nigeria also tended to be more
urban than the general population. We were unable
to source reliable location data for the UAE and

Slovenia. Data collected in Israel did not include

the West Bank settlement and data collected in

China was contained to mainland China.

Country samples represented the full diversity
of education levels. While levels of university
education broadly matched the respective
populations in most advanced economies, country
samples tended to overrepresent universityeducated people in emerging economies relative
to their respective general populations (using
OECD 2024 education data as a comparison [72] ). It is
common for online survey respondents in countries
with emerging economies to be better educated,
as well as more urban, younger, and affluent, than
those in the general population in those countries. [73]

© 2025 The University of Melbourne.

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.


Given non-representativeness related to age
and education in some of our country samples,
we performed additional robustness checks to
ensure differences reported across countries and
economies are not merely artifacts of differences
in age or education. We examined differences
between emerging and advanced economies—and
countries—on key indicators when controlling for
the effects of education and age, using multivariate
analysis of covariance (MANCOVA) tests. The pattern
of results did not change; when we report economy
and country differences, these remain significant
and meaningful when controlling for education and
age. These analyses indicate that the observed
differences across countries and economies are not
simply due to demographic differences in age or
education across country samples.

**Employee demographic profile**

Sixty-seven percent of the total sample were
employed (52% full-time; 15% part-time), yielding
32,352 respondents answering questions about
AI use at work. The proportion of employees
ranged from 50 percent (Belgium, Finland) to
89 percent (UAE). Among workers, 53 percent
were male and 47 percent female, with a mean
age of 41 (range = 18-95). Most were employed
by an organization (77%), followed by selfemployment (16%) and business ownership (7%).
Respondents worked across diverse industries
(e.g. power and utilities = 2%, manufacturing =
11%) and occupations (e.g. service and sales =
10%, professional and skilled = 32%).

**Student demographic profile**

Students comprised 5% of the sample (n = 2,499),
with 56 percent female and 44 percent male.
The mean age was 23 (range = 18-86), with 65%
enrolled in university, 18% in secondary education,
16% in vocational, trade, or technical programs,
and 1% in other forms of education. Student

respondents were present in all countries (range =
28 [Switzerland] to 115 [Nigeria]). Country-level and
economic group analyses were not conducted,
due to the small subsample sizes.

Trust, attitudes and use of AI: A global study 2025 | **107**

**Table A2-1: The demographic profile for each country sample**



Gender: W = Women, M = Men, O = Other reported genders; Education: <SS = Lower secondary school or less, SS = Upper
secondary school, Qual = Vocational or trade qualification, UG = Undergraduate degree, PG = Postgraduate degree; * indicates that
other gender and non-binary options were not provided in these countries due to cultural sensitivities.


© 2025 The University of Melbourne.

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.


Trust, attitudes and use of AI: A global study 2025 | **108**

###### **Appendix 3:
** **Key indicators for each country**





Trust = Trust in AI system, Twthy = Perceived trustworthiness of AI system, Accept = Acceptance of AI system, Benefits = Perceived benefits of AI system,
Risks = Perceived risks of AI system, Benefit-Risk = Perception that benefits of AI system outweigh the risks, Current Safeguards = Perceived adequacy of
current laws and regulations governing AI, AI knowledge = Self-reported knowledge of AI, AI Efficacy = Self-reported ability to use AI effectively.


© 2025 The University of Melbourne.

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.


Trust, attitudes and use of AI: A global study 2025 | **109**

###### **Appendix 4:
** **Changes in key indicators
** **over time for 17 countries**






Cells with darker shading indicate +/- .4 mean difference or more or percentage increases of 10% or more


© 2025 The University of Melbourne.

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.


Trust, attitudes and use of AI: A global study 2025 | **110**

###### **Appendix 4 continued**







Cells with darker shading indicate +/- .4 mean difference or more or percentage increases of 10% or more


© 2025 The University of Melbourne.

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.


Trust, attitudes and use of AI: A global study 2025 | **111**

###### **Endnotes**

1 Samborska, V. (2024). Investment in

generative AI has surged recently. Our
World in Data. [https://ourworldindata.](https://ourworldindata.org/data-insights/investment-in-generative-ai-has-surged-recently)
[org/data-insights/investment-in-](https://ourworldindata.org/data-insights/investment-in-generative-ai-has-surged-recently)
[generative-ai-has-surged-recently;](https://ourworldindata.org/data-insights/investment-in-generative-ai-has-surged-recently)
Statista. (2025). Number of artificial
intelligence (AI) tool users globally from
2020 to 2030. Statista. [https://www.](https://www.statista.com/forecasts/1449844/ai-tool-users-worldwide)
[statista.com/forecasts/1449844/ai-tool-](https://www.statista.com/forecasts/1449844/ai-tool-users-worldwide)
[users-worldwide. Qiang, C., Liu, Y., &](https://www.statista.com/forecasts/1449844/ai-tool-users-worldwide)
Wang, H. (2024). _Who on earth is using_
_generative AI?_ World Bank. [https://blogs.](https://blogs.worldbank.org/en/digital-development/who-on-earth-is-using-generative-ai-)

                      [worldbank.org/en/digital](https://blogs.worldbank.org/en/digital-development/who-on-earth-is-using-generative-ai-) development/
[who-on-earth-is-using-generative-ai-](https://blogs.worldbank.org/en/digital-development/who-on-earth-is-using-generative-ai-)

2 Rooney, K. (2025, February 2025).

OpenAI tops 400 million users despite
DeepSeek’s emergence. CNBC.
[https://www.cnbc.com/2025/02/20/](https://www.cnbc.com/2025/02/20/openai-tops-400-million-users-despite-deepseeks-emergence.html)
[openai-tops-400-million-users-despite-](https://www.cnbc.com/2025/02/20/openai-tops-400-million-users-despite-deepseeks-emergence.html)
[deepseeks-emergence.html; ChatGPT](https://www.cnbc.com/2025/02/20/openai-tops-400-million-users-despite-deepseeks-emergence.html)
took approximately 2 months to achieve
100 million users, making it the fastestgrowing consumer application in history.
In comparison, it took Instagram over 2
years to reach 100 million users. [https://](https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/)

                                  [www.reuters.com/technology/chatgpt](https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/)
[sets-record-fastest-growing-user-base-](https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/)
[analyst-note-2023-02-01/](https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/)

3 World Economic Forum (2025).

Industries in the Intelligent Age White
Paper Series. [https://www.weforum.](https://www.weforum.org/publications/industries-in-the-intelligent-age-white-paper-series/)
[org/publications/industries-in-the-](https://www.weforum.org/publications/industries-in-the-intelligent-age-white-paper-series/) _i_
[intelligent-age-white-paper-series/](https://www.weforum.org/publications/industries-in-the-intelligent-age-white-paper-series/)

4 See [National Cancer Institute. Talaat,](https://www.cancer.gov/research/infrastructure/artificial-intelligence)

F. M., Kabeel, A., & Shaban, W. M.
(2024). The role of utilizing artificial
intelligence and renewable energy in
reaching sustainable development goals.
_Renewable Energy_, _235_, 121311. [https://](https://doi.org/10.1016/j.renene.2024.121311)
[doi.org/10.1016/j.renene.2024.121311.](https://doi.org/10.1016/j.renene.2024.121311)
Center for Data Innovation. Evidence
Shows Productivity Benefits of AI.
[https://datainnovation.org/2024/06/](https://datainnovation.org/2024/06/evidence-shows-productivity-benefits-of-ai/)
[evidence-shows-productivity-benefits-](https://datainnovation.org/2024/06/evidence-shows-productivity-benefits-of-ai/)
[of-ai/](https://datainnovation.org/2024/06/evidence-shows-productivity-benefits-of-ai/)

5 Intentional use was differentiated

from the passive use of AI (e.g. when
AI operates behind the scenes in
tools such as email filters and search
engines). General-purpose generative
AI tools were the most common class
of AI intentionally used at work. We use
the term as defined and explained in this
[report by the European Parliament.](https://www.europarl.europa.eu/RegData/etudes/ATAG/2023/745708/EPRS_ATA(2023)745708_EN.pdf)

6 We adopted the International Monetary

Fund’s (IMF) classification of advanced
and emerging economies.

© 2025 The University of Melbourne.


_i_

i


_i_

i


_i_

i


_i_

i

Trust, attitudes and use of AI: A global study 2025 | **112**


7 Robustly answering the question

of which countries are leading AI
adoption and use requires a different
methodology to public attitude surveys.
The conclusions here are based on the

perceptions and experiences reported
by a representative sampling of the
public. They are not based on objective
indicators of AI adoption, investment, or
AI education and training.

8 To define global regions, we draw from

the United Nations (2023). [Standard](https://unstats.un.org/unsd/methodology/m49/)
[Country or Area Codes for Statistical](https://unstats.un.org/unsd/methodology/m49/)
[Use (49).](https://unstats.un.org/unsd/methodology/m49/)

9 Survey responses were collected from

individuals in mainland China only,
excluding Hong Kong, Macau, and Taiwan.

10 We focused primarily on the 2023

_[Government AI Readiness Index](https://oxfordinsights.com/ai-readiness/ai-readiness-index/)_ . This

index ranks and provides a total score
for 193 countries on AI readiness across
three pillars: Government (e.g. existence
of a national AI strategy, cybersecurity), Technology (e.g. number of AI
unicorns, R&D spending), and Data and
Infrastructure (e.g. telecommunications
infrastructure, households with internet
access). The countries selected had
rankings at or near the top for their
region on the 2023 Government AI
Readiness Index. We supplemented _i_
this with data from the 2024 _[Stanford](https://aiindex.stanford.edu/report/)_

_[AI Index](https://aiindex.stanford.edu/report/)_, which examines country-level
private investment in AI and acceleration
in AI activity over time to enable the
identification of countries that are rapidly
emerging in AI in regions that historically
lacked AI capacity and investment (e.g.
South Africa, Brazil, India, Mexico,
Portugal, the UAE, etc.).

11 See Adams, R., Adeleke, F., Florido,

A., de Magalhães Santos, L. G.,

i Grossman, N., Junck, L., & Stone, K.

(2024). Global Index on Responsible AI
2024 (1st Edition). South Africa: Global
[Center on AI Governance. https://girai-](https://girai-report-2024-corrected-edition.tiiny.site/)
[report-2024-corrected-edition.tiiny.](https://girai-report-2024-corrected-edition.tiiny.site/)
[site/](https://girai-report-2024-corrected-edition.tiiny.site/) This index assesses responsible
AI governance across 138 countries,
measuring human rights protections,
AI governance policy, and institutional
capacities through government actions,
frameworks, and non-state actor
initiatives.


12 China is considered an emerging

economy by the IMF despite its large
size and economic power because,
while it has experienced rapid GDP
growth and industrialization, its per
capita income remains significantly
lower than developed nations, indicating
that its economy is still transitioning
toward a fully developed state; this is
further supported by factors like ongoing
economic reforms, a large developing
market, and a focus on infrastructure
development.

13 Data was collected from representative

research panels sourced by Dynata, a
global leader in survey research panel
provision.

14 Income was assessed using a question

from the World Values Survey Group
(WVS; [Haerpfer et al., 2022). It was self-](https://doi.org/10.14281/18241.24)
reported on a 10-point scale from 1 =
Lowest income group to 10 = Highest
income group with a ‘Prefer not to say’
option. For demographic analysis, we recoded responses into three categories:
Low = 1-3, Medium = 4-7, High = 8-10.
This is aligned with WVS categorization.

15 Occupational groupings were

sourced from _[the International Labor](https://ilostat.ilo.org/methods/concepts-and-definitions/classification-occupation/)_

_[Organization’s International Standard](https://ilostat.ilo.org/methods/concepts-and-definitions/classification-occupation/)_
_[Classifications of Occupations.](https://ilostat.ilo.org/methods/concepts-and-definitions/classification-occupation/)_

16 We adapted and simplified the definition

to make it accessible to a broad and
diverse range of people with varying
levels of reading ability, while retaining
key defining elements. See discussion
of the evolution of the OECD definition
of AI in: _[What is AI? Can you make a](https://oecd.ai/en/wonk/definition)_
_[clear distinction between AI and non-](https://oecd.ai/en/wonk/definition)_

_[AI systems?](https://oecd.ai/en/wonk/definition)_ Across this report, the
terms “AI” and “AI System” are used
interchangeably for simplicity.

i

17 Four of the 17 countries surveyed

at both time points are emerging
economies: Brazil, China, India, and
South Africa. However, as there is no
clear differences between advanced

and emerging economies in changes
over time, so we do not distinguish
between them in reporting the findings
[of change.](https://oecd.ai/en/wonk/definition)

18 Responses to the four items assessing

AI knowledge were aggregated to
produce an overall score.


_i_

i

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

###### **Endnotes**

19 In support of this interpretation,

the _[2024 Stanford AI Index](https://aiindex.stanford.edu/report/)_ reports
accelerated use and adoption of AI in
several emerging economies, as well
as the increasing economic importance
of AI in these countries. Our pattern
[of findings aligns with a recent Ipsos/](https://publicpolicy.google/resources/our_life_with_ai_google_ipsos_report.pdf)
[Google survey that demonstrates AI use](https://publicpolicy.google/resources/our_life_with_ai_google_ipsos_report.pdf)
and positive attitudes are particularly
high in emerging economies.

20 This definition aligns with dominant

interdisciplinary definitions of trust (e.g.
Mayer et al., [1995; Rousseau et al.,](https://doi.org/10.2307/258792) [2009),](https://journals.aom.org/doi/10.5465/amr.1998.926617)
including trust in technological systems
(see McKnight et al., [2002, 2011).](https://pubsonline.informs.org/doi/epdf/10.1287/isre.13.3.334.81)

21 Perceptions of trustworthiness

are typically higher than trusting
intentions because trust involves risk

and vulnerability (e.g. by relying on AI
output or sharing information with an AI
system), whereas perceiving a system as
trustworthy does not. There is a strong
association between the perceived
trustworthiness of AI systems and
trusting AI systems (r=.79).

22 We also find people are more willing

to share information with healthcare AI

systems (57%, M=4.5), than rely on the
output of these systems (48%, M=4.2),
reflecting the expectation that sharing
information with healthcare providers
and systems is a routine and necessary
part of health care provision. We find this
difference between willingness to share
information and rely on AI systems
across applications.

23 Norway’s high level of trust in AI systems, i

compared to many other advanced
economies, may reflect Norwegians’
comparatively high levels of AI training i
and literacy, workplace adoption of
AI, trust in government use of AI, and
awareness of laws and regulation relating
to AI, as evidenced in this report.

24 The _[2024 Stanford AI Index](https://aiindex.stanford.edu/report/)_ reports

accelerated use and adoption of AI in
several emerging economies, as well as
the increasing economic importance of AI
in these countries. Our pattern of findings
aligns with a recent [Ipsos/Google survey](https://publicpolicy.google/resources/our_life_with_ai_google_ipsos_report.pdf)
that demonstrates AI use and positive
attitudes are particularly high in emerging i
economies.

© 2025 The University of Melbourne.


i

i

i


i

i

i


i

i

i


i

i

i

Trust, attitudes and use of AI: A global study 2025 | **113**


25 We asked questions related to the

experience or observation of benefits and
risks only of people who reported they
had experience with the AI application
they were allocated, i.e. AI systems (59%
reported experience; Emerging = 68%.
Advanced = 55%), Generative AI (50%
experienced; Emerging = 60%, Advanced
= 45%), AI use in Human Resources
(21% experienced; Emerging = 31%,
Advanced = 15%), or AI use in Healthcare
(18% experienced; Emerging = 28%,
Advanced = 13%).

26 Some benefits were observed or

experienced more in relation to the use of
AI in Human Resources and Healthcare.
Specifically, people had experienced or
observed increased fairness from AI use

in Human Resources and Healthcare

(62-64%) more so than from Generative
AI tools or AI systems in general (41%42%), and reduced costs and better
use of resources from AI use in Human

Resources and Healthcare (68-74%)
compared to Generative AI or AI
systems (59-60%).

27 The list of risks and benefits was the

outcome of extensive survey piloting
including analysis of open-ended
questions asking about benefits and
risks of AI systems.

28 Independent surveys showing public

desire for regulation include: The Ada
Lovelace Institute and The Alan Turing
Institute (2025). _[How do people feel](https://attitudestoai.uk/assets/documents/Ada-Lovelace-Institute-The-Alan-Turing-Institute-How-do-people-feel-about-AI.pdf)_
_[about AI? Wave two of a nationally](https://attitudestoai.uk/assets/documents/Ada-Lovelace-Institute-The-Alan-Turing-Institute-How-do-people-feel-about-AI.pdf)_
_[representative survey of UK attitudes](https://attitudestoai.uk/assets/documents/Ada-Lovelace-Institute-The-Alan-Turing-Institute-How-do-people-feel-about-AI.pdf)_
_[to AI](https://attitudestoai.uk/assets/documents/Ada-Lovelace-Institute-The-Alan-Turing-Institute-How-do-people-feel-about-AI.pdf)_ . Eurobarometer (2025). [Artificial](https://europa.eu/eurobarometer/surveys/detail/3222)
[Intelligence and the future of work. Saeri,](https://europa.eu/eurobarometer/surveys/detail/3222)
A., Noetel, M., & Graham, J. (2024).
[Survey Assessing Risks from Artifcial i](https://dx.doi.org/10.2139/ssrn.4750953)
[Intelligence (Technical Report). Rethink](https://dx.doi.org/10.2139/ssrn.4750953)
Priorities (2023). [US public opinion of AI](https://rethinkpriorities.org/wp-content/uploads/2023/05/FormattedAIPublicOpinionWriteUp3.pdf)
[policy and risk.](https://rethinkpriorities.org/wp-content/uploads/2023/05/FormattedAIPublicOpinionWriteUp3.pdf)

29 Ipsos (2024). [Public trust in AI:](https://www.ipsos.com/sites/default/files/ct/news/documents/2024-09/Ipsos Public Trust in AI.pdf)

[Implications for policy and regulation.](https://www.ipsos.com/sites/default/files/ct/news/documents/2024-09/Ipsos Public Trust in AI.pdf)
Seth, J. (2024). [Public Perception of AI:](https://doi.org/10.48550/arXiv.2407.15998)
[Sentiment and Opportunity.](https://doi.org/10.48550/arXiv.2407.15998)

30 One of the most significant reforms to

legislation and regulation of AI is the
EU AI Act, which governs members of
the European Union. [This act offcially i](https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai#:~:text=The AI Act entered into,application from 2 February 2025)
[entered into force on 1 August 2024, and](https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai#:~:text=The AI Act entered into,application from 2 February 2025)
[intends to be fully applicable by 2 August](https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai#:~:text=The AI Act entered into,application from 2 February 2025)
[2026, with some exceptions. We found](https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai#:~:text=The AI Act entered into,application from 2 February 2025)
no difference in the perceived adequacy
of regulation or awareness of regulation
between people in countries governed
by the EU AI Act and people in other
countries with advanced economies.
This likely reflects that our data collection
preceded the practical implementation of
the obligations of the EU AI Act, which
commenced on 2 February 2025.


31 Structural equation modeling (SEM) is a

suite of multivariate techniques that offers
advantages over other regression-based
approaches. It explicitly accounts for
measurement error to yield less biased
estimates, estimates latent constructs
from observed indicators, and evaluates
the fit between the model and the data.
Our model fit the data well: x2 (N =
46524, df = 2272) = 113119.70, p < .001;
CFI: .94, TLI: .94, SRMR: .07, RMSEA: .03.
For an accessible guide to the structural
equation modeling process, see Kline,
R. B. (2023). _Principles and Practices of_
_Structural Equation Modeling_ (5 [th] ed.).
Guilford Press: New York.

32 ‘B’ refers to the standardized beta

coefficient, which indicates the strength
of the effect of each independent variable
(i.e., driver) on the dependent variable
(i.e., outcome). Beta coefficients can be
compared to indicate the relative strength
of each independent variable. B=.43 from
trust to acceptance means that if trust
increases by one standard deviation,
acceptance is expected to increase by
about .43 standard deviations.

33 Bach, T. A., Khan, A., Hallock, H.,

Beltrão, G., & Sousa, S. (2024). A
systematic literature review of user
trust in AI-enabled systems: An HCI
perspective. _International Journal of_
_Human–Computer Interaction_, _40_ (5),
1251-1266. Oksanen, A., Savela, N.,
Latikka, R., & Koivula, A. (2020). Trust
toward robots and artificial intelligence:
An experimental approach to human–

i technology interactions online. _Frontiers_

_in Psychology_, _11_, 568256.

34 For example, the perceived usefulness

i

of technology is core to technology
acceptance models, e.g. Venkatesh,
V., & Davis, F. D. (2000). A theoretical
extension of the technology acceptance
model: Four longitudinal field studies.
_Management Science, 46_ (2), 186-204.
Perceived benefits have also been
found to enhance trust in automation:

Hoff, K. A., & Bashir, M. (2015). Trust
in automation: Integrating empirical
evidence on factors that influence trust.
_Human Factors_, 57(3), 407-434. https://

i doi.org/10.1177/0018720814547570.

35 Hoff, K. A., & Bashir, M. (2015). Trust in

automation: Integrating empirical evidence
on factors that influence trust. _Human_
_Factors_, _57_ (3), 407-434. https://doi.
org/10.1177/0018720814547570


i

i

i

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

###### **Endnotes**

36 Beldad, A., De Jong, M., & Steehouder,

M. (2010). How shall I trust the faceless
and the intangible? A literature review
on the antecedents of online trust.

_Computers in Human Behavior, 26_ (5),
857-869. McKnight, D. H., Choudhury,
V. & Kacmar, C. (2002). Developing
and Validating Trust Measures for
e-Commerce: An Integrative Typology.
_Information Systems Research,13_ (3),
334-359.

37 The model is similar to the one we

produced in our 2023 report, with
additional AI literacy metrics to better
reflect AI knowledge and efficacy.
The replication of the model using the
current data collected from 47 countries
speaks to the robustness of the model.
See Gillespie, N., Lockey, S., Curtis, i
C., Pool, J. & Akbari, A. (2023). _[Trust](https://kpmg.com/au/en/home/insights/2023/02/trust-in-ai-global-insights-2023.html)_
_[in Artificial Intelligence: A Global Study.](https://kpmg.com/au/en/home/insights/2023/02/trust-in-ai-global-insights-2023.html)_
The University of Queensland and KPMG
Australia. doi.org/10.14264/00d3c94

38 See appendix 2 for further details of the

employee sample.

39 As participants could select multiple

options, the percentages sum to more
than 100%. These options were derived
from thematic analysis of the key
reasons for not using AI identified by
employees during our two pilot studies i i
conducted to inform and validate the

survey questions. We also included an
‘other’ option in our global survey to
capture participants qualitative reasons
for not using AI, which was completed
by 360 participants. Thematic analysis
of this data revealed the majority (78%)
of reasons overlapped with the options
reported here.

40 There is no difference across economic

groups in the use of publicly available tools
(71% in emerging economies vs. 70%
in advanced) or tools managed by one’s
organization (43% vs. 41%, respectively).

41 A caveat is that these differences

between economic groups may, in part,
reflect that employees in emerging
economies have higher levels of AI
training and literacy, resulting in a greater
understanding of AI and when and how
it is used at work, rather than the actual
use of AI by the organization.

42 Social desirability bias refers to the

tendency for research subjects to
give socially desirable responses to
sensitive questions instead of providing
responses that reflect their true feelings
or experiences (see Grimm, [2010, for](https://doi.org/10.1002/9781444316568.wiem02057)
an overview).

© 2025 The University of Melbourne.


i

i i


i

i i


i

i i


i

i i

Trust, attitudes and use of AI: A global study 2025 | **114**


43 See, for example: Chesley, N. (2014).

Information and communication
technology use, work intensification
and employee strain and distress. Work,
Employment and Society, 28 (4), 589610. Malik, N., Tripathi, S., Kar, A., &
Gupta, S. (2021). Impact of artificial
intelligence on employees working
in industry 4.0 led organizations.
International Journal of Manpower,
43 (2), 334-354.

44 See, for example: Weibel, A., Den Hartog,

D., Gillespie, N., Searle, R., Six, F., &
Skinner, D. (2016). How do Controls Impact
Employee Trust in the Employer? _Human_
_Resource Management_, 55 (3), 437-462.

45 We adapted a measure from Haesevoets,

[de Cremer, Dierckx & van Hiel. (2021). Human-machine collaboration in](https://doi.org/10.1016/j.chb.2021.106730) i

[managerial decision making.](https://doi.org/10.1016/j.chb.2021.106730) _Computers_
_in Human Behavior, 119._

46 This finding also supports prior research

reporting concerns about potential job
losses resulting from AI and automation.
For example: ADP Research Institute
(2024). [People at Work 2024: A Global](https://www.adpresearch.com/assets/people-at-work-2024-a-global-workforce-view/)
[Workforce View; Eurobarometer (2025).](https://www.adpresearch.com/assets/people-at-work-2024-a-global-workforce-view/)
Artificial Intelligence and the future of
[work; Pew Research Center (2025). How](https://www.pewresearch.org/internet/2025/04/03/how-the-us-public-and-ai-experts-view-artificial-intelligence/)
[the U.S. Public and AI Experts View](https://www.pewresearch.org/internet/2025/04/03/how-the-us-public-and-ai-experts-view-artificial-intelligence/)
[Artificial Intelligence.](https://www.pewresearch.org/internet/2025/04/03/how-the-us-public-and-ai-experts-view-artificial-intelligence/) i

47 Organizational support of AI (AI strategy,

culture, and support for AI literacy)
has no discernible impact on critical
engagement. This is likely because its
power in predicting critical engagement
is largely captured by the more direct
measure of AI literacy.

48 Given some groups of employees are

significantly more likely to use AI at
work, we controlled for AI use frequency
when analyzing demographic influences
on inappropriate and complacent use
behaviors in multivariate analysis of
covariance (MANCOVA) models. This is
important because frequency of AI use at
work is a strong predictor of complacent
or inappropriate use of AI (effect size [n²]
= .05 to .12). Without controlling for use,
demographic effects may be inflated,
reflecting greater exposure to AI rather
than meaningful differences in how AI
is used by different groups of people.


49 The partial eta-squared effect size (n²)

helps to explain the practical magnitude
of the effect of one variable on another
after considering the influence of other
variables in the model. Effect sizes of

.01, .06, .14 indicate small, medium, and
large effects, respectively. The University
of Cambridge’s MRC Cognition and
Brain Sciences Unit provides a userfriendly [primer on effect sizes. See](https://imaging.mrc-cbu.cam.ac.uk/statswiki/FAQ/effectSize)
also see Lakens, D. (2013). [Calculating](https://www.frontiersin.org/articles/10.3389/fpsyg.2013.00863/full)
[and reporting effect sizes to facilitate](https://www.frontiersin.org/articles/10.3389/fpsyg.2013.00863/full)
[cumulative science: A practical primer](https://www.frontiersin.org/articles/10.3389/fpsyg.2013.00863/full)

[for t-tests and ANOVAS.](https://www.frontiersin.org/articles/10.3389/fpsyg.2013.00863/full) _Frontiers in_

_Psychology_, _4_, 863

50 Industry groups were adapted from

the [International Labour Organization](https://ilostat.ilo.org/methods/concepts-and-definitions/classification-economic-activities/)
[International Standard Industrial](https://ilostat.ilo.org/methods/concepts-and-definitions/classification-economic-activities/)

[Classifcationi](https://ilostat.ilo.org/methods/concepts-and-definitions/classification-economic-activities/) of all economic activities.

51 In a historical context, it can be viewed

as normal early in the journey of adopting
a powerful, disruptive and transformative
technology for there to be a period
of ambivalence and adjustment until
appropriate standards, best practice,
norms, governance and regulation
emerges to guide development and
use and mitigate harms.

52 See the European Commission’s (EC)

outline of the [European approach](https://digital-strategy.ec.europa.eu/en/policies/european-approach-artificial-intelligence)

i [to artificial intelligence, which is](https://digital-strategy.ec.europa.eu/en/policies/european-approach-artificial-intelligence)

underpinned by the EU AI Act. The EC
notes that fostering excellence in AI will
strengthen Europe’s ability to compete
globally, and that trust is central to the
vision of making the EU a world-class
hub for AI while ensuring safety and
fundamental rights.

53 As history has shown, this is not the

first time a technology has created
this tension, nor will it be the last time.
See Frey, C. (2019). _Technology Trap:_
_Capital, Labor, and Power in the Age of_
_Automation. Princeton University Press_ .

54 There is some evidence to suggest that

practical application of responsible AI
mechanisms remain at an early stage
including in emerging economies. For
examples, see Reul, A., Connolly, P.,
Meimandi, K., Tewari, S., Wiatrak, J.,
Venkatesh, D., & Kochenderfer, M.
(2024). Responsible AI in the Global
Context: Maturity Model and Survey.
[https://arxiv.org/abs/2410.09985;](https://arxiv.org/abs/2410.09985)
Renieris, E., Kiron, D, & Mills, S. (2022).
To Be a Responsible AI Leader, Focus
on Being Responsible. MIT Sloan
Management Review and Boston
[Consulting Group. https://sloanreview.](https://sloanreview.mit.edu/projects/to-be-a-responsible-ai-leader-focus-on-being-responsible/)
[mit.edu/projects/to-be-a-responsible-ai-](https://sloanreview.mit.edu/projects/to-be-a-responsible-ai-leader-focus-on-being-responsible/)
[leader-focus-on-being-responsible/;](https://sloanreview.mit.edu/projects/to-be-a-responsible-ai-leader-focus-on-being-responsible/)


i

i i

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

###### **Endnotes**

[55 See Google’s 2024 report examining](https://publicpolicy.google/resources/ai-digital-sprinters.pdf)

[the economic potential of AI in](https://publicpolicy.google/resources/ai-digital-sprinters.pdf)
[emerging markets.](https://publicpolicy.google/resources/ai-digital-sprinters.pdf)

[56 A recent Ipsos/Google survey](https://publicpolicy.google/resources/our_life_with_ai_google_ipsos_report.pdf) also

supports this view, showing that
people in emerging economies—and
particularly Nigeria—are more likely
to think that AI will have a positive
impact on the economy, suggesting
positive perceptions of AI as a driver
of economic prosperity.

[57 See UK and US refuse to sign](https://www.bbc.com/news/articles/c8edn0n58gwo)

[international AI declaration.](https://www.bbc.com/news/articles/c8edn0n58gwo)

58 See [https://artificialintelligenceact.eu/](https://artificialintelligenceact.eu/high-level-summary/)

[high-level-summary/](https://artificialintelligenceact.eu/high-level-summary/)

59 World Economic Forum (2024). The

Global Risks Report 2024 (19th ed.).
[https://www.weforum.org/publications/](https://www.weforum.org/publications/global-risks-report-2024)
[global-risks-report-2024](https://www.weforum.org/publications/global-risks-report-2024)

[60 Meta is abandoning fact checking—this](https://theconversation.com/meta-is-abandoning-fact-checking-this-doesnt-bode-well-for-the-fight-against-misinformation-246878)

[doesn’t bode well for the fight against](https://theconversation.com/meta-is-abandoning-fact-checking-this-doesnt-bode-well-for-the-fight-against-misinformation-246878)
[misinformation; For further evidence-](https://theconversation.com/meta-is-abandoning-fact-checking-this-doesnt-bode-well-for-the-fight-against-misinformation-246878)
based information on strategies for
countering disinformation see [Countering](https://carnegieendowment.org/research/2024/01/countering-disinformation-effectively-an-evidence-based-policy-guide?lang=en)

                                   [Disinformation Effectively: An Evidence](https://carnegieendowment.org/research/2024/01/countering-disinformation-effectively-an-evidence-based-policy-guide?lang=en)
[Based Policy Guide | Carnegie](https://carnegieendowment.org/research/2024/01/countering-disinformation-effectively-an-evidence-based-policy-guide?lang=en)
[Endowment for International Peace](https://carnegieendowment.org/research/2024/01/countering-disinformation-effectively-an-evidence-based-policy-guide?lang=en)

61 The levels of organizational support

for responsible AI may be even lower
in practice than how it is reported by
employees. This perception-practice gap
is illustrated by the 2024 Responsible
AI Index, which found that while most
executives believe their AI systems align
with responsible AI principles, fewer
than one-third had actively implemented
responsible AI practices.

62 A 2024 Boston Consulting Group study

found that only 26% of organizations
surveyed have developed the necessary
capabilities to move beyond proof-ofconcept and generate tangible AI value
at scale.

63 For example, ISO Standards [42001,](https://www.iso.org/standard/81230.html)

[23894, and 38507](https://www.iso.org/standard/77304.html) can all help
organizations with their AI governance.
Further, for an overview of over 900
resources to support responsible AI use,
see the OECD’s Tools for Trustworthy
AI - OECD.AI.

[64 Research by the Human Technology](https://utsd8.prod.acquia-sites.com/sites/default/files/2024-05/EssentialResearch%2BUTS_Invisible_Bystanders_0524_D4.pdf)

[Institute at the University of Technology](https://utsd8.prod.acquia-sites.com/sites/default/files/2024-05/EssentialResearch%2BUTS_Invisible_Bystanders_0524_D4.pdf)
Sydney finds that many employees
feel they are "invisible bystanders"
in the adoption of AI into their work;
that technology is imposed on them
rather than being designed with them.
The research recommends creating
avenues for structured engagement
with employees around AI deployment.

© 2025 The University of Melbourne.


i

i


i

i


i

i


i

i

Trust, attitudes and use of AI: A global study 2025 | **115**


65 Current AI governance has heavily

emphasized systemic issues—
addressing how AI systems are built and
how they impact society at large—and
comparatively less emphasis has been
placed on regulating or guiding the
use of AI by individuals. Major policy
frameworks and principles—from the
EU and OECD to national strategies—
emphasize themes such as fairness,
transparency, safety, accountability, and
human oversight, and typically target AI
developers and deployers. Regarding

i AI use in organizations, see Bird & Bird

[(2025) AI Governance: Essential Insights](https://www.twobirds.com/en/insights/2025/ai-governance-essential-insights-for-organisations-part-i--understanding-meaning-challenges-trends-a#:~:text=structures,this%20Article%20attempts%20to%20address)
[for Organizations for analysis observing](https://www.twobirds.com/en/insights/2025/ai-governance-essential-insights-for-organisations-part-i--understanding-meaning-challenges-trends-a#:~:text=structures,this%20Article%20attempts%20to%20address)
that most policies focus on high-level
standards rather than providing granular
guidance around training employees
on AI governance or setting rules for
employees’ day-to-day AI usage.

i 66 Solomon, L., & Davis, N. (2023) The State

of AI Governance in Australia, Human
Technology Institute, The University of
Technology Sydney; [see also International](https://assets.publishing.service.gov.uk/media/679a0c48a77d250007d313ee/International_AI_Safety_Report_2025_accessible_f.pdf)
[AI Safety Report (2025).](https://assets.publishing.service.gov.uk/media/679a0c48a77d250007d313ee/International_AI_Safety_Report_2025_accessible_f.pdf)

67 We received extensive feedback on

the survey throughout its development
from academic and industry experts
and conducted two large-scale pilot
tests (Pilot 1, N = 751 respondents
from the UK, USA, and Australia; Pilot
2, N = 793 respondents from the USA
and UK). During these pilot tests, we
specifically solicited feedback on the
construct and face validity of new
measures by providing respondents with
definitions and asking them to assess
whether these adequately covered the
intended construct, as well as broader
recommendations to enhance the survey.

68 Research suggests that using multiple

indicators to determine respondent
attentiveness is important: Ward, M.
K., & Meade, A. W. (2023). Dealing
with careless responding in survey
data: Prevention, identification, and
recommended best practices. _Annual_
_Review of Psychology_, _74_ (1), 577-596.
Meade, A. W., & Craig, S. B. (2012).
Identifying careless responses in survey
data. _Psychological Methods_, _17_ (3),
437. Oppenheimer, D. M., Meyvis, T.,
& Davidenko, N. (2009). Instructional
manipulation checks: Detecting
satisficing to increase statistical
power. _Journal of Experimental_
_Social Psychology_, _45_ (4), 867-872.


69 See Field, A. (2013). _Discovering statistics_

_using IBM SPSS statistics (4th ed.)_ . Sage:
London. (See page 474; values for w2 of
.01, .06, .14 indicate small, medium, and
large effects respectively).

70 As a rule of thumb, a _Hedges' g_ value of

.2 is considered a small effect size, .5 a
medium effect size, and .8 or larger, a
large effect size (see Lakens, D. (2013)
[Calculating and reporting effect sizes to](https://www.frontiersin.org/articles/10.3389/fpsyg.2013.00863/full)
[facilitate cumulative science: A practical](https://www.frontiersin.org/articles/10.3389/fpsyg.2013.00863/full)
[primer for t-tests and ANOVAS.](https://www.frontiersin.org/articles/10.3389/fpsyg.2013.00863/full) _Frontiers_

i _in Psychology_, _4_, 863). However,

interpretation of effect sizes is subjective,
and we have chosen a cut-off of .3 rather

than .2 because this ensures a practically
meaningful and robust difference which
trends toward a medium, rather than a
small effect.

71 Respondents’ belief that their

i organization uses AI was asked in a yes/

no/don’t know format in 2022, while the
extent of organizational use (ranging from
1 = not at all to 5 = to a very large extent)
was asked in 2024. As such, this variable
was re-coded into use (responses =
2-4) vs. no use (response = 1) in order to
make meaningful comparisons. Similarly,
employee AI use was measured slightly
differently across time. Change in total
use, rather than regular or semi-regular
use, is reported.

72 Comparative data sourced from

[https://data-explorer.oecd.org/ or](https://data-explorer.oecd.org/)
from [https://databank.worldbank.org/](https://databank.worldbank.org/source/education-statistics:-Education-Attainment)
[source/education-statistics:-Education-](https://databank.worldbank.org/source/education-statistics:-Education-Attainment)
[Attainment where not available from](https://databank.worldbank.org/source/education-statistics:-Education-Attainment)

OECD.

73 This is often a limitation of online public

attitude surveys (e.g. see University
of Oxford’s Reuters Institute report
[How we follow climate change:](http://dx.doi.org/10.60625/risj-89jy-c376)
[Climate news use and attitudes in eight](http://dx.doi.org/10.60625/risj-89jy-c376)
[countries, and the OECD's technical](http://dx.doi.org/10.60625/risj-89jy-c376)
[details](https://www.oecd.org/en/publications/survey-design-and-technical-documentation-supporting-the-2021-oecd-survey-on-drivers-of-trust-in-government-institutions_6f6093c5-en.html) of its 2021 survey of drivers
of trust in government institutions for
acknowledgement and discussion).


i

i

© 2025 Copyright owned by one or more of the KPMG International entities.
KPMG International entities provide no services to clients. All rights reserved.

**Key contacts**

**The University of Melbourne**


**Professor Nicole Gillespie**
**Chair of Trust**

**Professor of Management**
**Melbourne Business School,**
**The University of Melbourne**
**E:** [n.gillespie@unimelb.edu.au](mailto:n.gillespie%40unimelb.edu.au?subject=)

**KPMG**

**James Mabbott**

**National Leader,**

**KPMG Futures**

**KPMG Australia**

**E:** [jmabbott@kpmg.com.au](mailto:jmabbott%40kpmg.com.au?subject=)

© 2025 The University of Melbourne.


**Dr Steve Lockey**
**Senior Research Fellow**

**Melbourne Business School,**
**The University of Melbourne**
**E:** [s.lockey@mbs.edu](mailto:s.lockey%40mbs.edu?subject=)

**David Rowlands**

**Global Head of**
**Artificial Intelligence**
**KPMG International**

**E:** [david.rowlands@kpmg.co.uk](mailto:David.Rowlands%40KPMG.co.uk?subject=)


**Sam Gloede**

**Global Trusted AI**

**Transformation Leader**

**KPMG International**

**E:** [sgloede@kpmg.com](mailto:sgloede%40kpmg.com?subject=)


The information contained in this document is of a general nature and is not intended to address the objectives, financial situation or needs of any particular individual or
entity. It is provided for information purposes only and does not constitute, nor should it be regarded in any manner whatsoever, as advice and is not intended to influence
a person in making a decision, including, if applicable, in relation to any financial product or an interest in a financial product. Although we endeavour to provide accurate
and timely information, there can be no guarantee that such information is accurate as of the date it is received or that it will continue to be accurate in the future. No one
should act on such information without appropriate professional advice after a thorough examination of the particular situation.

To the extent permissible by law, KPMG and its associated entities shall not be liable for any errors, omissions, defects or misrepresentations in the information or for
any loss or damage suffered by persons who use or rely on such information (including for reasons of negligence, negligent misstatement or otherwise).

©2025 KPMG, an Australian partnership and a member firm of the KPMG global organisation of independent member firms affiliated with KPMG International Limited,
a private English company limited by guarantee. All rights reserved.

The KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organisation.

Liability limited by a scheme approved under Professional Standards Legislation.

April 2025. 1601254672FUT.


